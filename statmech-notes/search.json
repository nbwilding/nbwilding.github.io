[
  {
    "objectID": "chapter10.html",
    "href": "chapter10.html",
    "title": "10. Ideal gas in the low density limit",
    "section": "",
    "text": "Blundell and Blundell chapter 2, Baierlein 5.6\nIn the last chapter, the calculation of \\(Z(1)\\) for the ideal gas was begun. In this chapter, we finish the calculation and discuss when the semi-classical treatment is valid. The concept of a density of states is also introduced.\n\n\nWe first finish the job begun in the last chapter:\n\\[\nZ(1) = \\pi \\int_0^\\infty \\exp\\left(-an^2\\right) n^2 dn \\quad \\text{where} \\quad a = \\frac{\\hbar^2 \\pi^2}{2kTML^2}\n\\]\nThe Gaussian integral that should be familiar is:\n\\[\n\\int_{-\\infty}^\\infty \\exp(-an^2) dn = \\left(\\frac{\\pi}{a}\\right)^{1/2}\n\\]\nThus, we see:\n\\[\nZ(1) = -\\frac{1}{2} \\frac{d}{da} \\left( \\int_{-\\infty}^\\infty \\exp(-an^2) dn \\right) = -\\frac{1}{2} \\frac{d}{da} \\left(\\frac{\\pi}{a}\\right)^{1/2} = \\frac{\\sqrt{\\pi}}{4a^{3/2}}\n\\]\nThe factor \\(\\frac{1}{2}\\) comes from the limit \\(0\\) to \\(\\infty\\) in the integral for \\(Z(1)\\). Inserting the expression for \\(a\\) gives:\n\\[\nZ(1) = \\frac{\\pi^{3/2}}{8} \\left( \\frac{2ML^2}{\\beta \\hbar^2 \\pi^2} \\right)^{3/2} = V \\left( \\frac{2\\pi M k T}{h^2} \\right)^{3/2}\n\\]\nwhere \\(h = 2\\pi \\hbar\\) has been used.\n\n\n\nConsider again the approximation of a sum by an integral:\n\\[\n\\sum_{n_x,n_y,n_z} A(n) \\rightarrow \\frac{1}{8} \\int_0^\\infty A(n) 4\\pi n^2 dn\n\\]\nwhere \\(A(n)\\) is any function of \\(n\\). Now change variables to \\(k\\) (see 9.3):\n\\[\nn = \\frac{L}{\\pi} k \\quad dn = \\frac{L}{\\pi} dk\n\\]\nand we find:\n\\[\n\\sum_{n_x,n_y,n_z} A(n) \\rightarrow \\frac{1}{8} \\int A(k) \\frac{4\\pi L^2 k^2}{\\pi^2} \\frac{L}{\\pi} dk = \\int A(k) \\Gamma(k) dk\n\\]\nwhere:\n\\[\n\\Gamma(k) dk = \\frac{k^2}{2\\pi^2} V dk\n\\] is the number of states with \\(k\\) between \\(k\\) and \\(k + dk\\).\n\n\n\n\n\n\n\\(\\Gamma(k)\\)is known as the “density of states” (here, in \\(k\\) space).\n\n\n\nSimilarly, we can change variables to \\(\\epsilon\\):\n\\[\n\\epsilon = \\frac{\\hbar^2 k^2}{2M}\\Rightarrow \\quad k = \\left(\\frac{2M \\epsilon}{\\hbar^2}\\right)^{1/2}\\Rightarrow \\quad dk = \\frac{dk}{d\\epsilon}d\\epsilon=\\frac{1}{2} \\left(\\frac{2M}{\\hbar^2}\\right)^{1/2} \\epsilon^{-1/2} d\\epsilon\n\\]\nNow, to obtain the density of states in energy space, denoted here by \\(g(\\epsilon)\\), equate:\n\\[\n\\Gamma(k) dk = g(\\epsilon) d\\epsilon\n\\]\nand change variables:\n\\[\ng(\\epsilon) d\\epsilon = \\left(\\frac{2M}{\\hbar^2}\\right)^{3/2} \\frac{V}{4\\pi^2} \\epsilon^{1/2} d\\epsilon\n\\]\nThe meaning of the density of states \\(g(\\epsilon)\\) is that \\(g(\\epsilon) d\\epsilon\\) is the number of states with energy between \\(\\epsilon\\) and \\(\\epsilon + d\\epsilon\\).\nIf you’re unsure of this change of variables from \\(\\Gamma(k)\\) to \\(g(\\epsilon)\\), you can carry it out as follows: Consider for any function \\(A(k)\\):\n\\[\n\\int A(k) \\Gamma(k) dk = \\int A(k) \\Gamma(k) \\frac{dk}{d\\epsilon} d\\epsilon = \\int A(\\epsilon) g(\\epsilon) d\\epsilon\n\\]\nUsing the density of states, one can write, for example:\n\\[\nZ(1) = \\int_0^\\infty  g(\\epsilon) \\exp(-\\beta \\epsilon) d\\epsilon\n\\]\nNotice that the density of states increases with energy, meaning that at higher energies, more states are available to the particle.\n\n\n\n\n\n\nFigure 1: Density of states as a function of energy\n\n\n\n\n\n\nIn the semi-classical treatment developed over this and the previous chapter:\n\\[\nZ = \\frac{Z(1)^N}{N!} = \\frac{V^N}{N!} \\left( \\frac{2\\pi M k T}{h^2} \\right)^{3N/2}\n\\]\nWe now proceed to calculate the usual thermodynamic variables:\n\\[\nF = -kT \\ln Z = NkT \\left[ \\ln \\left( \\frac{N}{V} \\right) - 1 - \\frac{3}{2} \\ln \\left( \\frac{2\\pi M k T}{h^2} \\right) \\right]\n\\]\nwhere Stirling’s approximation for \\(\\ln N!\\) was used.\nEnergy:\n\\[\n\\overline{E} = k T^2 \\frac{\\partial \\ln Z}{\\partial T} = \\frac{3}{2} NkT\n\\]\nEntropy:\n\\[\nS = \\frac{\\overline{E} - F}{T} = Nk \\left[ \\ln \\left( \\frac{V}{N} \\left( \\frac{2\\pi M k T}{h^2} \\right)^{3/2} \\right) + \\frac{5}{2} \\right]\n\\]\nPressure:\n\\[\nP = - \\left( \\frac{\\partial F}{\\partial V} \\right)_T = \\frac{NkT}{V} \\quad \\text{(ideal gas equation of state)}\n\\]\nHeat capacity:\n\\[\nC_V = \\frac{3}{2} Nk \\quad \\text{(equipartition)}\n\\]\nNote that\n\nthe ideal gas law and equipartition of energy are recovered.\nThe recovery of the ideal gas law finally identifies our statistical mechanical definition of temperature (see key point 9) with the thermodynamic temperature.\n\n\n\n\nThe semi-classical treatment developed here is valid when the particles are partly distinguishable, i.e., when \\(d_{typ}\\) the typical distance between particles is much larger than the typical thermal de Broglie wavelength \\(\\lambda_{typ}\\):\n\\[\nd_{typ} \\gg \\lambda_{typ}\n\\]\nFor an ideal gas, this is equivalent to requiring the number density \\(n = N/V\\) be low, specifically:\n\\[\nn \\lambda_{typ}^3 \\ll 1\n\\]\nUsing \\(\\lambda_{typ} = \\frac{h}{\\sqrt{3MkT}}\\), this condition becomes:\n\\[\nn \\left( \\frac{h}{\\sqrt{3MkT}} \\right)^3 \\ll 1 \\quad \\Rightarrow \\quad n \\left( \\frac{h^2}{2\\pi MkT} \\right)^{3/2} \\ll 1\n\\]\nThis inequality is often expressed as \\(n \\lambda_{typ}^3 \\ll 1\\), known as the classical limit or dilute gas limit. When this condition is violated, quantum effects become important, and we must turn to quantum statistics (Bose-Einstein or Fermi-Dirac statistics).\nThe semi-classical treatment breaks down when the temperature is very low or the density is very high because, under these conditions, the wavefunctions of individual particles overlap, and quantum indistinguishability becomes essential.\n\n\n\n\n\n\nFigure 2: Criterion for assessing validity of semi-classical treatment. Reproduced from Baierlein Fig 5.5\n\n\n\n\n\n\nIn this chapter we completed the calculation of \\(Z(1)\\) for an ideal gas and introduced the concept of a density of states. We discussed the validity of the semi-classical treatment, noting that it is applicable in the low-density limit where the wave packets of individual particles do not overlap. When the typical particle separation becomes comparable to or smaller than the de Broglie wavelength, quantum effects must be considered, and the semi-classical treatment breaks down.\nNext we will explore quantum statistics and examine how the behavior of ideal gases changes in the high-density regime where quantum effects become significant."
  },
  {
    "objectID": "chapter10.html#calculation-of-z1",
    "href": "chapter10.html#calculation-of-z1",
    "title": "10. Ideal gas in the low density limit",
    "section": "",
    "text": "We first finish the job begun in the last chapter:\n\\[\nZ(1) = \\pi \\int_0^\\infty \\exp\\left(-an^2\\right) n^2 dn \\quad \\text{where} \\quad a = \\frac{\\hbar^2 \\pi^2}{2kTML^2}\n\\]\nThe Gaussian integral that should be familiar is:\n\\[\n\\int_{-\\infty}^\\infty \\exp(-an^2) dn = \\left(\\frac{\\pi}{a}\\right)^{1/2}\n\\]\nThus, we see:\n\\[\nZ(1) = -\\frac{1}{2} \\frac{d}{da} \\left( \\int_{-\\infty}^\\infty \\exp(-an^2) dn \\right) = -\\frac{1}{2} \\frac{d}{da} \\left(\\frac{\\pi}{a}\\right)^{1/2} = \\frac{\\sqrt{\\pi}}{4a^{3/2}}\n\\]\nThe factor \\(\\frac{1}{2}\\) comes from the limit \\(0\\) to \\(\\infty\\) in the integral for \\(Z(1)\\). Inserting the expression for \\(a\\) gives:\n\\[\nZ(1) = \\frac{\\pi^{3/2}}{8} \\left( \\frac{2ML^2}{\\beta \\hbar^2 \\pi^2} \\right)^{3/2} = V \\left( \\frac{2\\pi M k T}{h^2} \\right)^{3/2}\n\\]\nwhere \\(h = 2\\pi \\hbar\\) has been used."
  },
  {
    "objectID": "chapter10.html#density-of-states",
    "href": "chapter10.html#density-of-states",
    "title": "10. Ideal gas in the low density limit",
    "section": "",
    "text": "Consider again the approximation of a sum by an integral:\n\\[\n\\sum_{n_x,n_y,n_z} A(n) \\rightarrow \\frac{1}{8} \\int_0^\\infty A(n) 4\\pi n^2 dn\n\\]\nwhere \\(A(n)\\) is any function of \\(n\\). Now change variables to \\(k\\) (see 9.3):\n\\[\nn = \\frac{L}{\\pi} k \\quad dn = \\frac{L}{\\pi} dk\n\\]\nand we find:\n\\[\n\\sum_{n_x,n_y,n_z} A(n) \\rightarrow \\frac{1}{8} \\int A(k) \\frac{4\\pi L^2 k^2}{\\pi^2} \\frac{L}{\\pi} dk = \\int A(k) \\Gamma(k) dk\n\\]\nwhere:\n\\[\n\\Gamma(k) dk = \\frac{k^2}{2\\pi^2} V dk\n\\] is the number of states with \\(k\\) between \\(k\\) and \\(k + dk\\).\n\n\n\n\n\n\n\\(\\Gamma(k)\\)is known as the “density of states” (here, in \\(k\\) space).\n\n\n\nSimilarly, we can change variables to \\(\\epsilon\\):\n\\[\n\\epsilon = \\frac{\\hbar^2 k^2}{2M}\\Rightarrow \\quad k = \\left(\\frac{2M \\epsilon}{\\hbar^2}\\right)^{1/2}\\Rightarrow \\quad dk = \\frac{dk}{d\\epsilon}d\\epsilon=\\frac{1}{2} \\left(\\frac{2M}{\\hbar^2}\\right)^{1/2} \\epsilon^{-1/2} d\\epsilon\n\\]\nNow, to obtain the density of states in energy space, denoted here by \\(g(\\epsilon)\\), equate:\n\\[\n\\Gamma(k) dk = g(\\epsilon) d\\epsilon\n\\]\nand change variables:\n\\[\ng(\\epsilon) d\\epsilon = \\left(\\frac{2M}{\\hbar^2}\\right)^{3/2} \\frac{V}{4\\pi^2} \\epsilon^{1/2} d\\epsilon\n\\]\nThe meaning of the density of states \\(g(\\epsilon)\\) is that \\(g(\\epsilon) d\\epsilon\\) is the number of states with energy between \\(\\epsilon\\) and \\(\\epsilon + d\\epsilon\\).\nIf you’re unsure of this change of variables from \\(\\Gamma(k)\\) to \\(g(\\epsilon)\\), you can carry it out as follows: Consider for any function \\(A(k)\\):\n\\[\n\\int A(k) \\Gamma(k) dk = \\int A(k) \\Gamma(k) \\frac{dk}{d\\epsilon} d\\epsilon = \\int A(\\epsilon) g(\\epsilon) d\\epsilon\n\\]\nUsing the density of states, one can write, for example:\n\\[\nZ(1) = \\int_0^\\infty  g(\\epsilon) \\exp(-\\beta \\epsilon) d\\epsilon\n\\]\nNotice that the density of states increases with energy, meaning that at higher energies, more states are available to the particle.\n\n\n\n\n\n\nFigure 1: Density of states as a function of energy"
  },
  {
    "objectID": "chapter10.html#thermodynamic-variables",
    "href": "chapter10.html#thermodynamic-variables",
    "title": "10. Ideal gas in the low density limit",
    "section": "",
    "text": "In the semi-classical treatment developed over this and the previous chapter:\n\\[\nZ = \\frac{Z(1)^N}{N!} = \\frac{V^N}{N!} \\left( \\frac{2\\pi M k T}{h^2} \\right)^{3N/2}\n\\]\nWe now proceed to calculate the usual thermodynamic variables:\n\\[\nF = -kT \\ln Z = NkT \\left[ \\ln \\left( \\frac{N}{V} \\right) - 1 - \\frac{3}{2} \\ln \\left( \\frac{2\\pi M k T}{h^2} \\right) \\right]\n\\]\nwhere Stirling’s approximation for \\(\\ln N!\\) was used.\nEnergy:\n\\[\n\\overline{E} = k T^2 \\frac{\\partial \\ln Z}{\\partial T} = \\frac{3}{2} NkT\n\\]\nEntropy:\n\\[\nS = \\frac{\\overline{E} - F}{T} = Nk \\left[ \\ln \\left( \\frac{V}{N} \\left( \\frac{2\\pi M k T}{h^2} \\right)^{3/2} \\right) + \\frac{5}{2} \\right]\n\\]\nPressure:\n\\[\nP = - \\left( \\frac{\\partial F}{\\partial V} \\right)_T = \\frac{NkT}{V} \\quad \\text{(ideal gas equation of state)}\n\\]\nHeat capacity:\n\\[\nC_V = \\frac{3}{2} Nk \\quad \\text{(equipartition)}\n\\]\nNote that\n\nthe ideal gas law and equipartition of energy are recovered.\nThe recovery of the ideal gas law finally identifies our statistical mechanical definition of temperature (see key point 9) with the thermodynamic temperature."
  },
  {
    "objectID": "chapter10.html#validity-of-semi-classical-treatment",
    "href": "chapter10.html#validity-of-semi-classical-treatment",
    "title": "10. Ideal gas in the low density limit",
    "section": "",
    "text": "The semi-classical treatment developed here is valid when the particles are partly distinguishable, i.e., when \\(d_{typ}\\) the typical distance between particles is much larger than the typical thermal de Broglie wavelength \\(\\lambda_{typ}\\):\n\\[\nd_{typ} \\gg \\lambda_{typ}\n\\]\nFor an ideal gas, this is equivalent to requiring the number density \\(n = N/V\\) be low, specifically:\n\\[\nn \\lambda_{typ}^3 \\ll 1\n\\]\nUsing \\(\\lambda_{typ} = \\frac{h}{\\sqrt{3MkT}}\\), this condition becomes:\n\\[\nn \\left( \\frac{h}{\\sqrt{3MkT}} \\right)^3 \\ll 1 \\quad \\Rightarrow \\quad n \\left( \\frac{h^2}{2\\pi MkT} \\right)^{3/2} \\ll 1\n\\]\nThis inequality is often expressed as \\(n \\lambda_{typ}^3 \\ll 1\\), known as the classical limit or dilute gas limit. When this condition is violated, quantum effects become important, and we must turn to quantum statistics (Bose-Einstein or Fermi-Dirac statistics).\nThe semi-classical treatment breaks down when the temperature is very low or the density is very high because, under these conditions, the wavefunctions of individual particles overlap, and quantum indistinguishability becomes essential.\n\n\n\n\n\n\nFigure 2: Criterion for assessing validity of semi-classical treatment. Reproduced from Baierlein Fig 5.5"
  },
  {
    "objectID": "chapter10.html#summary",
    "href": "chapter10.html#summary",
    "title": "10. Ideal gas in the low density limit",
    "section": "",
    "text": "In this chapter we completed the calculation of \\(Z(1)\\) for an ideal gas and introduced the concept of a density of states. We discussed the validity of the semi-classical treatment, noting that it is applicable in the low-density limit where the wave packets of individual particles do not overlap. When the typical particle separation becomes comparable to or smaller than the de Broglie wavelength, quantum effects must be considered, and the semi-classical treatment breaks down.\nNext we will explore quantum statistics and examine how the behavior of ideal gases changes in the high-density regime where quantum effects become significant."
  },
  {
    "objectID": "chapter13.html",
    "href": "chapter13.html",
    "title": "13. The Ideal Fermi gas",
    "section": "",
    "text": "Blundell and Blundell chapter 30\n\n\nConsider the limit \\(e^{\\mu/kT} \\ll 1\\), i.e., \\(\\mu\\) large and negative. Then:\n\\[\n\\overline{n_i} = \\frac{1}{\\exp\\left(\\frac{\\epsilon_i - \\mu}{kT}\\right) \\pm 1} \\approx e^{-\\frac{\\epsilon_i - \\mu}{kT}}\n\\]\nfor both Fermi-Dirac (F-D) and Bose-Einstein (B-E) distributions.\nNow fix \\(\\mu(N,T)\\) through the constraint\n\\[\nN = \\sum_i \\overline{n_i} \\approx e^{\\mu/kT} \\sum_i e^{-\\epsilon_i/kT} = e^{\\mu/kT} Z(1)\n\\]\nwhere we have identified the canonical single particle partition function \\(Z(1)\\). Thus we see that in the limit \\(e^{\\mu/kT} \\ll 1\\):\n\\[\n\\frac{Z(1)}{N} \\gg 1\n\\]\nwhich, referring to chapter 10.1, 10.4, is the low density/high temperature limit where a semi-classical treatment is adequate.\n\n\n\nWe return now to the Fermi-Dirac distribution from key point 17 (chapter 12). The function \\(f_+\\) is often referred to simply as the Fermi function.\nConsider the limit \\(T \\to 0\\) (\\(\\beta \\to \\infty\\)):\n\\[\nf_+(\\epsilon) = \\frac{1}{\\exp\\left(\\frac{\\epsilon - \\mu}{kT}\\right) + 1} \\to\n\\begin{cases}\n1 & \\text{if } \\epsilon &lt; \\epsilon_f \\\\\n0 & \\text{if } \\epsilon &gt; \\epsilon_f\n\\end{cases}\n\\]\nwhere \\(\\epsilon_f\\) is the Fermi energy defined by:\n\n\n\n\n\n\nKey Point 18\n\n\n\n\\[\n\\epsilon_f = \\lim_{T \\to 0} \\mu(T)\n\\]\n\n\nYou should convince yourself that the Fermi-Dirac distribution is equivalent to the probability that a quantum state of energy \\(\\epsilon\\) is occupied.\n\n\n\n\n\n\nFigure 1: Fermi function at zero temperature\n\n\n\nIn Figure 1, we observe the following features:\n\nAll states up to \\(\\epsilon_f\\) are filled with probability 1.\nAll states above \\(\\epsilon_f\\) are empty.\nThis is very different from a classical gas where at zero temperature, all gas molecules would have zero energy.\nIt is a direct result of the exclusion principle, which leads to an “effective repulsion” between fermions.\n\nWe now calculate \\(\\epsilon_f\\). First, turn the sum over quantum states into an integral:\n\\[\nN = \\sum_i f_+(\\epsilon_i) \\approx \\int_0^{\\infty} d\\epsilon\\, g(\\epsilon) f_+(\\epsilon)\n\\]\nwhere \\(g(\\epsilon)\\) is the density of states, a concept first introduced in chapter 10. As a reminder:\n\\[\ng(\\epsilon) d\\epsilon = \\text{number of states in energy range } [\\epsilon, \\epsilon + d\\epsilon]\n\\]\nWe saw in chapter 10 that for spinless particles in a box:\n\\[\ng(\\epsilon) = D \\epsilon^{1/2}, \\quad D = \\left( \\frac{2m}{h^2} \\right)^{3/2} \\frac{1}{4\\pi^2}\n\\]\nIncorporating the idea of spin, each translational (“standing wave”) state corresponds to \\(2s + 1\\) states since the particle has \\(2s + 1\\) possible spin states. Therefore:\n\\[\ng(\\epsilon) = \\tilde{D} \\epsilon^{1/2}, \\quad \\tilde{D} = (2s + 1) D\n\\]\nThus:\n\\[\nN = \\int_0^{\\epsilon_f} \\tilde{D} \\epsilon^{1/2} d\\epsilon = \\frac{2}{3} \\tilde{D} \\epsilon_f^{3/2}\n\\]\nwhich implies:\n\\[\n\\epsilon_f = \\left( \\frac{3N}{2\\tilde{D}V} \\right)^{2/3}\n\\]\nor equivalently:\n\\[\n\\epsilon_f = \\frac{h^2}{2m} \\left( \\frac{6\\pi^2 N}{(2s + 1)V} \\right)^{2/3}\n\\]\nIn Question 4.5, it is shown that:\n\\[\nE = \\int_0^{\\epsilon_f} g(\\epsilon) \\epsilon d\\epsilon = \\frac{3}{5} N \\epsilon_f\n\\]\n\n\n\n\\(\\epsilon_f\\) decreases with the mass \\(M\\) of the fermion.\n\\(\\epsilon_f\\) increases with the number density \\(N/V\\).\n\\(\\epsilon_f\\) defines a characteristic temperature through \\(\\epsilon_f = kT_f\\).\nAt zero temperature, there is a finite energy per particle \\(\\epsilon = \\frac{3}{5} \\epsilon_f\\).\n\n\n\n\n\nNow consider the Fermi function at low but finite \\(T\\). The meaning of “low” will be specified shortly. Note that:\n\\[\nf_+(\\epsilon) = \\frac{1}{\\exp\\left(\\frac{\\epsilon - \\mu}{kT}\\right) + 1}\n\\]\napproaches 1 if \\((\\epsilon - \\mu)/kT \\ll -1\\) and 0 if \\((\\epsilon - \\mu)/kT \\gg 1\\), with \\(f_+(\\epsilon) = 1/2\\) when \\(\\epsilon = \\mu\\). The Fermi function is a sigmoid shape illustrated in Figure 2. It differs from the zero-temperature step-function only when \\(|\\epsilon - \\mu| \\sim O(kT)\\).\n\n\n\n\n\n\nFigure 2: Fermi function at zero temperature\n\n\n\nFor the Fermi function to retain its characteristic shape, we must have \\(kT \\ll \\mu(T) \\approx \\epsilon_f\\), implying \\(T \\ll T_f\\). We assume that for low \\(T\\):\n\\[\n\\mu(T) \\approx \\epsilon_f \\left( 1 - O\\left( \\frac{T^2}{T_f^2} \\right) \\right)\n\\]\nThus, the change in \\(\\mu\\) is second-order in \\(T/T_f\\) and can safely be ignored in the regime \\(T \\ll T_f\\).\nAn intuitive interpretation of Figure 2 is that at low temperatures, the general scenario is similar to that at zero temperature, where all states up to energy \\(\\epsilon_f\\) are filled. The difference is that states within energy \\(O(kT)\\) below \\(\\epsilon_f\\) are vacated with some probability, and previously empty states within \\(O(kT)\\) above \\(\\epsilon_f\\) are filled with some probability. In other words, some fermions are thermally excited above the Fermi energy.\nWe investigate the result of this thermal excitation by calculating the heat capacity. To avoid a complicated calculation required to get the exact result, we instead make a rough estimate (for a more careful argument, see Baierlein 9.1). We expect:\n\\[\nE(T) - E(0) \\sim N \\cdot \\frac{kT}{\\epsilon_f} \\cdot kT\n\\]\nTherefore, the heat capacity is approximately:\n\\[\nC_V \\sim \\frac{E(T) - E(0)}{T} \\sim \\frac{Nk^2T}{\\epsilon_f}\n\\]\nThe important point is that this is linear in \\(T\\). This contrasts with the classical gas, where \\(C_V\\) is a constant (equal to \\(3Nk/2\\))."
  },
  {
    "objectID": "chapter13.html#low-density-limit",
    "href": "chapter13.html#low-density-limit",
    "title": "13. The Ideal Fermi gas",
    "section": "",
    "text": "Consider the limit \\(e^{\\mu/kT} \\ll 1\\), i.e., \\(\\mu\\) large and negative. Then:\n\\[\n\\overline{n_i} = \\frac{1}{\\exp\\left(\\frac{\\epsilon_i - \\mu}{kT}\\right) \\pm 1} \\approx e^{-\\frac{\\epsilon_i - \\mu}{kT}}\n\\]\nfor both Fermi-Dirac (F-D) and Bose-Einstein (B-E) distributions.\nNow fix \\(\\mu(N,T)\\) through the constraint\n\\[\nN = \\sum_i \\overline{n_i} \\approx e^{\\mu/kT} \\sum_i e^{-\\epsilon_i/kT} = e^{\\mu/kT} Z(1)\n\\]\nwhere we have identified the canonical single particle partition function \\(Z(1)\\). Thus we see that in the limit \\(e^{\\mu/kT} \\ll 1\\):\n\\[\n\\frac{Z(1)}{N} \\gg 1\n\\]\nwhich, referring to chapter 10.1, 10.4, is the low density/high temperature limit where a semi-classical treatment is adequate."
  },
  {
    "objectID": "chapter13.html#ideal-fermi-gas",
    "href": "chapter13.html#ideal-fermi-gas",
    "title": "13. The Ideal Fermi gas",
    "section": "",
    "text": "We return now to the Fermi-Dirac distribution from key point 17 (chapter 12). The function \\(f_+\\) is often referred to simply as the Fermi function.\nConsider the limit \\(T \\to 0\\) (\\(\\beta \\to \\infty\\)):\n\\[\nf_+(\\epsilon) = \\frac{1}{\\exp\\left(\\frac{\\epsilon - \\mu}{kT}\\right) + 1} \\to\n\\begin{cases}\n1 & \\text{if } \\epsilon &lt; \\epsilon_f \\\\\n0 & \\text{if } \\epsilon &gt; \\epsilon_f\n\\end{cases}\n\\]\nwhere \\(\\epsilon_f\\) is the Fermi energy defined by:\n\n\n\n\n\n\nKey Point 18\n\n\n\n\\[\n\\epsilon_f = \\lim_{T \\to 0} \\mu(T)\n\\]\n\n\nYou should convince yourself that the Fermi-Dirac distribution is equivalent to the probability that a quantum state of energy \\(\\epsilon\\) is occupied.\n\n\n\n\n\n\nFigure 1: Fermi function at zero temperature\n\n\n\nIn Figure 1, we observe the following features:\n\nAll states up to \\(\\epsilon_f\\) are filled with probability 1.\nAll states above \\(\\epsilon_f\\) are empty.\nThis is very different from a classical gas where at zero temperature, all gas molecules would have zero energy.\nIt is a direct result of the exclusion principle, which leads to an “effective repulsion” between fermions.\n\nWe now calculate \\(\\epsilon_f\\). First, turn the sum over quantum states into an integral:\n\\[\nN = \\sum_i f_+(\\epsilon_i) \\approx \\int_0^{\\infty} d\\epsilon\\, g(\\epsilon) f_+(\\epsilon)\n\\]\nwhere \\(g(\\epsilon)\\) is the density of states, a concept first introduced in chapter 10. As a reminder:\n\\[\ng(\\epsilon) d\\epsilon = \\text{number of states in energy range } [\\epsilon, \\epsilon + d\\epsilon]\n\\]\nWe saw in chapter 10 that for spinless particles in a box:\n\\[\ng(\\epsilon) = D \\epsilon^{1/2}, \\quad D = \\left( \\frac{2m}{h^2} \\right)^{3/2} \\frac{1}{4\\pi^2}\n\\]\nIncorporating the idea of spin, each translational (“standing wave”) state corresponds to \\(2s + 1\\) states since the particle has \\(2s + 1\\) possible spin states. Therefore:\n\\[\ng(\\epsilon) = \\tilde{D} \\epsilon^{1/2}, \\quad \\tilde{D} = (2s + 1) D\n\\]\nThus:\n\\[\nN = \\int_0^{\\epsilon_f} \\tilde{D} \\epsilon^{1/2} d\\epsilon = \\frac{2}{3} \\tilde{D} \\epsilon_f^{3/2}\n\\]\nwhich implies:\n\\[\n\\epsilon_f = \\left( \\frac{3N}{2\\tilde{D}V} \\right)^{2/3}\n\\]\nor equivalently:\n\\[\n\\epsilon_f = \\frac{h^2}{2m} \\left( \\frac{6\\pi^2 N}{(2s + 1)V} \\right)^{2/3}\n\\]\nIn Question 4.5, it is shown that:\n\\[\nE = \\int_0^{\\epsilon_f} g(\\epsilon) \\epsilon d\\epsilon = \\frac{3}{5} N \\epsilon_f\n\\]\n\n\n\n\\(\\epsilon_f\\) decreases with the mass \\(M\\) of the fermion.\n\\(\\epsilon_f\\) increases with the number density \\(N/V\\).\n\\(\\epsilon_f\\) defines a characteristic temperature through \\(\\epsilon_f = kT_f\\).\nAt zero temperature, there is a finite energy per particle \\(\\epsilon = \\frac{3}{5} \\epsilon_f\\)."
  },
  {
    "objectID": "chapter13.html#low-temperature-behaviour",
    "href": "chapter13.html#low-temperature-behaviour",
    "title": "13. The Ideal Fermi gas",
    "section": "",
    "text": "Now consider the Fermi function at low but finite \\(T\\). The meaning of “low” will be specified shortly. Note that:\n\\[\nf_+(\\epsilon) = \\frac{1}{\\exp\\left(\\frac{\\epsilon - \\mu}{kT}\\right) + 1}\n\\]\napproaches 1 if \\((\\epsilon - \\mu)/kT \\ll -1\\) and 0 if \\((\\epsilon - \\mu)/kT \\gg 1\\), with \\(f_+(\\epsilon) = 1/2\\) when \\(\\epsilon = \\mu\\). The Fermi function is a sigmoid shape illustrated in Figure 2. It differs from the zero-temperature step-function only when \\(|\\epsilon - \\mu| \\sim O(kT)\\).\n\n\n\n\n\n\nFigure 2: Fermi function at zero temperature\n\n\n\nFor the Fermi function to retain its characteristic shape, we must have \\(kT \\ll \\mu(T) \\approx \\epsilon_f\\), implying \\(T \\ll T_f\\). We assume that for low \\(T\\):\n\\[\n\\mu(T) \\approx \\epsilon_f \\left( 1 - O\\left( \\frac{T^2}{T_f^2} \\right) \\right)\n\\]\nThus, the change in \\(\\mu\\) is second-order in \\(T/T_f\\) and can safely be ignored in the regime \\(T \\ll T_f\\).\nAn intuitive interpretation of Figure 2 is that at low temperatures, the general scenario is similar to that at zero temperature, where all states up to energy \\(\\epsilon_f\\) are filled. The difference is that states within energy \\(O(kT)\\) below \\(\\epsilon_f\\) are vacated with some probability, and previously empty states within \\(O(kT)\\) above \\(\\epsilon_f\\) are filled with some probability. In other words, some fermions are thermally excited above the Fermi energy.\nWe investigate the result of this thermal excitation by calculating the heat capacity. To avoid a complicated calculation required to get the exact result, we instead make a rough estimate (for a more careful argument, see Baierlein 9.1). We expect:\n\\[\nE(T) - E(0) \\sim N \\cdot \\frac{kT}{\\epsilon_f} \\cdot kT\n\\]\nTherefore, the heat capacity is approximately:\n\\[\nC_V \\sim \\frac{E(T) - E(0)}{T} \\sim \\frac{Nk^2T}{\\epsilon_f}\n\\]\nThe important point is that this is linear in \\(T\\). This contrasts with the classical gas, where \\(C_V\\) is a constant (equal to \\(3Nk/2\\))."
  },
  {
    "objectID": "chapter8.html",
    "href": "chapter8.html",
    "title": "8 Einstein’s model of a simple solid",
    "section": "",
    "text": "Here we apply the Boltzmann distribution to the vibrational energy of atoms in a solid. In so doing we use a fundamental model of quantum physics—the quantum harmonic oscillator.\n\n\nSee Mandl 6.2\nRecall from your first year Properties of Matter course that in a crystalline solid, atoms sit on the sites of a regular array (lattice). A model for the vibrational motion of the atoms about their equilibrium positions (lattice sites) is to consider the atoms attached to each other by springs (see Figure 1 (a)). However this a (strongly) interacting system since energy is stored in interaction potentials (the springs) between atoms.\n\n\n\n\n\n\n\n\n\n\n\n(a) Model of crystal solid as atoms connect by harmonic springs.\n\n\n\n\n\n\n\n\n\n\n\n(b) Model of each atom as independent harmonic oscillator\n\n\n\n\n\n\n\nFigure 1\n\n\n\nWe seek a weakly interacting system that models the behaviour. A simple approximation is illustrated in Figure 1 (b). Each atom sits in its own harmonic potential and the motion of atoms independent of the others. This is the Einstein model.\nLet us first consider the system classically. Each oscillator has energy\n\\[\n\\varepsilon = \\frac{1}{2} \\kappa x^2 + \\frac{1}{2} mv^2\n\\]\nwhere \\(x\\) is the displacement from equilibrium position. Thus in 3d the system has \\(N \\times 3 \\times 2\\) quadratic contributions (\\(N\\) particles, \\(3\\) dimensions, P.E. + K.E.) to the energy. The equipartition principle then implies that the energy and heat capacity of the system should be\n\\[\n\\overline{E} = 6N \\times \\frac{1}{2} kT = 3NkT\n\\]\n\\[\nC_V = \\frac{\\partial \\overline{E}}{\\partial T} = 3Nk\n\\]\nIf you have forgotten the equipartition theorem you can find it in your first year Properties of Matter notes. However we will understand it at a deeper level in a few chapters’ time.\nThe prediction for the heat capacity was well borne out experimentally for many monoatomic solids and is known as the Dulong–Petit law. However diamond has a smaller heat capacity than the Dulong–Petit law predicts. Einstein showed that this is a quantum effect.\nTo understand why at a qualitative level quantum effects are important in diamond. Recall that diamond is very hard. This means that the ‘spring constant’ of the classical oscillator modelling each atom in Figure 1 is very large. Therefore the frequency of the oscillators is very large and and the typical displacements very small. This last fact implies that quantum effects have to be taken into account– roughly speaking, from the uncertainty principle if the displacements are very small then we know the atoms’ positions with high accuracy, therefore there must be uncertainty in the momenta (velocities).\n\n\n\nFrom quantum mechanics recall that a one dimensional harmonic oscillator has energy levels\n\\[\n\\varepsilon = \\left(n + \\frac{1}{2}\\right)\\hbar\\omega\n\\]\nwhere \\(n = 0, 1, 2 \\ldots\\)The ground state energy \\(n = 0\\) is \\(\\frac{1}{2}\\hbar\\omega\\). The generalisation to a three-dimensional oscillator is easy\n\\[\n\\varepsilon_{3d} = \\left(n_x + n_y + n_z + \\frac{3}{2}\\right)\\hbar\\omega\n\\]\nEinstein’s model is a system of \\(N\\) 3d quantum oscillators all with the same frequency \\(\\omega\\) in thermal equilibrium; \\(\\omega\\) is chosen to fit the experimental data.\nFrom the statistical mechanics of weakly interacting systems we have \\(Z = [Z(1)]^N\\). Now since the energy of an oscillator is a sum of three similar contributions we will have a further factorisation (see chapter 6.3) and\n\\[\nZ = [Z(1)]^N = \\left[Z_{1d}(1)\\right]^{3N}\n\\]\nwhere \\(Z_{1d}(1)\\) is the partition function for a single 1d oscillator. The task reduces to the calculation of \\(Z_{1d}(1)\\)\n\\[\nZ_{1d}(1) = \\sum_{n=0}^{\\infty} \\exp\\left(-\\beta\\hbar\\omega\\left[n + \\frac{1}{2}\\right]\\right)\n\\]\nTo evaluate the sum recall the geometric series\n\\[\n\\sum_{n=0}^{\\infty} a^n = \\frac{1}{1 - a}\n\\]\nThus\n\\[\nZ_{1d}(1) = \\frac{\\exp\\left(-\\frac{x}{2}\\right)}{1 - \\exp(-x)}\n\\]\nwhere \\(x = \\beta\\hbar\\omega\\).\n\n\n\nWe now turn the handle to crank out the thermodynamics variables. First note\n\\[\n\\overline{E} = 3N\\overline{\\varepsilon} = 3N\\hbar\\omega\\left(\\overline{n} + \\frac{1}{2}\\right)\n\\]\nIt is easiest to calculate \\(\\varepsilon\\) directly from the relation\n\\[\n\\overline{\\varepsilon} = -\\frac{\\partial}{\\partial\\beta} \\ln Z_{1d}(1) = -\\frac{dx}{d\\beta} \\frac{\\partial}{\\partial x} \\ln Z_{1d}(1)\n\\]\n\\[\n= -\\hbar\\omega \\frac{\\partial}{\\partial x} \\left[-\\ln\\left(1 - \\exp(-x)\\right) - \\frac{x}{2}\\right]\n\\]\n\\[\n= \\hbar\\omega \\left[\\frac{\\exp(-x)}{1 - \\exp(-x)} + \\frac{1}{2}\\right]\n\\]\nand we see\n\\[\n\\overline{n} = \\frac{\\exp(-x)}{1 - \\exp(-x)} = \\frac{1}{\\exp(x) - 1}\n\\]\n\n\n\n\n\n\nExpand to see an alternative derivation\n\n\n\n\n\nAside: An alternative derivation is to use the Boltzmann distribution \\(P_n\\) for the levels \\(n\\) of a 1d oscillator explicitly\n\\[\n\\overline{n} = \\sum_{n=0}^{\\infty} P_n n = \\frac{1}{Z_{1d}(1)} \\sum_{n=0}^{\\infty} n \\exp\\left[-x\\left(n + \\frac{1}{2}\\right)\\right]\n\\]\nthen use the identity\n\\[\n\\sum_{n=0}^{\\infty} n a^n = a \\frac{d}{da} \\sum_{n=0}^{\\infty} a^n = a \\frac{d}{da} \\frac{1}{1 - a} = \\frac{a}{(1 - a)^2}\n\\]\n\n\n\nThe mean total energy is given by\n\\[\n\\overline{E} = 3N\\hbar\\omega \\left[\\frac{\\exp(-x)}{1 - \\exp(-x)} + \\frac{1}{2}\\right]\n\\]\nand the heat capacity is given by\n\\[\nC_V = \\left(\\frac{\\partial \\overline{E}}{\\partial T}\\right)_V = \\left(\\frac{\\partial x}{\\partial T}\\right)_\\omega \\left(\\frac{\\partial \\overline{E}}{\\partial x}\\right)_\\omega\n\\]\nNote that here the ‘constant volume’ constraint on the partial derivative is the same as the ‘constant \\(\\omega\\)’ constraint. This is because the only way volume (or any other variable except temperature) can enter into the model is through the parameter \\(\\omega\\).\nWe find\n\\[\nC_V = -3N \\frac{\\hbar\\omega}{k T^2} \\frac{d}{dx} \\left[\\frac{1}{\\exp(x) - 1}\\right] = 3Nk \\frac{x^2 \\exp(x)}{(\\exp(x) - 1)^2}\n\\]\n\n\n\nTo define the high temperature and low temperature regimes we define a characteristic temperature \\(T^*\\) defined by \\(x = 1\\) i.e. when \\(kT\\) equals the excitation energy \\(\\hbar\\omega\\)\n\\[\nT^* = \\frac{\\hbar\\omega}{k}\n\\]\nThus in the high temperature regime \\(T \\gg T^\\star\\) (\\(x \\ll 1\\)) we find\n\\[\n\\overline{n} \\approx \\frac{1}{1 + x \\ldots - 1} \\approx \\frac{1}{x} = \\frac{kT}{\\hbar\\omega}\n\\]\n\\[\n\\overline{E} \\approx 3NkT + \\frac{3}{2}N\\hbar\\omega\n\\]\nwe see that \\(\\overline{n}\\) and hence \\(\\overline{\\varepsilon}\\) and \\(\\overline{E}\\) increase linearly with temperature. Moreover the heat capacity becomes\n\\[\nC_V = \\left(\\frac{\\partial \\overline{E}}{\\partial T}\\right)_V \\approx 3Nk\n\\]\nand we recover the Dulong–Petit law.\nIn the low temperature regime \\(T \\ll T^\\star\\) (\\(x \\gg 1\\)) we have\n\\[\n\\overline{n} \\approx \\exp(-x)\n\\]\n\\[\n\\frac{C_V}{3Nk} \\approx x^2 \\exp(-x)\n\\]\nThe mean energy level \\(\\overline{n}\\) tends to zero meaning most oscillators are in the ground state; the heat capacity is very much less than \\(3Nk\\).\nThus when \\(x = \\frac{\\hbar\\omega}{kT}\\) is large, quantum effects, in particular the effect of a discrete gap between the ground state and first excited state, become important. The Einstein model quite successfully explained the experimentally observed heat capacity of diamond (see Mandl 6.2). However the very low temperature behaviour of the heat capacity was still not quite right. In fact a \\(T^3\\) dependence appeared to emerge. This can be explained by an elaborated version of the Einstein theory known as the Debye theory. In essence the central weakness of the Einstein model is its neglect of collective motion of atoms (so called phonons) which exhibit a spectrum of frequencies.\nWhen \\(x = \\frac{\\hbar\\omega}{kT}\\) is small we recover the ‘classical’ results i.e. results where Planck’s constant does not appear in the thermodynamic quantities (except as an arbitrary constant in the energy). Quite generally high temperature is the classical limit (if indeed one exists).",
    "crumbs": [
      "Chapters",
      "8. Einstein’s model of a simple solid"
    ]
  },
  {
    "objectID": "chapter8.html#simple-model-of-a-solid",
    "href": "chapter8.html#simple-model-of-a-solid",
    "title": "8 Einstein’s model of a simple solid",
    "section": "",
    "text": "See Mandl 6.2\nRecall from your first year Properties of Matter course that in a crystalline solid, atoms sit on the sites of a regular array (lattice). A model for the vibrational motion of the atoms about their equilibrium positions (lattice sites) is to consider the atoms attached to each other by springs (see Figure 1 (a)). However this a (strongly) interacting system since energy is stored in interaction potentials (the springs) between atoms.\n\n\n\n\n\n\n\n\n\n\n\n(a) Model of crystal solid as atoms connect by harmonic springs.\n\n\n\n\n\n\n\n\n\n\n\n(b) Model of each atom as independent harmonic oscillator\n\n\n\n\n\n\n\nFigure 1\n\n\n\nWe seek a weakly interacting system that models the behaviour. A simple approximation is illustrated in Figure 1 (b). Each atom sits in its own harmonic potential and the motion of atoms independent of the others. This is the Einstein model.\nLet us first consider the system classically. Each oscillator has energy\n\\[\n\\varepsilon = \\frac{1}{2} \\kappa x^2 + \\frac{1}{2} mv^2\n\\]\nwhere \\(x\\) is the displacement from equilibrium position. Thus in 3d the system has \\(N \\times 3 \\times 2\\) quadratic contributions (\\(N\\) particles, \\(3\\) dimensions, P.E. + K.E.) to the energy. The equipartition principle then implies that the energy and heat capacity of the system should be\n\\[\n\\overline{E} = 6N \\times \\frac{1}{2} kT = 3NkT\n\\]\n\\[\nC_V = \\frac{\\partial \\overline{E}}{\\partial T} = 3Nk\n\\]\nIf you have forgotten the equipartition theorem you can find it in your first year Properties of Matter notes. However we will understand it at a deeper level in a few chapters’ time.\nThe prediction for the heat capacity was well borne out experimentally for many monoatomic solids and is known as the Dulong–Petit law. However diamond has a smaller heat capacity than the Dulong–Petit law predicts. Einstein showed that this is a quantum effect.\nTo understand why at a qualitative level quantum effects are important in diamond. Recall that diamond is very hard. This means that the ‘spring constant’ of the classical oscillator modelling each atom in Figure 1 is very large. Therefore the frequency of the oscillators is very large and and the typical displacements very small. This last fact implies that quantum effects have to be taken into account– roughly speaking, from the uncertainty principle if the displacements are very small then we know the atoms’ positions with high accuracy, therefore there must be uncertainty in the momenta (velocities).",
    "crumbs": [
      "Chapters",
      "8. Einstein’s model of a simple solid"
    ]
  },
  {
    "objectID": "chapter8.html#statistical-mechanics-of-the-quantum-harmonic-oscillator",
    "href": "chapter8.html#statistical-mechanics-of-the-quantum-harmonic-oscillator",
    "title": "8 Einstein’s model of a simple solid",
    "section": "",
    "text": "From quantum mechanics recall that a one dimensional harmonic oscillator has energy levels\n\\[\n\\varepsilon = \\left(n + \\frac{1}{2}\\right)\\hbar\\omega\n\\]\nwhere \\(n = 0, 1, 2 \\ldots\\)The ground state energy \\(n = 0\\) is \\(\\frac{1}{2}\\hbar\\omega\\). The generalisation to a three-dimensional oscillator is easy\n\\[\n\\varepsilon_{3d} = \\left(n_x + n_y + n_z + \\frac{3}{2}\\right)\\hbar\\omega\n\\]\nEinstein’s model is a system of \\(N\\) 3d quantum oscillators all with the same frequency \\(\\omega\\) in thermal equilibrium; \\(\\omega\\) is chosen to fit the experimental data.\nFrom the statistical mechanics of weakly interacting systems we have \\(Z = [Z(1)]^N\\). Now since the energy of an oscillator is a sum of three similar contributions we will have a further factorisation (see chapter 6.3) and\n\\[\nZ = [Z(1)]^N = \\left[Z_{1d}(1)\\right]^{3N}\n\\]\nwhere \\(Z_{1d}(1)\\) is the partition function for a single 1d oscillator. The task reduces to the calculation of \\(Z_{1d}(1)\\)\n\\[\nZ_{1d}(1) = \\sum_{n=0}^{\\infty} \\exp\\left(-\\beta\\hbar\\omega\\left[n + \\frac{1}{2}\\right]\\right)\n\\]\nTo evaluate the sum recall the geometric series\n\\[\n\\sum_{n=0}^{\\infty} a^n = \\frac{1}{1 - a}\n\\]\nThus\n\\[\nZ_{1d}(1) = \\frac{\\exp\\left(-\\frac{x}{2}\\right)}{1 - \\exp(-x)}\n\\]\nwhere \\(x = \\beta\\hbar\\omega\\).",
    "crumbs": [
      "Chapters",
      "8. Einstein’s model of a simple solid"
    ]
  },
  {
    "objectID": "chapter8.html#thermodynamic-properties",
    "href": "chapter8.html#thermodynamic-properties",
    "title": "8 Einstein’s model of a simple solid",
    "section": "",
    "text": "We now turn the handle to crank out the thermodynamics variables. First note\n\\[\n\\overline{E} = 3N\\overline{\\varepsilon} = 3N\\hbar\\omega\\left(\\overline{n} + \\frac{1}{2}\\right)\n\\]\nIt is easiest to calculate \\(\\varepsilon\\) directly from the relation\n\\[\n\\overline{\\varepsilon} = -\\frac{\\partial}{\\partial\\beta} \\ln Z_{1d}(1) = -\\frac{dx}{d\\beta} \\frac{\\partial}{\\partial x} \\ln Z_{1d}(1)\n\\]\n\\[\n= -\\hbar\\omega \\frac{\\partial}{\\partial x} \\left[-\\ln\\left(1 - \\exp(-x)\\right) - \\frac{x}{2}\\right]\n\\]\n\\[\n= \\hbar\\omega \\left[\\frac{\\exp(-x)}{1 - \\exp(-x)} + \\frac{1}{2}\\right]\n\\]\nand we see\n\\[\n\\overline{n} = \\frac{\\exp(-x)}{1 - \\exp(-x)} = \\frac{1}{\\exp(x) - 1}\n\\]\n\n\n\n\n\n\nExpand to see an alternative derivation\n\n\n\n\n\nAside: An alternative derivation is to use the Boltzmann distribution \\(P_n\\) for the levels \\(n\\) of a 1d oscillator explicitly\n\\[\n\\overline{n} = \\sum_{n=0}^{\\infty} P_n n = \\frac{1}{Z_{1d}(1)} \\sum_{n=0}^{\\infty} n \\exp\\left[-x\\left(n + \\frac{1}{2}\\right)\\right]\n\\]\nthen use the identity\n\\[\n\\sum_{n=0}^{\\infty} n a^n = a \\frac{d}{da} \\sum_{n=0}^{\\infty} a^n = a \\frac{d}{da} \\frac{1}{1 - a} = \\frac{a}{(1 - a)^2}\n\\]\n\n\n\nThe mean total energy is given by\n\\[\n\\overline{E} = 3N\\hbar\\omega \\left[\\frac{\\exp(-x)}{1 - \\exp(-x)} + \\frac{1}{2}\\right]\n\\]\nand the heat capacity is given by\n\\[\nC_V = \\left(\\frac{\\partial \\overline{E}}{\\partial T}\\right)_V = \\left(\\frac{\\partial x}{\\partial T}\\right)_\\omega \\left(\\frac{\\partial \\overline{E}}{\\partial x}\\right)_\\omega\n\\]\nNote that here the ‘constant volume’ constraint on the partial derivative is the same as the ‘constant \\(\\omega\\)’ constraint. This is because the only way volume (or any other variable except temperature) can enter into the model is through the parameter \\(\\omega\\).\nWe find\n\\[\nC_V = -3N \\frac{\\hbar\\omega}{k T^2} \\frac{d}{dx} \\left[\\frac{1}{\\exp(x) - 1}\\right] = 3Nk \\frac{x^2 \\exp(x)}{(\\exp(x) - 1)^2}\n\\]",
    "crumbs": [
      "Chapters",
      "8. Einstein’s model of a simple solid"
    ]
  },
  {
    "objectID": "chapter8.html#high-and-low-temperature-behaviour",
    "href": "chapter8.html#high-and-low-temperature-behaviour",
    "title": "8 Einstein’s model of a simple solid",
    "section": "",
    "text": "To define the high temperature and low temperature regimes we define a characteristic temperature \\(T^*\\) defined by \\(x = 1\\) i.e. when \\(kT\\) equals the excitation energy \\(\\hbar\\omega\\)\n\\[\nT^* = \\frac{\\hbar\\omega}{k}\n\\]\nThus in the high temperature regime \\(T \\gg T^\\star\\) (\\(x \\ll 1\\)) we find\n\\[\n\\overline{n} \\approx \\frac{1}{1 + x \\ldots - 1} \\approx \\frac{1}{x} = \\frac{kT}{\\hbar\\omega}\n\\]\n\\[\n\\overline{E} \\approx 3NkT + \\frac{3}{2}N\\hbar\\omega\n\\]\nwe see that \\(\\overline{n}\\) and hence \\(\\overline{\\varepsilon}\\) and \\(\\overline{E}\\) increase linearly with temperature. Moreover the heat capacity becomes\n\\[\nC_V = \\left(\\frac{\\partial \\overline{E}}{\\partial T}\\right)_V \\approx 3Nk\n\\]\nand we recover the Dulong–Petit law.\nIn the low temperature regime \\(T \\ll T^\\star\\) (\\(x \\gg 1\\)) we have\n\\[\n\\overline{n} \\approx \\exp(-x)\n\\]\n\\[\n\\frac{C_V}{3Nk} \\approx x^2 \\exp(-x)\n\\]\nThe mean energy level \\(\\overline{n}\\) tends to zero meaning most oscillators are in the ground state; the heat capacity is very much less than \\(3Nk\\).\nThus when \\(x = \\frac{\\hbar\\omega}{kT}\\) is large, quantum effects, in particular the effect of a discrete gap between the ground state and first excited state, become important. The Einstein model quite successfully explained the experimentally observed heat capacity of diamond (see Mandl 6.2). However the very low temperature behaviour of the heat capacity was still not quite right. In fact a \\(T^3\\) dependence appeared to emerge. This can be explained by an elaborated version of the Einstein theory known as the Debye theory. In essence the central weakness of the Einstein model is its neglect of collective motion of atoms (so called phonons) which exhibit a spectrum of frequencies.\nWhen \\(x = \\frac{\\hbar\\omega}{kT}\\) is small we recover the ‘classical’ results i.e. results where Planck’s constant does not appear in the thermodynamic quantities (except as an arbitrary constant in the energy). Quite generally high temperature is the classical limit (if indeed one exists).",
    "crumbs": [
      "Chapters",
      "8. Einstein’s model of a simple solid"
    ]
  },
  {
    "objectID": "chapter14.html",
    "href": "chapter14.html",
    "title": "14. The ideal bose gas",
    "section": "",
    "text": "In this chapter, we will be dealing with bosons for which the relevant distribution is the Bose-Einstein distribution:\n\\[\nn(\\epsilon) = f_B(\\epsilon) = \\frac{1}{\\exp\\left(\\frac{\\epsilon - \\mu}{kT}\\right) - 1}\n\\]\nRecall that this gives the average number of bosons in a quantum state of energy \\(\\epsilon\\).\nFor bosons, there is no exclusion principle (so no restriction on the number of bosons in a quantum state); however, there are still strong effects of indistinguishability\n\n\nA quantum harmonic oscillator of frequency \\(\\omega\\) has energy levels:\n\\[\n\\epsilon_n = \\hbar \\omega \\left( n + \\frac{1}{2} \\right)\n\\]\nRecall from chapter 9 that for such an oscillator in contact with a heat reservoir:\n\\[\nn = \\frac{1}{\\exp\\left(\\frac{\\hbar \\omega}{kT}\\right) - 1}\n\\]\nComparing this with the Bose-Einstein distribution above, we see that the average energy level \\(n\\) of the quantum oscillator can be thought of as the average number of bosons in a quantum state with energy \\(\\hbar \\omega\\) and chemical potential \\(\\mu = 0\\).\nThus, quanta of energy can be treated as bosons. This is reasonable since there is no restriction on the number of quanta of energy in the oscillator, and the quanta of energy clearly cannot be labeled and are indistinguishable.\nThat \\(\\mu = 0\\) expresses the fact that there is no constraint on the number of quanta, i.e., the number of quanta is not conserved. Returning to a definition of \\(\\mu\\) from chapter 14:\n\\[\n\\mu = \\left(\\frac{\\partial F}{\\partial N}\\right)_{T, V}\n\\]\nwe see that \\(\\mu = 0\\) implies that we minimize the free energy with respect to \\(N\\) (number of quanta).\nNow, proceed to use the idea of quanta as bosons to view the wave/particle duality you will have encountered in quantum mechanics courses in a new light. For our purposes, we consider only standing waves.\n\n\n\nThe standing waves or normal modes of the system have discrete frequencies \\(\\nu\\) (or angular frequencies \\(\\omega\\)). We think of them as the quantum states.\nQuantum mechanics tells us energy is quantized, thus the energy \\(\\epsilon\\) in a mode is \\(n h \\nu + \\text{constant}\\).\nThese quanta can be treated as bosons; therefore, the average number of quanta in a quantum state is given by the B-E distribution \\(n(\\nu) = \\frac{1}{\\exp\\left(\\frac{h \\nu}{kT}\\right) - 1}\\). This will tell us how energy is distributed amongst the possible frequencies.\n\nIf you are unsure of how it all fits together, a simple example might help: consider the 1D wave equation\n\\[\n\\frac{1}{v^2} \\frac{\\partial^2 \\psi}{\\partial t^2} = \\frac{\\partial^2 \\psi}{\\partial x^2}\n\\]\nThis is solved (for boundary conditions that vanish at \\(0\\), \\(L\\)) by the substitution \\(\\psi = T(t) X(x)\\) with \\(X = \\sin(n \\pi x / L)\\):\n\\[\n\\frac{1}{v^2} \\frac{d^2 T}{dt^2} = - k^2 T\n\\]\nwhere \\(k = n \\pi / L\\), which is just the equation for a harmonic oscillator with angular frequency \\(\\omega = k v\\).\nTherefore, quantum mechanically, the wave modes should be treated as quantum oscillators with angular frequency \\(\\omega\\) or frequency \\(\\nu\\), which in turn can be treated as Bose states with the energy quanta as bosons.\n\n\n\n\nA “blackbody” is one that absorbs all incident radiation (none is reflected). Thus, when cold, a blackbody looks black. However, if the body is not to heat up indefinitely, it eventually must radiate as much energy as it absorbs. Thus, a hot blackbody does not look black because it is emitting radiation. The question is as to the spectrum of the emitted radiation.\nHistorically, this was an important question since empirically it seemed that the radiation spectrum depended only on temperature, not the composition of the black body. Also, a classical description ran into a problem known as the “ultraviolet catastrophe” to be discussed below.\nThe idealized model of blackbody radiation is to think of the radiation inside a cavity (like an oven) at temperature \\(T\\) as being in equilibrium. If a very small hole is drilled in the cavity, the radiation that escapes will be the same as blackbody radiation and will have the same spectrum as the radiation in equilibrium inside the cavity.\nThe radiation inside the cavity is just made up of electromagnetic standing waves (or normal modes) of frequency \\(\\nu\\). Treating the photons as bosons as above and using the B-E distribution, we have for the average energy in a mode of frequency \\(\\nu\\):\n\\[\n\\epsilon(\\nu) = h \\nu n(\\nu) = \\frac{h \\nu}{\\exp\\left(\\frac{h \\nu}{kT}\\right) - 1}\n\\]\nWe just need to calculate the density of modes in the same way we calculated the density of states for a free particle in a box. In fact, the calculation is very nearly identical. We reproduce it for completeness.\nConsider a cubic cavity side \\(L\\); to have the wave vanish at the boundaries, we must have \\(L \\nu_x = 2 n_x c\\).\nThe momentum component of the wave in the \\(x\\)-direction is\n\\[\np_x = \\frac{h \\nu_x}{c} = \\frac{h n_x}{2L}\n\\]\nand the magnitude of the momentum is\n\\[\np = \\frac{h}{2L} (n_x^2 + n_y^2 + n_z^2)^{1/2}\n\\]\nThus, from \\(\\nu = p c / h\\), the allowed value of \\(\\nu\\) is given by triplets \\(\\{n_x, n_y, n_z\\}\\). We now approximate the sum over all triplets as an integral in analogy with chapter 10.3. However, we also have two possible transverse polarizations of the electromagnetic wave, so an additional factor of two enters:\n\\[\n\\sum_{\\text{modes}} A(\\nu) \\rightarrow 2 \\times 8 \\int A(\\nu) 4 \\pi n^2 dn = 8 \\times \\int A(\\nu) 4 \\pi \\nu^2 L^3 \\frac{d\\nu}{c^3}\n\\]\nThe density of modes \\(g(\\nu)\\) is defined through:\n\\[\ng(\\nu) d\\nu = \\text{number of electromagnetic waves in frequency range } \\nu \\rightarrow \\nu + d\\nu = \\frac{8 \\pi \\nu^2 V d\\nu}{c^3}\n\\]\nThus, the energy in modes \\(\\nu \\rightarrow \\nu + d\\nu\\) is:\n\\[\n\\epsilon(\\nu) g(\\nu) d\\nu = \\frac{8 \\pi V h \\nu^3 d\\nu}{c^3 \\left(\\exp\\left(\\frac{h \\nu}{kT}\\right) - 1\\right)}\n\\]\nAnd the spatial energy density is given by the Planck distribution:\n\\[\nu(\\nu) = \\frac{8 \\pi h \\nu^3}{c^3 \\left(\\exp\\left(\\frac{h \\nu}{kT}\\right) - 1\\right)}\n\\]\nSometimes the distribution appears as a function of wavelength \\(\\lambda = c / \\nu\\), which is obtained by changing variables:\n\\[\nd\\nu = - \\frac{c}{\\lambda^2} d\\lambda\n\\]\n\\[\nu(\\lambda) d\\lambda = \\frac{8 \\pi h c}{\\lambda^5 \\left(\\exp\\left(\\frac{h c}{\\lambda kT}\\right) - 1\\right)} d\\lambda\n\\]\n\n\n\nAn important feature is that it only depends on the cavity through temperature, not through composition or shape (see Figure 1).\n\n\nFirst, consider the limit of low \\(\\nu\\) when \\(\\frac{h \\nu}{kT} \\ll 1\\):\n\\[\nu(\\nu) \\approx \\frac{8 \\pi k T \\nu^2}{c^3}\n\\]\nThis is a classical result since \\(h\\) does not appear (it can be obtained by purely classical arguments). However, if one tries to apply the formula for all frequencies, in particular high frequencies, the “ultraviolet catastrophe” occurs, whereby the higher the frequency, the more energy is stored—essentially, a blackbody radiator would kill us all with high-frequency radiation!\n\n\n\nThe correct high-frequency limit is from the Planck distribution when \\(\\frac{h \\nu}{kT} \\gg 1\\):\n\\[\nu(\\nu) \\approx \\frac{8 \\pi h \\nu^3}{c^3} \\exp\\left(- \\frac{h \\nu}{kT}\\right)\n\\]\nIn this formula, the negative exponential kills the cubic power in \\(\\nu\\) and the “ultraviolet catastrophe” is removed. This is a direct result of using a quantum picture for the normal modes in the cavity and was one of the first successes of quantum statistical mechanics."
  },
  {
    "objectID": "chapter14.html#quanta-as-bosons",
    "href": "chapter14.html#quanta-as-bosons",
    "title": "14. The ideal bose gas",
    "section": "",
    "text": "A quantum harmonic oscillator of frequency \\(\\omega\\) has energy levels:\n\\[\n\\epsilon_n = \\hbar \\omega \\left( n + \\frac{1}{2} \\right)\n\\]\nRecall from chapter 9 that for such an oscillator in contact with a heat reservoir:\n\\[\nn = \\frac{1}{\\exp\\left(\\frac{\\hbar \\omega}{kT}\\right) - 1}\n\\]\nComparing this with the Bose-Einstein distribution above, we see that the average energy level \\(n\\) of the quantum oscillator can be thought of as the average number of bosons in a quantum state with energy \\(\\hbar \\omega\\) and chemical potential \\(\\mu = 0\\).\nThus, quanta of energy can be treated as bosons. This is reasonable since there is no restriction on the number of quanta of energy in the oscillator, and the quanta of energy clearly cannot be labeled and are indistinguishable.\nThat \\(\\mu = 0\\) expresses the fact that there is no constraint on the number of quanta, i.e., the number of quanta is not conserved. Returning to a definition of \\(\\mu\\) from chapter 14:\n\\[\n\\mu = \\left(\\frac{\\partial F}{\\partial N}\\right)_{T, V}\n\\]\nwe see that \\(\\mu = 0\\) implies that we minimize the free energy with respect to \\(N\\) (number of quanta).\nNow, proceed to use the idea of quanta as bosons to view the wave/particle duality you will have encountered in quantum mechanics courses in a new light. For our purposes, we consider only standing waves.\n\n\n\nThe standing waves or normal modes of the system have discrete frequencies \\(\\nu\\) (or angular frequencies \\(\\omega\\)). We think of them as the quantum states.\nQuantum mechanics tells us energy is quantized, thus the energy \\(\\epsilon\\) in a mode is \\(n h \\nu + \\text{constant}\\).\nThese quanta can be treated as bosons; therefore, the average number of quanta in a quantum state is given by the B-E distribution \\(n(\\nu) = \\frac{1}{\\exp\\left(\\frac{h \\nu}{kT}\\right) - 1}\\). This will tell us how energy is distributed amongst the possible frequencies.\n\nIf you are unsure of how it all fits together, a simple example might help: consider the 1D wave equation\n\\[\n\\frac{1}{v^2} \\frac{\\partial^2 \\psi}{\\partial t^2} = \\frac{\\partial^2 \\psi}{\\partial x^2}\n\\]\nThis is solved (for boundary conditions that vanish at \\(0\\), \\(L\\)) by the substitution \\(\\psi = T(t) X(x)\\) with \\(X = \\sin(n \\pi x / L)\\):\n\\[\n\\frac{1}{v^2} \\frac{d^2 T}{dt^2} = - k^2 T\n\\]\nwhere \\(k = n \\pi / L\\), which is just the equation for a harmonic oscillator with angular frequency \\(\\omega = k v\\).\nTherefore, quantum mechanically, the wave modes should be treated as quantum oscillators with angular frequency \\(\\omega\\) or frequency \\(\\nu\\), which in turn can be treated as Bose states with the energy quanta as bosons."
  },
  {
    "objectID": "chapter14.html#blackbody-or-cavity-radiation",
    "href": "chapter14.html#blackbody-or-cavity-radiation",
    "title": "14. The ideal bose gas",
    "section": "",
    "text": "A “blackbody” is one that absorbs all incident radiation (none is reflected). Thus, when cold, a blackbody looks black. However, if the body is not to heat up indefinitely, it eventually must radiate as much energy as it absorbs. Thus, a hot blackbody does not look black because it is emitting radiation. The question is as to the spectrum of the emitted radiation.\nHistorically, this was an important question since empirically it seemed that the radiation spectrum depended only on temperature, not the composition of the black body. Also, a classical description ran into a problem known as the “ultraviolet catastrophe” to be discussed below.\nThe idealized model of blackbody radiation is to think of the radiation inside a cavity (like an oven) at temperature \\(T\\) as being in equilibrium. If a very small hole is drilled in the cavity, the radiation that escapes will be the same as blackbody radiation and will have the same spectrum as the radiation in equilibrium inside the cavity.\nThe radiation inside the cavity is just made up of electromagnetic standing waves (or normal modes) of frequency \\(\\nu\\). Treating the photons as bosons as above and using the B-E distribution, we have for the average energy in a mode of frequency \\(\\nu\\):\n\\[\n\\epsilon(\\nu) = h \\nu n(\\nu) = \\frac{h \\nu}{\\exp\\left(\\frac{h \\nu}{kT}\\right) - 1}\n\\]\nWe just need to calculate the density of modes in the same way we calculated the density of states for a free particle in a box. In fact, the calculation is very nearly identical. We reproduce it for completeness.\nConsider a cubic cavity side \\(L\\); to have the wave vanish at the boundaries, we must have \\(L \\nu_x = 2 n_x c\\).\nThe momentum component of the wave in the \\(x\\)-direction is\n\\[\np_x = \\frac{h \\nu_x}{c} = \\frac{h n_x}{2L}\n\\]\nand the magnitude of the momentum is\n\\[\np = \\frac{h}{2L} (n_x^2 + n_y^2 + n_z^2)^{1/2}\n\\]\nThus, from \\(\\nu = p c / h\\), the allowed value of \\(\\nu\\) is given by triplets \\(\\{n_x, n_y, n_z\\}\\). We now approximate the sum over all triplets as an integral in analogy with chapter 10.3. However, we also have two possible transverse polarizations of the electromagnetic wave, so an additional factor of two enters:\n\\[\n\\sum_{\\text{modes}} A(\\nu) \\rightarrow 2 \\times 8 \\int A(\\nu) 4 \\pi n^2 dn = 8 \\times \\int A(\\nu) 4 \\pi \\nu^2 L^3 \\frac{d\\nu}{c^3}\n\\]\nThe density of modes \\(g(\\nu)\\) is defined through:\n\\[\ng(\\nu) d\\nu = \\text{number of electromagnetic waves in frequency range } \\nu \\rightarrow \\nu + d\\nu = \\frac{8 \\pi \\nu^2 V d\\nu}{c^3}\n\\]\nThus, the energy in modes \\(\\nu \\rightarrow \\nu + d\\nu\\) is:\n\\[\n\\epsilon(\\nu) g(\\nu) d\\nu = \\frac{8 \\pi V h \\nu^3 d\\nu}{c^3 \\left(\\exp\\left(\\frac{h \\nu}{kT}\\right) - 1\\right)}\n\\]\nAnd the spatial energy density is given by the Planck distribution:\n\\[\nu(\\nu) = \\frac{8 \\pi h \\nu^3}{c^3 \\left(\\exp\\left(\\frac{h \\nu}{kT}\\right) - 1\\right)}\n\\]\nSometimes the distribution appears as a function of wavelength \\(\\lambda = c / \\nu\\), which is obtained by changing variables:\n\\[\nd\\nu = - \\frac{c}{\\lambda^2} d\\lambda\n\\]\n\\[\nu(\\lambda) d\\lambda = \\frac{8 \\pi h c}{\\lambda^5 \\left(\\exp\\left(\\frac{h c}{\\lambda kT}\\right) - 1\\right)} d\\lambda\n\\]"
  },
  {
    "objectID": "chapter14.html#limits-of-the-planck-distribution",
    "href": "chapter14.html#limits-of-the-planck-distribution",
    "title": "14. The ideal bose gas",
    "section": "",
    "text": "An important feature is that it only depends on the cavity through temperature, not through composition or shape (see Figure 1).\n\n\nFirst, consider the limit of low \\(\\nu\\) when \\(\\frac{h \\nu}{kT} \\ll 1\\):\n\\[\nu(\\nu) \\approx \\frac{8 \\pi k T \\nu^2}{c^3}\n\\]\nThis is a classical result since \\(h\\) does not appear (it can be obtained by purely classical arguments). However, if one tries to apply the formula for all frequencies, in particular high frequencies, the “ultraviolet catastrophe” occurs, whereby the higher the frequency, the more energy is stored—essentially, a blackbody radiator would kill us all with high-frequency radiation!\n\n\n\nThe correct high-frequency limit is from the Planck distribution when \\(\\frac{h \\nu}{kT} \\gg 1\\):\n\\[\nu(\\nu) \\approx \\frac{8 \\pi h \\nu^3}{c^3} \\exp\\left(- \\frac{h \\nu}{kT}\\right)\n\\]\nIn this formula, the negative exponential kills the cubic power in \\(\\nu\\) and the “ultraviolet catastrophe” is removed. This is a direct result of using a quantum picture for the normal modes in the cavity and was one of the first successes of quantum statistical mechanics."
  },
  {
    "objectID": "NotesFigs/notebooks/idealgas.html",
    "href": "NotesFigs/notebooks/idealgas.html",
    "title": "Statistical Physics",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n\nimport matplotlib.animation as animation\n\nfig, ax = plt.subplots()\nt = np.linspace(0, 3, 40)\ng = -9.81\nv0 = 12\nz = g * t**2 / 2 + v0 * t\n\nv02 = 5\nz2 = g * t**2 / 2 + v02 * t\n\nscat = ax.scatter(t[0], z[0], c=\"b\", s=5, label=f'v0 = {v0} m/s')\nline2 = ax.plot(t[0], z2[0], label=f'v0 = {v02} m/s')[0]\nax.set(xlim=[0, 3], ylim=[-4, 10], xlabel='Time [s]', ylabel='Z [m]')\nax.legend()\n\n\ndef update(frame):\n    # for each frame, update the data stored on each artist.\n    x = t[:frame]\n    y = z[:frame]\n    # update the scatter plot:\n    data = np.stack([x, y]).T\n    scat.set_offsets(data)\n    # update the line plot:\n    line2.set_xdata(t[:frame])\n    line2.set_ydata(z2[:frame])\n    return (scat, line2)\n\n\nani = animation.FuncAnimation(fig=fig, func=update, frames=40, interval=30)\nplt.show()\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport matplotlib\n#matplotlib.use('TkAgg')\n\n# P = nRT/V\n# n = N/N_A\n\n# Constants \nnum_particles = 100  #n\ncontainer_size = 10 # V\ntime_steps = 1000\nparticles_speed = 0.5 # ¬ kinda T\n\n# Initial positions and velocities of particles\npositions = np.random.rand(num_particles, 2)* container_size\nvelocities = np.random.rand(num_particles, 2)* particles_speed\n\n# Creating our plot\nfig, ax = plt.subplots()\nscatter = ax.scatter(positions[:, 0], positions[:, 1], marker='o')\nax.set_xlim(0, container_size)\nax.set_ylim(0, container_size)\nplt.gca().set_aspect('equal', adjustable='box')\n\n# Simulation Loop\ncollisions_array = []\n\nfor step in range(time_steps):\n    positions += velocities\n    n_collision = 0\n\n    for i in range(num_particles):\n        for j in range(2):\n            if positions[i, j] &lt; 0 or positions[i, j] &gt; container_size:\n                #Collision\n                velocities[i, j] *= -1\n                n_collision += 1\n\n    collisions_array = np.append(collisions_array, n_collision)\n\n    print(f\"N of collisions: {n_collision} - On average: {np.average(collisions_array):.2f}\", end='\\r')\n\n    if not plt.fignum_exists(fig.number):\n        break\n\n    scatter.set_offsets(positions)\n    plt.pause(0.001)\n\nplt.show()\n    \n\nN of collisions: 4 - On average: 4.00N of collisions: 6 - On average: 5.00"
  },
  {
    "objectID": "NotesFigs/notebooks/scalemagwhtfn.html",
    "href": "NotesFigs/notebooks/scalemagwhtfn.html",
    "title": "Statistical Physics",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the function s(x)\ndef s(x):\n    return - (1 - x) * np.log(1 - x) - x * np.log(x)\n\n# Generate x values in the range from 0 to 1\nx_values = np.linspace(0.01, 0.99, 100)  # Avoiding x = 0 and x = 1 for logarithmic function\n\n# Generate N values in the range from 1 to 10\nN_values = [2,4,8,16,32,64]\n\n# Create a grid of subplots\nfig, axes = plt.subplots(3, 2, figsize=(10, 15))\n\n# Flatten axes for easy iteration\naxes = axes.flatten()\n\n# Plot exp(N * s(x)) for each N\nfor i, N in enumerate(N_values):\n    # Compute corresponding y values\n    y_values = np.exp(N * s(x_values))\n\n    # Plot the function\n    axes[i].plot(x_values, y_values)\n    axes[i].set_title(f'N = {N}')\n    axes[i].set_xlabel('$n/N$')\n    axes[i].set_ylabel('$\\Omega(N,E)=\\exp(N \\cdot s(n/N))$')\n    axes[i].grid(True)\n\n# Adjust layout\nplt.tight_layout()\npath=\"/Users/phxnw/Dropbox/Statphys/Course_material/NotesFigs/scalemagweightfn.png\"\nplt.savefig(path,format=\"png\",dpi=1200)\nplt.show()"
  },
  {
    "objectID": "NotesFigs/notebooks/Untitled1.html",
    "href": "NotesFigs/notebooks/Untitled1.html",
    "title": "Statistical Physics",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the function s(x)\ndef s(x):\n    return - (1 - x) * np.log(1 - x) - x * np.log(x)\n\n# Generate x values in the range from 0 to 1\nx_values = np.linspace(0.01, 0.99, 100)  # Avoiding x = 0 and x = 1 for logarithmic function\n\n# Generate N values in the range from 1 to 10\nN_values = [2,4,8,16,32,64]\n\n# Create a grid of subplots\nfig, axes = plt.subplots(3, 2, figsize=(10, 15))\n\n# Flatten axes for easy iteration\naxes = axes.flatten()\n\n# Plot exp(N * s(x)) for each N\nfor i, N in enumerate(N_values):\n    # Compute corresponding y values\n    y_values = np.exp(N * s(x_values))\n\n    # Plot the function\n    axes[i].plot(x_values, y_values)\n    axes[i].set_title(f'N = {N}')\n    axes[i].set_xlabel('$x$')\n    axes[i].set_ylabel('$\\exp(N \\cdot s(x))$')\n    axes[i].grid(True)\n\n# Adjust layout\nplt.tight_layout()\npath=\"/Users/phxnw/Desktop/scalemagweightfn.png\"\nplt.savefig(path,format=\"png\")\nplt.show()"
  },
  {
    "objectID": "Coursework/assignment_March2025.html",
    "href": "Coursework/assignment_March2025.html",
    "title": "Coursework assignment: Investigating ferro- and para-magnetism in a two dimensional model magnet.",
    "section": "",
    "text": "In this coursework assignment, you will explore the two-dimensional (2d) square Ising model. This model is used to represent the interactions between spins (either pointing up or down) arranged on a two-dimensional lattice, which is relevant for understanding ferromagnetic materials. At lower temperatures, the system becomes ordered, meaning most of the spins align in the same direction. However, as the temperature rises above a certain critical point \\(T_c\\), the order is disrupted due to temperature fluctuations, leading to a random arrangement of spins (see Figure 1).\nThe objective of this project is to analyze this phase transition through numerical simulations using Monte Carlo simulation techniques. While some of the concepts we will use are theoretically advanced, we will focus on understanding them at a level that allows us to implement them in code. Further reading is encouraged for those interested in the more detailed theoretical background.\n\n\n\n\n\n\n\nFigure 1: Representation of spin alignment in the 2D Ising model.The arrows indicate spin-up or spin-down states. - (a) System below \\(T_c\\), where spins are ordered. - (b) System above \\(T_c\\), where spins are disordered."
  },
  {
    "objectID": "Coursework/assignment_March2025.html#the-2d-ising-model",
    "href": "Coursework/assignment_March2025.html#the-2d-ising-model",
    "title": "Coursework assignment: Investigating ferro- and para-magnetism in a two dimensional model magnet.",
    "section": "2.1 The 2D Ising Model",
    "text": "2.1 The 2D Ising Model\nWe consider a square lattice with \\(L\\times L\\) sites, where the spins at each site \\((i,j)\\) are denoted as \\(s_{i,j}\\), with possible values of \\(s_{i,j} = \\pm 1\\). The system’s energy, is given by:\n\\[\nE(S) = -\\frac{J}{2} \\sum_{i,j} s_{i,j} \\left( s_{i+1,j} + s_{i-1,j} + s_{i,j+1} + s_{i,j-1} \\right),\n\\tag{1}\\] where \\(J&gt;0\\) is the interaction strength between neighboring spins, and the factor \\(\\frac{1}{2}\\) avoids counting the same interactions twice. Here, \\(S = \\{s_{i,j}\\}\\) represents a particular microstate of spins, and we assume periodic boundary conditions, such that \\(s_{i+N_x,j} = s_{i,j}\\) and \\(s_{i,j+N_y} = s_{i,j}\\).\nThe partition function \\(Z(\\beta)\\), which sums over all possible microstates and describes the system’s statistical properties, is defined as:\n\\[\nZ(\\beta) = \\sum_{S} \\exp\\left( -\\beta E(S) \\right),\n\\tag{2}\\]\nwhere \\(\\beta = \\frac{1}{k T}\\), with \\(T\\) being the temperature and \\(k\\) the Boltzmann constant. The sum runs over all possible spin microstates \\(S\\).\nThe average (also called the `expectation’) value of an observable macrovariable \\(O\\) is given by:\n\\[\n\\overline O  = \\frac{1}{Z(\\beta)} \\sum_{S} O(S) \\exp\\left( -\\beta E(S) \\right).\n\\tag{3}\\]\nRelevant macrovariables for the Ising magnet are the total energy \\(E\\) defined above and the total magnetisation \\(M=\\sum_i s_i\\).\nThe total number of possible states \\(S\\) for a system of size \\(L \\times L\\) is \\(2^{L^2}\\). This makes direct computation of sums like Equation 3 impractical for even relatively small volumes. Therefore, we will introduce a statistical method to approximate these sums."
  },
  {
    "objectID": "Coursework/assignment_March2025.html#monte-carlo-simulation",
    "href": "Coursework/assignment_March2025.html#monte-carlo-simulation",
    "title": "Coursework assignment: Investigating ferro- and para-magnetism in a two dimensional model magnet.",
    "section": "2.2 Monte Carlo simulation",
    "text": "2.2 Monte Carlo simulation\nMonte Carlo methods are a class of algorithms that use repeated random sampling to obtain numerical solutions. For instance, integrals can be approximated using Monte Carlo techniques by generating \\({\\cal N}\\) random samples from the domain of integration \\(A\\) and evaluating the function at these points. For a 1D integral, the result is approximated as:\n\\[\nI = \\int_A dx \\, f(x) \\approx \\frac{1}{{\\cal N}} \\sum_{i=1}^{{\\cal N}} f(x_i),\n\\]\nwhere \\(x_i\\) are random points uniformly distributed in \\(A\\). In practice, while we cannot generate an infinite number of samples, using a large but finite \\({\\cal N}\\) still provides a good approximation.\nThis idea can also be applied to sums, like the one in Eq. (3). However, for the Ising model, where the sum runs over all possible spin microstates, generating uniformly distributed random samples is inefficient, as most microstates contribute negligibly to the partition function. A more efficient approach focuses on generating only those microstates that have a significant contribution. This technique is known as importance sampling.\nTo generate these important microstates, we use the Metropolis algorithm. Starting from an initial microstate, we generate the next microstate as follows:\n\nSelect a random spin \\(s_{i,j}\\) and flip it: \\(s_{i,j} \\to -s_{i,j}\\).\nThis changes the microstate from \\(S = \\{s_{1,1}, s_{1,2}, \\dots, s_{i,j}, \\dots, s_{L,L}\\}\\) to \\(S' = \\{s_{1,1}, s_{1,2}, \\dots, -s_{i,j}, \\dots, s_{L,L}\\}\\).\nCompute the energy change \\(\\Delta E = E(S') - E(S)\\).\nAccept the new microstate \\(S'\\) with the following probability:\n\n\\[\nW(s_{i,j} \\to -s_{i,j}) =\n\\begin{cases}\n1 & \\text{if } \\Delta E &lt; 0, \\\\\ne^{-\\beta \\Delta E} & \\text{otherwise}.\n\\end{cases}\n\\]\nIf the new microstate is rejected, the system remains in the current microstate \\(S\\). Note that in performing these updates it is convenient to define a dimensionless temperature \\(T=kT/J\\). Another way of thinking about this is that we are working with a system for which \\(J=k=1\\).\nA single update involves repeating these steps for all \\(L^2\\) spins in the lattice.\nThe expectation value of a macrovariable \\(O\\) can then be approximated as:\n\\[\n\\overline O \\approx \\frac{1}{{\\cal N}} \\sum_{n=0}^{\\cal N} O_n,\n\\]\nwhere \\(O_n\\) is the value of the macrovariable measured for the \\(n\\)-th microstate \\(S_n\\), and \\({\\cal N}\\) is the total number of microstates generated."
  },
  {
    "objectID": "Coursework/assignment_March2025.html#setup",
    "href": "Coursework/assignment_March2025.html#setup",
    "title": "Coursework assignment: Investigating ferro- and para-magnetism in a two dimensional model magnet.",
    "section": "3.1 Setup",
    "text": "3.1 Setup\nThis should be completed in the week prior to the release of the assignment to make sure that any technical problems are resolved.\n\nOn the PH20040 Blackboard page open the Resources and Tools tab\nScroll down to Notable and open it (if off campus make sure you have the UoB VPN enabled)\nSelect the Jupyter Notebook (Legacy) notebook server option\nWhen the notebook has opened click the +Gitrepo button\nUnder enter Git Repository insert: https://github.com/nbwilding/Ising-coursework\nPress the “clone” button. This will download a notebook called Ising.ipynb into Jupyter\nCheck that the program runs\nFamiliarise yourself with the main features of the program. Pay attention to how to change the temperature range and number of spins \\(N=L^2\\), and how the energy and magnetisation and their averages are calculated. Be aware that the program can take several minutes to run depending on the system size.\n\nYou should perform measurements of various quantities using the program as set out below. Instructions are given in italics."
  },
  {
    "objectID": "Coursework/assignment_March2025.html#microstates-as-a-function-of-temperature",
    "href": "Coursework/assignment_March2025.html#microstates-as-a-function-of-temperature",
    "title": "Coursework assignment: Investigating ferro- and para-magnetism in a two dimensional model magnet.",
    "section": "3.2 Microstates as a function of temperature",
    "text": "3.2 Microstates as a function of temperature\nUse the program to generate a sequence of microstates at a temperature \\(\\:T=3.5\\) for system size \\(L=15\\). Save a copy of a typical configuration for your report (eg via a screen grab).\nRepeat for a temperature \\(\\:T=1.5\\). Again save a copy of a typical configuration.\nComment on the qualitative differences between the microstates at the two temperatures."
  },
  {
    "objectID": "Coursework/assignment_March2025.html#macrostates-as-a-function-of-temperature-energy-and-magnetisation",
    "href": "Coursework/assignment_March2025.html#macrostates-as-a-function-of-temperature-energy-and-magnetisation",
    "title": "Coursework assignment: Investigating ferro- and para-magnetism in a two dimensional model magnet.",
    "section": "3.3 Macrostates as a function of temperature: Energy and magnetisation",
    "text": "3.3 Macrostates as a function of temperature: Energy and magnetisation\nRun the simulation at a series of temperatures in the range \\(\\:T=(1.5,3.5)\\) for system size \\(L=15\\). Show a plot of the average energy per spin \\(\\:\\overline{E}/N\\) as a function of temperature. Comment on the shape of the curve.\nShow a plot of the average magnetisation per spin \\(\\overline{M}/N\\) for the same temperature range and system size. Comment on the shape of the curve."
  },
  {
    "objectID": "Coursework/assignment_March2025.html#specific-heat-and-magnetic-susceptibilty",
    "href": "Coursework/assignment_March2025.html#specific-heat-and-magnetic-susceptibilty",
    "title": "Coursework assignment: Investigating ferro- and para-magnetism in a two dimensional model magnet.",
    "section": "3.4 Specific heat and magnetic susceptibilty",
    "text": "3.4 Specific heat and magnetic susceptibilty\nAs covered in lectures (chapter 5), the heat capacity can be calculated via measurements of the variance of the energy fluctuations:\n\\[\nC=\\frac{1}{kT^2} \\left[\\overline{E^2} - \\overline{E}^2\\right] = \\frac{(\\Delta E)^2}{kT^2}\n\\]\nIt turns out that the magnetic susceptibility in zero magnetic field \\(\\chi \\equiv \\left( \\frac{\\partial M}{\\partial H}\\right )_{H=0}\\) is given by the variance of the total magnetisation fluctuations\n\\[\n\\chi = \\frac{1}{kT} \\left[\\overline{M^2} - \\overline{M}^2\\right] = \\frac{(\\Delta M)^2}{kT}\n\\]\nModify the simulation code to calculate the specific heat capacity \\(\\:C_s=C/N\\) and the susceptibility per spin \\(\\:\\chi_s=\\chi/N\\). Plot the temperature dependence of \\(\\:C_s(T)\\) and \\(\\:\\chi_s(T)\\) for temperatures in the range \\(\\:T=(1.5,3.5)\\). Comment on the shape of these plots. (Note that for simplicity the Monte Carlo program sets the Boltzmann factor \\(\\:k=1\\), you can do this too for the calculation of \\(\\:C_s\\) and \\(\\:\\chi_s\\))"
  },
  {
    "objectID": "Coursework/assignment_March2025.html#the-ferromagnetic-paramagnetic-phase-transition",
    "href": "Coursework/assignment_March2025.html#the-ferromagnetic-paramagnetic-phase-transition",
    "title": "Coursework assignment: Investigating ferro- and para-magnetism in a two dimensional model magnet.",
    "section": "3.5 The ferromagnetic-paramagnetic phase transition",
    "text": "3.5 The ferromagnetic-paramagnetic phase transition\nYour investigations should be pointing to something interesting happening at around \\(T=2.3\\).\nIn fact the exact ‘critical’ temperature of this model is \\(T_c=2/(1+\\sqrt{2})\\approx 2.269\\). (Aside: Nobel laureate Lars Onsager provided an exact analytical solution of the properties of the Ising model in 1944.)\nBelow the critical temperature, there is a net long-ranged ordering of the system, ie on average more than half the spins point in the same direction. We say that the system is ferromagnetic as it has a spontaneous magnetic moment. Above the critical temperature, there is no net ordering and the system is a paramagnet.\nAt the critical temperature the microstates form interesting fractal patterns (for a really large simulation see here), and as the number of spins \\(N=L^2\\to\\infty\\), \\(C_s(T_c)\\) and \\(\\chi_s(T_c)\\) diverge to infinity.\nMake plots showing how the specific heat \\(\\:C_s(T)\\) varies with the number of sites for \\(L=5,10,20\\). Comment on the dependence with system size.\nNote that the program might take several minutes to run for \\(L=20\\). If your data points are very noisy try increasing the number of steps via the parameter “mcsteps”."
  },
  {
    "objectID": "Coursework/assignment_March2025.html#your-report",
    "href": "Coursework/assignment_March2025.html#your-report",
    "title": "Coursework assignment: Investigating ferro- and para-magnetism in a two dimensional model magnet.",
    "section": "4.0 Your report",
    "text": "4.0 Your report\nYou should produce a short skeleton report (upto 2 sides of A4) focussing on and summarising the results of the above investigations and your comments/observations on the findings.\nAs well as a title and your name, the report should be laid out using the headings above:\n\nMicrostates as a function of temperature\nMacrostates as a function of temperature: Energy and magnetisation\nSpecific heat and magnetic susceptibility\nThe ferromagnetic-paramagnetic phase transition\n\nSubmit your report in pdf format for grading via blackboard. The report will be marked out of 25, and will have the same weighting as all other courseworks in the unit."
  },
  {
    "objectID": "Coursework/Ising.html",
    "href": "Coursework/Ising.html",
    "title": "Statistical Physics",
    "section": "",
    "text": "## We import standard libraries that provide required functionality such as random numbers, averaging and plotting\n\nimport numpy as np\nfrom numpy.random import rand\nimport matplotlib.pyplot as plt\nfrom scipy.sparse import spdiags,linalg,eye\n\n\n##----------------------------------------------------------------------\n##  Here we define some functions that will be used in the main code\n##----------------------------------------------------------------------\n\ndef initialstate(L):\n    '''\n    Generates a random spin configuration for initial condition\n    '''\n    state = 2*np.random.randint(2, size=(L,L))-1\n    return state\n\n\ndef mcmove(config, beta):\n    '''\n    Here we perform successive Monte Carlo updates of the microstates. \n    We choose a spin at random and propose to flip it to the 'other' orientation. \n    The proposal is accepted or rejected depending on the energy change associated with the flip.\n    If the energy change of a flip is negative (ie the energy decreases) we always accept the proposed flip. \n    If the energy change is positive we accept the proposal with a probability dependent on the Boltzmann factor for the energy change.\n    '''\n\n    for i in range(L):\n        for j in range(L):\n                a = np.random.randint(0, L)\n                b = np.random.randint(0, L)\n                s =  config[a, b]\n                nb = config[(a+1)%L,b] + config[a,(b+1)%L] + config[(a-1)%L,b] + config[a,(b-1)%L]\n                cost = 2*s*nb\n\n                if cost &lt; 0:\n                    s *= -1\n                elif rand() &lt; np.exp(-cost*beta):\n                    s *= -1\n                config[a, b] = s\n    return config\n\n\ndef calcEnergy(config):\n    '''\n    Calculates the total energy of a given configuration, ie it calculates the energy macrostate associated with a given microstate.\n    '''\n    energy = 0\n\n    for i in range(len(config)):\n        for j in range(len(config)):\n            S = config[i,j]\n            nb = config[(i+1)%L, j] + config[i,(j+1)%L] + config[(i-1)%L, j] + config[i,(j-1)%L]\n            energy += -nb*S\n    return energy/2.  # to compensate for over-counting\n\n\ndef calcMag(config):\n    '''\n    Calculates the total magnetization of a given configuration, ie it calculates the magnetisation macrostate associated with a given microstate.\n    '''\n    mag = np.sum(config)\n    return mag\n\n\ndef microstatePlot(config, i, L, T):\n    ''' This modules plts the microstate once passed to it along with time etc '''\n    X, Y = np.meshgrid(range(L), range(L))\n    plt.pcolormesh(X, Y, config, cmap=plt.cm.viridis);\n    plt.title('Temperature={:.2f}, Time={}'.format(T,i)); \n    plt.axis('tight')\n    plt.show()\n\n\n#----------------------------------------------------------------------\n## Simulation parameters: Here we set (and can change) various parameters\n#----------------------------------------------------------------------\n\nL       = 15         #  size of the lattice, L x L\nNspins  = L*L\neqSteps = 2**9       #  number of MC sweeps (ie. updates of entire lattice) for equilibration\nmcSteps = 2**12      #  number of MC sweeps for calculation\nminTemp = 1.5        #  Minimum temperature for temperature loop \nmaxTemp = 3.5        #  Maximum temperature for temperature loop\nnt      = 20         #  number of temperatures between minTemp and maxTemp to loop over\n\nT  = np.linspace(minTemp, maxTemp, nt); #Temperature range to be investigated in nt steps\n\nE  = np.zeros(nt)  #Here we define arrays to hold our estimates of the average energy and total magnetisation at each temperature\nM  = np.zeros(nt)   \n\n#-----Modify here----\n#Declare arrays here called C_s and X_s to hold the average of the specific heat and susceptibility per spin. See how this was done for E,M above.  \n\n#------------------\n\n\n#----------------------------------------------------------------------\n#  Main Loop\n#----------------------------------------------------------------------\n\nfor tt in range(nt):\n    config = initialstate(L)         # initialise system in a random microstate\n\n    E1 = M1 = 0                      # Initialise counters for total energy and total magnetisation\n    \n    #------Modify here-------\n    # Similar to previous line, initialise counters for any quantity that might be needed for the calculation of heat capacity and magnetic susceptibility, see sec 3.4 of assignment. \n    #------Modify here--------\n\n    iT=1.0/T[tt]                     # Inverse temperature. For convenience we set Boltzmann's constant=1, so this is also beta. \n    iT2=iT*iT                        # Square of inverse temperature, needed for heat capacity measurements\n\n    for i in range(eqSteps):         # equilibrate\n        mcmove(config, iT)           # Monte Carlo 'moves' to smaple microstates at prescribed T \n    microstatePlot(config, i, L, T[tt])\n    \n    for i in range(mcSteps):\n        mcmove(config, iT)\n        Ene = calcEnergy(config)     # calculate the energy\n        Mag = calcMag(config)        # calculate the magnetisation\n\n        E1 = E1 + Ene\n        M1 = M1 + Mag\n        #------Modify here-------\n        # Add code here to accumulate in the counters initialised above, the quantities needed for the heat capacity and magnetic susceptibility, see sec 3.4 of assignment. \n        #------Modify here--------\n\n    # Calculate intensive per-spin average quantities \n    E[tt] = E1/(mcSteps*Nspins)\n    M[tt] = M1/(mcSteps*Nspins)\n \n    #----Modify here----------------\n    # Add code here to calculate the specfic heat capacity and magnetic susceptibility for each T. See sec 3.4 of the assignment. You will need iT,iT2.\n    #-------------------------------\n\n\n#----------------------------------------------------------------------\n#  plot the calculated values\n#----------------------------------------------------------------------\n\nf = plt.figure(figsize=(18, 18)); #\n\nsp =  f.add_subplot(2, 2, 1 );\nplt.scatter(T, E, s=50, marker='o', color='IndianRed')\nplt.xlabel(\"Temperature (T)\", fontsize=20);\nplt.ylabel(\"Average Energy per spin \", fontsize=20);         plt.axis('tight');\n\nsp =  f.add_subplot(2, 2, 2 );\nplt.scatter(T, abs(M), s=50, marker='o', color='RoyalBlue')\nplt.xlabel(\"Temperature (T)\", fontsize=20);\nplt.ylabel(\"Average magnetization per spin \", fontsize=20);   plt.axis('tight');\n\n#------Modify here---------\n# uncomment the lines below to add plots of your measurements of the specific heat capacity and susceptibilty per spin (stored in arrays C_s and X_s) as a function of T\n\n# sp =  f.add_subplot(2, 2, 3 );\n# plt.scatter(T, C_s, s=50, marker='o', color='IndianRed')\n# plt.xlabel(\"Temperature (T)\", fontsize=20);\n# plt.ylabel(\"Specific heat capacity\", fontsize=20);   plt.axis('tight');\n\n\n# sp =  f.add_subplot(2, 2, 4 );\n# plt.scatter(T, X_s, s=50, marker='o', color='RoyalBlue')\n# plt.xlabel(\"Temperature (T)\", fontsize=20);\n# plt.ylabel(\"Magnetic susceptibility per spin\", fontsize=20);   plt.axis('tight');\n#-------------------------"
  },
  {
    "objectID": "solutions_set_3.html",
    "href": "solutions_set_3.html",
    "title": "Solutions to problem set 3",
    "section": "",
    "text": "The Boltzmann distribution gives the probability \\(p_r\\) that a system will be found in a nominated microstate \\(r\\) of energy \\(E_r\\). Explicitly\n\\[\np_r = \\frac{1}{Z} e^{-\\beta E_r}\n\\]\nwhere\n\\[\nZ = \\sum_r e^{-\\beta E_r}\n\\]\nIf we choose two microstates with different energies, our system is always more likely to be found in the one with lower energy. The reason is that in accessing the higher energy microstate the system has to grab more energy from its environment, which means that its environment can access fewer microstates (has a smaller entropy). In other words, overall (taking into account system and environment), the higher energy microstate of the system can be accessed in fewer ways.\nWe can make this argument more explicit as follows. Consider two microstates of energies \\(E_1\\) and \\(E_2\\) respectively, and suppose that \\(E_1\\) is smaller than \\(E_2\\). When the system is in the microstate of energy \\(E_1\\), the surrounding bath has energy \\(E_b = E_c - E_1\\), where (as in section 3.2) we use the subscripts \\(b\\) and \\(c\\) to label respectively the bath and the composite system comprising the (physical) system and the bath. Thus, when the system is in the microstate 1, the bath may be in any of \\(\\Omega_b(E_c - E_1)\\) microstates. Similarly, when the system is in the microstate 2, the bath may be in any of \\(\\Omega_b(E_c - E_2)\\) microstates. Since the composite system is (can be thought of as being) isolated, each of its microstates is equally likely. It follows that\n\\[\n\\frac{\\text{probability system is in microstate 1}}{\\text{probability system is in microstate 2}} = \\frac{\\text{number of microstates open to composite system when system is in microstate 1}}{\\text{number of microstates open to composite system when system is in microstate 2}}\n\\]\n\\[\n= \\frac{1 \\times \\Omega_b(E_c - E_1)}{1 \\times \\Omega_b(E_c - E_2)} = \\frac{\\Omega_b(E_c - E_1)}{\\Omega_b(E_c - E_2)}\n\\]\nSince \\(\\Omega_b(E_b)\\) is an increasing function (whose gradient measures the inverse of the temperature) \\(\\Omega_b(E_c - E_1)\\) must be bigger than \\(\\Omega_b(E_c - E_2)\\) if, as supposed, \\(E_1\\) is smaller than \\(E_2\\). It follows that microstate 1 is the likelier.\n\n\n\nLabel the two states 1 and 2; let them have energies \\(\\varepsilon_1\\) and \\(\\varepsilon_2\\) respectively. The probability that a particle has energy \\(\\varepsilon_2\\) is just the probability that it will be found in the state 2. Thus\n\\[\np(\\varepsilon = \\varepsilon_2) = p_2 = \\frac{e^{-\\beta \\varepsilon_2}}{Z(1)}\n\\]\nSimilarly, the probability the particle has energy \\(\\varepsilon_1\\) is\n\\[\np(\\varepsilon = \\varepsilon_1) = p_1 = \\frac{e^{-\\beta \\varepsilon_1}}{Z(1)}\n\\]\nThus\n\\[\n\\frac{p(\\varepsilon = \\varepsilon_2)}{p(\\varepsilon = \\varepsilon_1)} = \\frac{p_2}{p_1} = \\frac{e^{-\\beta \\varepsilon_2}}{e^{-\\beta \\varepsilon_1}} = e^{-\\beta \\Delta \\varepsilon}\n\\]\nwhere \\(\\Delta \\varepsilon \\equiv \\varepsilon_2 - \\varepsilon_1\\). Inverting this equation gives\n\\[\nT = \\frac{\\Delta \\varepsilon}{k \\ln(p_1 / p_2)} = \\frac{\\Delta \\varepsilon}{k \\ln 2}\n\\]\nwhere we have set \\(p_1 / p_2 = 2\\). Setting \\(\\Delta \\varepsilon = 0.1 \\, \\text{eV}\\) gives \\(T = 1.68 \\times 10^3 \\, \\text{K}\\).\n\n\n\nNow we have to be a little more careful. In this case each particle has access to three states, two of which have the same energy. Label the three states 1, 2, and 3, with the understanding that\n\\[\n\\varepsilon_3 = \\varepsilon_2 = \\varepsilon_1 + \\Delta \\varepsilon\n\\]\nThen\n\\[\np(\\varepsilon = \\varepsilon_2) = p_2 + p_3 = 2p_2 = 2 \\times \\frac{e^{-\\beta \\varepsilon_2}}{Z(1)}\n\\]\nwhile\n\\[\np(\\varepsilon = \\varepsilon_1) = p_1 = \\frac{e^{-\\beta \\varepsilon_1}}{Z(1)}\n\\]\nThus, now\n\\[\n\\frac{p(\\varepsilon = \\varepsilon_2)}{p(\\varepsilon = \\varepsilon_1)} = \\frac{2 \\times p_2}{p_1} = 2 \\times e^{-\\beta \\Delta \\varepsilon}\n\\]\nand\n\\[\nT = \\frac{\\Delta \\varepsilon}{k \\ln(p(\\varepsilon = \\varepsilon_1) / 2p(\\varepsilon = \\varepsilon_2))} = \\frac{\\Delta \\varepsilon}{k \\ln 4} = \\frac{\\Delta \\varepsilon}{2k \\ln 2}\n\\]\nleading to \\(T = 864 \\, \\text{K}\\), half the temperature we found in the preceding question. We need a lower temperature to achieve the same proportion of particles with the higher energy because, in this case, there are more ways (twice as many!) in which each particle can have the higher energy.\n\n\n\nConsider the magnetic and the vibrational ‘aspects’ of a solid. The associated energy in microstate \\(r\\) can usually be written (to a good first approximation) as a sum:\n\\[\nE_r = E_{r_v} + E_{r_m}\n\\]\nwhere \\(r_v\\) and \\(r_m\\) label the vibrational and magnetic states implicit in the microstate label \\(r\\), and the two terms on the right-hand side give the energies associated with the two aspects.\nThen the partition function can be written as follows:\n\\[\nZ = \\sum_r e^{-\\beta E_r} = \\sum_{r_v} e^{-\\beta E_{r_v}} \\times \\sum_{r_m} e^{-\\beta E_{r_m}} = Z^v \\times Z^m\n\\]\nSo the partition function is just the product of the partition functions associated with the two aspects.\nThis result is a generalisation of the claim (Key Point 12) that the partition function of \\(N\\) weakly interacting particles is just the \\(N^{\\text{th}}\\) power of the partition function of one of them.\nIts utility lies in the fact that it allows us to deal separately with the different aspects of the behaviour of a complex system, calculating the partition function for each one in turn.\nThe result will, hopefully, seem intuitively reasonable: when we are considering one aspect, any other just acts as an auxiliary heat bath, a reservoir of energy with which the aspect of interest can exchange energy.\n\n\n\nThe Helmholtz free energy is defined as:\n\\[\nF = E - TS\n\\]\nTaking the derivative of \\(F\\) with respect to \\(E\\):\n\\[\n\\frac{dF}{dE} = \\frac{d}{dE} \\left( E - TS \\right)\n\\]\nIn general, \\(S = S(E)\\), and the temperature \\(T\\) is constant (for this derivation). Thus:\n\\[\\begin{eqnarray*}\n\\frac{dF}{dE} &=& \\frac{dE}{dE} - T \\frac{dS}{dE}\\\\\n& = & 1 - T \\frac{dS}{dE}\\\\\n\\end{eqnarray*}\\]\nThus the identification\n\\[\nT = \\left( \\frac{\\partial E}{\\partial S} \\right)^{-1}\n\\] minimises the free energy. This definition of temperature in terms of entropy is the one given in lectures.\n\n\n\nWhen applied to a system of weakly interacting particles, the Boltzmann theory tells us that the partition function can be written as \\(Z = Z(1)^N\\), where\n\\[\nZ(1) = \\sum_i e^{-\\beta \\varepsilon_i}\n\\]\nis the partition function for one particle, so that the sum extends over the states \\(i = 1, 2, \\dots q\\) of one particle, with energies denoted by \\(\\varepsilon_i\\), and we are told that the number of states \\(q\\) is finite. The probability of finding a (nominated) particle to be in a state \\(i\\) is then\n\\[\np_i = \\frac{e^{-\\beta \\varepsilon_i}}{Z(1)}\n\\]\nwhich one may view as a direct application of the original Boltzmann distribution, with the ‘system’ chosen as one particle, with all the other particles acting as a heat bath.\nThe basic physics is controlled by the limiting (low and high \\(T\\)) behaviour of the set of probabilities \\(p_i\\).\n\nFor low \\(T\\), a particle is most likely to be found in one of the lowest energy states; it will be in its ground state when \\(T \\ll \\Delta \\varepsilon / k\\) where \\(\\Delta \\varepsilon\\) is the energy spacing between the ground state and the first excited state.\nFor high enough \\(T\\) (specifically when \\(T \\gg (\\varepsilon_q - \\varepsilon_1)/k\\); convince yourself about this!) each particle is essentially equally likely to be in any one of its states.\n\nThe implications for \\(S\\), \\(E\\), and the heat capacity \\(C\\) should be clear:\n\nThe entropy evolves smoothly from zero, at \\(T = 0\\) (where the underlying picture is one of ‘order’, with everything in its ground state) to a finite upper limiting value at high \\(T\\) (associated with the ‘most disordered’ macrostate in which a particle is equally likely to be in any one of its states).\nThe energy \\(E\\) evolves in a qualitatively similar way, from a minimum at \\(T = 0\\) (just \\(N \\times \\varepsilon_1\\), the ground state energy for a single particle) to a finite upper limit (just \\(N \\times \\varepsilon_{\\text{av}}\\), where \\(\\varepsilon_{\\text{av}} = q^{-1} \\sum_{i=1}^{q} \\varepsilon_i\\) is the average of the one-particle energy levels).\nThe heat capacity \\(C\\) mirrors the slope of the \\(E\\)-versus-\\(T\\) curve and thus evolves from zero, to a peak, and back to zero, forming a hump.\n\n\n\nEntropy vs. Temperature: Shows a smooth increase from zero to a limiting value.\nMean Energy vs. Temperature: Starts at a minimum and approaches a finite upper limit.\nHeat Capacity vs. Temperature: Peaks at an intermediate temperature and approaches zero at both low and high temperatures."
  },
  {
    "objectID": "solutions_set_3.html#knowing-and-understanding-the-boltzmann-distribution",
    "href": "solutions_set_3.html#knowing-and-understanding-the-boltzmann-distribution",
    "title": "Solutions to problem set 3",
    "section": "",
    "text": "The Boltzmann distribution gives the probability \\(p_r\\) that a system will be found in a nominated microstate \\(r\\) of energy \\(E_r\\). Explicitly\n\\[\np_r = \\frac{1}{Z} e^{-\\beta E_r}\n\\]\nwhere\n\\[\nZ = \\sum_r e^{-\\beta E_r}\n\\]\nIf we choose two microstates with different energies, our system is always more likely to be found in the one with lower energy. The reason is that in accessing the higher energy microstate the system has to grab more energy from its environment, which means that its environment can access fewer microstates (has a smaller entropy). In other words, overall (taking into account system and environment), the higher energy microstate of the system can be accessed in fewer ways.\nWe can make this argument more explicit as follows. Consider two microstates of energies \\(E_1\\) and \\(E_2\\) respectively, and suppose that \\(E_1\\) is smaller than \\(E_2\\). When the system is in the microstate of energy \\(E_1\\), the surrounding bath has energy \\(E_b = E_c - E_1\\), where (as in section 3.2) we use the subscripts \\(b\\) and \\(c\\) to label respectively the bath and the composite system comprising the (physical) system and the bath. Thus, when the system is in the microstate 1, the bath may be in any of \\(\\Omega_b(E_c - E_1)\\) microstates. Similarly, when the system is in the microstate 2, the bath may be in any of \\(\\Omega_b(E_c - E_2)\\) microstates. Since the composite system is (can be thought of as being) isolated, each of its microstates is equally likely. It follows that\n\\[\n\\frac{\\text{probability system is in microstate 1}}{\\text{probability system is in microstate 2}} = \\frac{\\text{number of microstates open to composite system when system is in microstate 1}}{\\text{number of microstates open to composite system when system is in microstate 2}}\n\\]\n\\[\n= \\frac{1 \\times \\Omega_b(E_c - E_1)}{1 \\times \\Omega_b(E_c - E_2)} = \\frac{\\Omega_b(E_c - E_1)}{\\Omega_b(E_c - E_2)}\n\\]\nSince \\(\\Omega_b(E_b)\\) is an increasing function (whose gradient measures the inverse of the temperature) \\(\\Omega_b(E_c - E_1)\\) must be bigger than \\(\\Omega_b(E_c - E_2)\\) if, as supposed, \\(E_1\\) is smaller than \\(E_2\\). It follows that microstate 1 is the likelier."
  },
  {
    "objectID": "solutions_set_3.html#some-numbers",
    "href": "solutions_set_3.html#some-numbers",
    "title": "Solutions to problem set 3",
    "section": "",
    "text": "Label the two states 1 and 2; let them have energies \\(\\varepsilon_1\\) and \\(\\varepsilon_2\\) respectively. The probability that a particle has energy \\(\\varepsilon_2\\) is just the probability that it will be found in the state 2. Thus\n\\[\np(\\varepsilon = \\varepsilon_2) = p_2 = \\frac{e^{-\\beta \\varepsilon_2}}{Z(1)}\n\\]\nSimilarly, the probability the particle has energy \\(\\varepsilon_1\\) is\n\\[\np(\\varepsilon = \\varepsilon_1) = p_1 = \\frac{e^{-\\beta \\varepsilon_1}}{Z(1)}\n\\]\nThus\n\\[\n\\frac{p(\\varepsilon = \\varepsilon_2)}{p(\\varepsilon = \\varepsilon_1)} = \\frac{p_2}{p_1} = \\frac{e^{-\\beta \\varepsilon_2}}{e^{-\\beta \\varepsilon_1}} = e^{-\\beta \\Delta \\varepsilon}\n\\]\nwhere \\(\\Delta \\varepsilon \\equiv \\varepsilon_2 - \\varepsilon_1\\). Inverting this equation gives\n\\[\nT = \\frac{\\Delta \\varepsilon}{k \\ln(p_1 / p_2)} = \\frac{\\Delta \\varepsilon}{k \\ln 2}\n\\]\nwhere we have set \\(p_1 / p_2 = 2\\). Setting \\(\\Delta \\varepsilon = 0.1 \\, \\text{eV}\\) gives \\(T = 1.68 \\times 10^3 \\, \\text{K}\\)."
  },
  {
    "objectID": "solutions_set_3.html#and-some-thinking",
    "href": "solutions_set_3.html#and-some-thinking",
    "title": "Solutions to problem set 3",
    "section": "",
    "text": "Now we have to be a little more careful. In this case each particle has access to three states, two of which have the same energy. Label the three states 1, 2, and 3, with the understanding that\n\\[\n\\varepsilon_3 = \\varepsilon_2 = \\varepsilon_1 + \\Delta \\varepsilon\n\\]\nThen\n\\[\np(\\varepsilon = \\varepsilon_2) = p_2 + p_3 = 2p_2 = 2 \\times \\frac{e^{-\\beta \\varepsilon_2}}{Z(1)}\n\\]\nwhile\n\\[\np(\\varepsilon = \\varepsilon_1) = p_1 = \\frac{e^{-\\beta \\varepsilon_1}}{Z(1)}\n\\]\nThus, now\n\\[\n\\frac{p(\\varepsilon = \\varepsilon_2)}{p(\\varepsilon = \\varepsilon_1)} = \\frac{2 \\times p_2}{p_1} = 2 \\times e^{-\\beta \\Delta \\varepsilon}\n\\]\nand\n\\[\nT = \\frac{\\Delta \\varepsilon}{k \\ln(p(\\varepsilon = \\varepsilon_1) / 2p(\\varepsilon = \\varepsilon_2))} = \\frac{\\Delta \\varepsilon}{k \\ln 4} = \\frac{\\Delta \\varepsilon}{2k \\ln 2}\n\\]\nleading to \\(T = 864 \\, \\text{K}\\), half the temperature we found in the preceding question. We need a lower temperature to achieve the same proportion of particles with the higher energy because, in this case, there are more ways (twice as many!) in which each particle can have the higher energy."
  },
  {
    "objectID": "solutions_set_3.html#how-to-divide-and-conquer",
    "href": "solutions_set_3.html#how-to-divide-and-conquer",
    "title": "Solutions to problem set 3",
    "section": "",
    "text": "Consider the magnetic and the vibrational ‘aspects’ of a solid. The associated energy in microstate \\(r\\) can usually be written (to a good first approximation) as a sum:\n\\[\nE_r = E_{r_v} + E_{r_m}\n\\]\nwhere \\(r_v\\) and \\(r_m\\) label the vibrational and magnetic states implicit in the microstate label \\(r\\), and the two terms on the right-hand side give the energies associated with the two aspects.\nThen the partition function can be written as follows:\n\\[\nZ = \\sum_r e^{-\\beta E_r} = \\sum_{r_v} e^{-\\beta E_{r_v}} \\times \\sum_{r_m} e^{-\\beta E_{r_m}} = Z^v \\times Z^m\n\\]\nSo the partition function is just the product of the partition functions associated with the two aspects.\nThis result is a generalisation of the claim (Key Point 12) that the partition function of \\(N\\) weakly interacting particles is just the \\(N^{\\text{th}}\\) power of the partition function of one of them.\nIts utility lies in the fact that it allows us to deal separately with the different aspects of the behaviour of a complex system, calculating the partition function for each one in turn.\nThe result will, hopefully, seem intuitively reasonable: when we are considering one aspect, any other just acts as an auxiliary heat bath, a reservoir of energy with which the aspect of interest can exchange energy."
  },
  {
    "objectID": "solutions_set_3.html#minimising-the-free-energy",
    "href": "solutions_set_3.html#minimising-the-free-energy",
    "title": "Solutions to problem set 3",
    "section": "",
    "text": "The Helmholtz free energy is defined as:\n\\[\nF = E - TS\n\\]\nTaking the derivative of \\(F\\) with respect to \\(E\\):\n\\[\n\\frac{dF}{dE} = \\frac{d}{dE} \\left( E - TS \\right)\n\\]\nIn general, \\(S = S(E)\\), and the temperature \\(T\\) is constant (for this derivation). Thus:\n\\[\\begin{eqnarray*}\n\\frac{dF}{dE} &=& \\frac{dE}{dE} - T \\frac{dS}{dE}\\\\\n& = & 1 - T \\frac{dS}{dE}\\\\\n\\end{eqnarray*}\\]\nThus the identification\n\\[\nT = \\left( \\frac{\\partial E}{\\partial S} \\right)^{-1}\n\\] minimises the free energy. This definition of temperature in terms of entropy is the one given in lectures."
  },
  {
    "objectID": "solutions_set_3.html#a-qualitative-thinking-exercise",
    "href": "solutions_set_3.html#a-qualitative-thinking-exercise",
    "title": "Solutions to problem set 3",
    "section": "",
    "text": "When applied to a system of weakly interacting particles, the Boltzmann theory tells us that the partition function can be written as \\(Z = Z(1)^N\\), where\n\\[\nZ(1) = \\sum_i e^{-\\beta \\varepsilon_i}\n\\]\nis the partition function for one particle, so that the sum extends over the states \\(i = 1, 2, \\dots q\\) of one particle, with energies denoted by \\(\\varepsilon_i\\), and we are told that the number of states \\(q\\) is finite. The probability of finding a (nominated) particle to be in a state \\(i\\) is then\n\\[\np_i = \\frac{e^{-\\beta \\varepsilon_i}}{Z(1)}\n\\]\nwhich one may view as a direct application of the original Boltzmann distribution, with the ‘system’ chosen as one particle, with all the other particles acting as a heat bath.\nThe basic physics is controlled by the limiting (low and high \\(T\\)) behaviour of the set of probabilities \\(p_i\\).\n\nFor low \\(T\\), a particle is most likely to be found in one of the lowest energy states; it will be in its ground state when \\(T \\ll \\Delta \\varepsilon / k\\) where \\(\\Delta \\varepsilon\\) is the energy spacing between the ground state and the first excited state.\nFor high enough \\(T\\) (specifically when \\(T \\gg (\\varepsilon_q - \\varepsilon_1)/k\\); convince yourself about this!) each particle is essentially equally likely to be in any one of its states.\n\nThe implications for \\(S\\), \\(E\\), and the heat capacity \\(C\\) should be clear:\n\nThe entropy evolves smoothly from zero, at \\(T = 0\\) (where the underlying picture is one of ‘order’, with everything in its ground state) to a finite upper limiting value at high \\(T\\) (associated with the ‘most disordered’ macrostate in which a particle is equally likely to be in any one of its states).\nThe energy \\(E\\) evolves in a qualitatively similar way, from a minimum at \\(T = 0\\) (just \\(N \\times \\varepsilon_1\\), the ground state energy for a single particle) to a finite upper limit (just \\(N \\times \\varepsilon_{\\text{av}}\\), where \\(\\varepsilon_{\\text{av}} = q^{-1} \\sum_{i=1}^{q} \\varepsilon_i\\) is the average of the one-particle energy levels).\nThe heat capacity \\(C\\) mirrors the slope of the \\(E\\)-versus-\\(T\\) curve and thus evolves from zero, to a peak, and back to zero, forming a hump.\n\n\n\nEntropy vs. Temperature: Shows a smooth increase from zero to a limiting value.\nMean Energy vs. Temperature: Starts at a minimum and approaches a finite upper limit.\nHeat Capacity vs. Temperature: Peaks at an intermediate temperature and approaches zero at both low and high temperatures."
  },
  {
    "objectID": "chapter6.html",
    "href": "chapter6.html",
    "title": "6. Systems of weakly interacting constituents",
    "section": "",
    "text": "We have already discussed in chapter 3 the concept of weakly interacting particles (or constituents)—basically, we can write the energy of the system as a sum of contributions from each particle since no energy is stored in any interaction potential. The Boltzmann distribution is quite general and is not restricted to weakly interacting systems; the derivation of chapter 5 only required that the system interact weakly with its environment. However, it is weakly interacting systems that are most easily treated.\nFor a weakly interacting system, we can write the energy of a microstate labelled \\(r\\) as:\n\\[\nE_r = \\epsilon_{i_1} + \\epsilon_{i_2} + \\epsilon_{i_3} + \\cdots + \\epsilon_{i_N}\n\\]\nwhere \\(\\epsilon_{i_n}\\) is the energy of particle \\(n\\) which is in state \\(i_n\\).\nFor example, in the model magnet, the microstate is given by the states of all \\(N\\) dipoles; the state of each dipole \\(n\\) is either the ground state (\\(i_n = 1; \\epsilon_1 = -mH\\)) or the excited state (\\(i_n = 2; \\epsilon_2 = +mH\\)). The total energy is the sum of the energies of each dipole.\nRecalling the definition of \\(Z\\) from key point 10, we have:\n\\[\\begin{eqnarray*}\nZ &=& \\sum_r \\exp(-\\beta E_r)\\\\\n&=& \\sum_{i_1 \\cdots i_N} \\exp\\left(-\\beta \\left[\\epsilon_{i_1} + \\epsilon_{i_2} + \\cdots + \\epsilon_{i_N}\\right]\\right) \\\\\n&=& \\left[\\sum_{i_1} \\exp(-\\beta \\epsilon_{i_1})\\right] \\cdots \\left[\\sum_{i_N} \\exp(-\\beta \\epsilon_{i_N})\\right]\\\\\n&=& [Z(1)]^N\n\\end{eqnarray*}\\]\nwhere \\(Z(1)\\) is the partition function for a single particle, c.f. the single dipole calculation of chapter 5.2. You should make sure you follow each step in the above development.\nThe significance of this is that the partition function for the \\(N\\) particle system factorises into a product of \\(N\\) partition functions, one for each particle. This produces an immense simplification, basically because:\n\\[\n\\ln Z = \\ln \\left[Z(1)\\right]^N = N \\ln Z(1)\n\\]\nand \\(Z(1)\\) should be simple to calculate.\nAs an example, consider the mean energy which from chapter 6.1 or key point 12 is given by:\n\\[\n\\overline{E} = -\\frac{\\partial}{\\partial \\beta} \\ln Z = -N \\frac{\\partial}{\\partial \\beta} \\ln Z(1) = N\\bar{\\epsilon}\n\\]\nwhere \\(\\bar{\\epsilon}\\) is the average energy of a single dipole. The fact that the mean energy is the sum of the mean energies of each dipole is, of course, expected in the absence of (strong) interactions between particles.\nSimilarly,\n\\[\nF(T) = -kT \\ln Z = -NkT \\ln Z(1)\n\\]\nAnother consequence of the factorisation of \\(Z\\) is that if we are interested in the state of, say, particle 1, then we can ‘sum out’ the states of all the other particles 2 to \\(N\\):\n\\[\\begin{eqnarray*}\nP_{i_1} &=& \\sum_{i_2 \\cdots i_N} \\exp\\left(-\\beta \\left[\\epsilon_{i_1} + \\epsilon_{i_2} + \\cdots + \\epsilon_{i_N}\\right]\\right) \\times Z^{-1}\\\\\n& =& \\frac{\\exp(-\\beta \\epsilon_{i_1}) Z(1)^{N-1}}{Z(1)^N}\\\\\n& =& \\frac{\\exp(-\\beta \\epsilon_{i_1})}{Z(1)}\n\\end{eqnarray*}\\] This is identical to the probability distribution for a single particle discussed in 4.2.\n\n\n\n\n\n\nKey Point 13:\n\n\n\nIn a system of \\(N\\) weakly interacting, distinguishable particles, the system partition function is simply \\(Z = [Z(1)]^N\\) and the single particle probability distribution is \\(P_i = \\frac{\\exp(-\\beta \\epsilon_i)}{Z(1)}\\).\n\n\nThus we see that the problem is reduced to that of the single particle problem. The key thing to remember though is that for the single particle problem the energy, etc., fluctuate, whereas in the \\(N\\) particle system for large \\(N\\), the macroscopic variables are sharp and have well-defined thermodynamic values.\nN.B. the reason for the qualification ‘distinguishable’ will become apparent later when we address quantum particles.\n\n\n\nSee Mandl 3.1-3.3\nIn 5.2, we examined the single particle partition function, i.e., the partition function for a single dipole, so effectively we have already done the hard work. We saw:\n\\[\nZ(1) = 2 \\cosh (x) \\quad \\text{where} \\quad x = \\frac{mH}{kT}\n\\]\nUsing the general results for weakly interacting systems (previous subsection), we have:\n\\[\n\\overline{E} = N\\bar{\\epsilon} = -N \\frac{\\partial}{\\partial \\beta} \\ln Z(1) = -NmH \\frac{\\partial}{\\partial x} \\ln Z(1) = -NmH \\tanh (x)\n\\]\n\\[\\begin{eqnarray*}\nS(T) &=& k \\ln Z + \\frac{\\overline{E}}{T} \\\\\n     &=& Nk \\ln Z(1) + \\frac{N\\bar{\\epsilon}}{T} \\\\\n     &=& Nk \\left[\\ln (\\exp (x) + \\exp (-x)) - x \\tanh (x)\\right]\n\\end{eqnarray*}\\]\nAn important quantity in magnetism is the magnetisation \\(M\\), which counts the total magnet moment \\(M=(n_1-n_2)m\\), where \\(n_i (i=1,2)\\) are the number of dipoles in levels 1 (\\(\\epsilon=−mH\\)) and 2 (\\(\\epsilon=+mH)\\). The magnetisation is related to the energy by \\(E=-MH\\). Thus we have:\n\\[\n\\overline{M} = Nm \\tanh (x)\n\\]\nwe see that for low fields, which means \\(mH \\ll kT\\) so that \\(x\\) is small:\n\\[\n\\overline{M} = \\frac{Nm^2H}{kT}\n\\]\nand for high fields \\(mH \\gg kT\\) so that \\(x \\gg 1\\):\n\\[\n\\overline{M} \\approx Nm\n\\]\nLet us now consider the zero-field magnetic susceptibility \\(\\chi(H = 0)\\) defined as:\n\\[\n\\chi(H = 0) \\equiv \\left(\\frac{\\partial \\overline{M}}{\\partial H}\\right)\\bigg|_{H=0} = \\frac{Nm^2}{kT}\n\\]\nYou should understand that the susceptibility measures the response of the magnetisation of the system to a small externally applied field. It turns out that this response is actually related to the magnetisation fluctuations at zero field. This is analogous to the results for the specific heat in terms of the variance of the energy fluctuations.\nThe \\(\\frac{1}{T}\\) dependence of \\(\\chi\\) is known as the Curie law. It implies that the dipoles become more ‘susceptible’ to an external magnetic field at lower temperature. See Mandl 3.1 for a comparison with real experiments.\nFinally, let’s consider the heat capacity (at constant field):\n\\[\nC_H = \\left(\\frac{\\partial \\overline{E}}{\\partial T}\\right)_H = \\left(\\frac{\\partial x}{\\partial T}\\right)_H \\left(\\frac{\\partial \\overline{E}}{\\partial x}\\right)_H = Nk x^2 \\text{sech}^2(x)\n\\]\nNote the low-temperature (large \\(x\\)) behaviour. Recalling:\n\\[\n\\text{sech } (x) = \\frac{2}{\\exp (x) + \\exp (-x)} \\sim 2 \\exp (-x) \\text{ for large } x\n\\]\nwe see that the heat capacity vanishes for small \\(T\\).\nThis is a general feature. Basically, at zero temperature, all particles are in the ground state. One has to raise the temperature until \\(kT\\) is comparable with \\(2mH\\) (the energy difference to the excited state) before a significant number of dipoles are excited. Thus near \\(T = 0\\), the derivative of internal energy with respect to \\(T\\) is zero.\n\\(kT\\) is often referred to as the ‘thermal energy’.\n\n\n\nWe have seen in this chapter how the fact that for weakly interacting particles the energy is a sum of terms allows factorisation of \\(Z\\).\nA similar mechanism occurs in systems which are not necessarily weakly interacting, but whose energy may be expressed as a sum of contributions associated with different ‘aspects’ of its behaviour say \\(\\alpha, \\beta\\).\n\\[\nE = E_\\alpha + E_\\beta + \\cdots\n\\]\nThen if the microscopic ‘degrees of freedom’ which contribute to the different aspects are independent, we find that the partition function factorises:\n\\[\nZ = Z_\\alpha \\times Z_\\beta \\times \\cdots\n\\]\nA consequence of this is that the free energy is a sum of contributions from each aspect.\nFor example, in a real magnetic solid, contributions to the energy will come from vibrations of the atoms about their average positions (see chapter 8) as well as the energy from the interaction with a magnetic field discussed in this lecture. Thus we can write\n\\[\nE = E_{\\text{Vib}} + E_{\\text{Mag}}\n\\]\nWe have seen that the microscopic degrees of freedom which contribute to \\(E_{Mag}\\) are the dipole moments of the atoms.\nThe microscopic degrees of freedom which contribute to \\(E_{Vib}\\) will be something like the displacements from the atoms’ equilibrium positions, say. These degrees of freedom (d.o.f) are independent of the dipole moment degrees of freedom. Thus\n\\[\nZ = \\sum_{\\text{Vib d.o.f}} \\sum_{\\text{Mag d.o.f}} \\exp \\left( -\\beta [E_{\\text{Vib}} + E_{\\text{Mag}}] \\right) = Z_{\\text{Vib}} \\times Z_{\\text{Mag}}.\n\\]",
    "crumbs": [
      "Chapters",
      "6. Systems of weakly interacting constituents"
    ]
  },
  {
    "objectID": "chapter6.html#factorisation-of-the-partition-function",
    "href": "chapter6.html#factorisation-of-the-partition-function",
    "title": "6. Systems of weakly interacting constituents",
    "section": "",
    "text": "We have already discussed in chapter 3 the concept of weakly interacting particles (or constituents)—basically, we can write the energy of the system as a sum of contributions from each particle since no energy is stored in any interaction potential. The Boltzmann distribution is quite general and is not restricted to weakly interacting systems; the derivation of chapter 5 only required that the system interact weakly with its environment. However, it is weakly interacting systems that are most easily treated.\nFor a weakly interacting system, we can write the energy of a microstate labelled \\(r\\) as:\n\\[\nE_r = \\epsilon_{i_1} + \\epsilon_{i_2} + \\epsilon_{i_3} + \\cdots + \\epsilon_{i_N}\n\\]\nwhere \\(\\epsilon_{i_n}\\) is the energy of particle \\(n\\) which is in state \\(i_n\\).\nFor example, in the model magnet, the microstate is given by the states of all \\(N\\) dipoles; the state of each dipole \\(n\\) is either the ground state (\\(i_n = 1; \\epsilon_1 = -mH\\)) or the excited state (\\(i_n = 2; \\epsilon_2 = +mH\\)). The total energy is the sum of the energies of each dipole.\nRecalling the definition of \\(Z\\) from key point 10, we have:\n\\[\\begin{eqnarray*}\nZ &=& \\sum_r \\exp(-\\beta E_r)\\\\\n&=& \\sum_{i_1 \\cdots i_N} \\exp\\left(-\\beta \\left[\\epsilon_{i_1} + \\epsilon_{i_2} + \\cdots + \\epsilon_{i_N}\\right]\\right) \\\\\n&=& \\left[\\sum_{i_1} \\exp(-\\beta \\epsilon_{i_1})\\right] \\cdots \\left[\\sum_{i_N} \\exp(-\\beta \\epsilon_{i_N})\\right]\\\\\n&=& [Z(1)]^N\n\\end{eqnarray*}\\]\nwhere \\(Z(1)\\) is the partition function for a single particle, c.f. the single dipole calculation of chapter 5.2. You should make sure you follow each step in the above development.\nThe significance of this is that the partition function for the \\(N\\) particle system factorises into a product of \\(N\\) partition functions, one for each particle. This produces an immense simplification, basically because:\n\\[\n\\ln Z = \\ln \\left[Z(1)\\right]^N = N \\ln Z(1)\n\\]\nand \\(Z(1)\\) should be simple to calculate.\nAs an example, consider the mean energy which from chapter 6.1 or key point 12 is given by:\n\\[\n\\overline{E} = -\\frac{\\partial}{\\partial \\beta} \\ln Z = -N \\frac{\\partial}{\\partial \\beta} \\ln Z(1) = N\\bar{\\epsilon}\n\\]\nwhere \\(\\bar{\\epsilon}\\) is the average energy of a single dipole. The fact that the mean energy is the sum of the mean energies of each dipole is, of course, expected in the absence of (strong) interactions between particles.\nSimilarly,\n\\[\nF(T) = -kT \\ln Z = -NkT \\ln Z(1)\n\\]\nAnother consequence of the factorisation of \\(Z\\) is that if we are interested in the state of, say, particle 1, then we can ‘sum out’ the states of all the other particles 2 to \\(N\\):\n\\[\\begin{eqnarray*}\nP_{i_1} &=& \\sum_{i_2 \\cdots i_N} \\exp\\left(-\\beta \\left[\\epsilon_{i_1} + \\epsilon_{i_2} + \\cdots + \\epsilon_{i_N}\\right]\\right) \\times Z^{-1}\\\\\n& =& \\frac{\\exp(-\\beta \\epsilon_{i_1}) Z(1)^{N-1}}{Z(1)^N}\\\\\n& =& \\frac{\\exp(-\\beta \\epsilon_{i_1})}{Z(1)}\n\\end{eqnarray*}\\] This is identical to the probability distribution for a single particle discussed in 4.2.\n\n\n\n\n\n\nKey Point 13:\n\n\n\nIn a system of \\(N\\) weakly interacting, distinguishable particles, the system partition function is simply \\(Z = [Z(1)]^N\\) and the single particle probability distribution is \\(P_i = \\frac{\\exp(-\\beta \\epsilon_i)}{Z(1)}\\).\n\n\nThus we see that the problem is reduced to that of the single particle problem. The key thing to remember though is that for the single particle problem the energy, etc., fluctuate, whereas in the \\(N\\) particle system for large \\(N\\), the macroscopic variables are sharp and have well-defined thermodynamic values.\nN.B. the reason for the qualification ‘distinguishable’ will become apparent later when we address quantum particles.",
    "crumbs": [
      "Chapters",
      "6. Systems of weakly interacting constituents"
    ]
  },
  {
    "objectID": "chapter6.html#the-model-magnet",
    "href": "chapter6.html#the-model-magnet",
    "title": "6. Systems of weakly interacting constituents",
    "section": "",
    "text": "See Mandl 3.1-3.3\nIn 5.2, we examined the single particle partition function, i.e., the partition function for a single dipole, so effectively we have already done the hard work. We saw:\n\\[\nZ(1) = 2 \\cosh (x) \\quad \\text{where} \\quad x = \\frac{mH}{kT}\n\\]\nUsing the general results for weakly interacting systems (previous subsection), we have:\n\\[\n\\overline{E} = N\\bar{\\epsilon} = -N \\frac{\\partial}{\\partial \\beta} \\ln Z(1) = -NmH \\frac{\\partial}{\\partial x} \\ln Z(1) = -NmH \\tanh (x)\n\\]\n\\[\\begin{eqnarray*}\nS(T) &=& k \\ln Z + \\frac{\\overline{E}}{T} \\\\\n     &=& Nk \\ln Z(1) + \\frac{N\\bar{\\epsilon}}{T} \\\\\n     &=& Nk \\left[\\ln (\\exp (x) + \\exp (-x)) - x \\tanh (x)\\right]\n\\end{eqnarray*}\\]\nAn important quantity in magnetism is the magnetisation \\(M\\), which counts the total magnet moment \\(M=(n_1-n_2)m\\), where \\(n_i (i=1,2)\\) are the number of dipoles in levels 1 (\\(\\epsilon=−mH\\)) and 2 (\\(\\epsilon=+mH)\\). The magnetisation is related to the energy by \\(E=-MH\\). Thus we have:\n\\[\n\\overline{M} = Nm \\tanh (x)\n\\]\nwe see that for low fields, which means \\(mH \\ll kT\\) so that \\(x\\) is small:\n\\[\n\\overline{M} = \\frac{Nm^2H}{kT}\n\\]\nand for high fields \\(mH \\gg kT\\) so that \\(x \\gg 1\\):\n\\[\n\\overline{M} \\approx Nm\n\\]\nLet us now consider the zero-field magnetic susceptibility \\(\\chi(H = 0)\\) defined as:\n\\[\n\\chi(H = 0) \\equiv \\left(\\frac{\\partial \\overline{M}}{\\partial H}\\right)\\bigg|_{H=0} = \\frac{Nm^2}{kT}\n\\]\nYou should understand that the susceptibility measures the response of the magnetisation of the system to a small externally applied field. It turns out that this response is actually related to the magnetisation fluctuations at zero field. This is analogous to the results for the specific heat in terms of the variance of the energy fluctuations.\nThe \\(\\frac{1}{T}\\) dependence of \\(\\chi\\) is known as the Curie law. It implies that the dipoles become more ‘susceptible’ to an external magnetic field at lower temperature. See Mandl 3.1 for a comparison with real experiments.\nFinally, let’s consider the heat capacity (at constant field):\n\\[\nC_H = \\left(\\frac{\\partial \\overline{E}}{\\partial T}\\right)_H = \\left(\\frac{\\partial x}{\\partial T}\\right)_H \\left(\\frac{\\partial \\overline{E}}{\\partial x}\\right)_H = Nk x^2 \\text{sech}^2(x)\n\\]\nNote the low-temperature (large \\(x\\)) behaviour. Recalling:\n\\[\n\\text{sech } (x) = \\frac{2}{\\exp (x) + \\exp (-x)} \\sim 2 \\exp (-x) \\text{ for large } x\n\\]\nwe see that the heat capacity vanishes for small \\(T\\).\nThis is a general feature. Basically, at zero temperature, all particles are in the ground state. One has to raise the temperature until \\(kT\\) is comparable with \\(2mH\\) (the energy difference to the excited state) before a significant number of dipoles are excited. Thus near \\(T = 0\\), the derivative of internal energy with respect to \\(T\\) is zero.\n\\(kT\\) is often referred to as the ‘thermal energy’.",
    "crumbs": [
      "Chapters",
      "6. Systems of weakly interacting constituents"
    ]
  },
  {
    "objectID": "chapter6.html#more-general-factorisation-non-examinable",
    "href": "chapter6.html#more-general-factorisation-non-examinable",
    "title": "6. Systems of weakly interacting constituents",
    "section": "",
    "text": "We have seen in this chapter how the fact that for weakly interacting particles the energy is a sum of terms allows factorisation of \\(Z\\).\nA similar mechanism occurs in systems which are not necessarily weakly interacting, but whose energy may be expressed as a sum of contributions associated with different ‘aspects’ of its behaviour say \\(\\alpha, \\beta\\).\n\\[\nE = E_\\alpha + E_\\beta + \\cdots\n\\]\nThen if the microscopic ‘degrees of freedom’ which contribute to the different aspects are independent, we find that the partition function factorises:\n\\[\nZ = Z_\\alpha \\times Z_\\beta \\times \\cdots\n\\]\nA consequence of this is that the free energy is a sum of contributions from each aspect.\nFor example, in a real magnetic solid, contributions to the energy will come from vibrations of the atoms about their average positions (see chapter 8) as well as the energy from the interaction with a magnetic field discussed in this lecture. Thus we can write\n\\[\nE = E_{\\text{Vib}} + E_{\\text{Mag}}\n\\]\nWe have seen that the microscopic degrees of freedom which contribute to \\(E_{Mag}\\) are the dipole moments of the atoms.\nThe microscopic degrees of freedom which contribute to \\(E_{Vib}\\) will be something like the displacements from the atoms’ equilibrium positions, say. These degrees of freedom (d.o.f) are independent of the dipole moment degrees of freedom. Thus\n\\[\nZ = \\sum_{\\text{Vib d.o.f}} \\sum_{\\text{Mag d.o.f}} \\exp \\left( -\\beta [E_{\\text{Vib}} + E_{\\text{Mag}}] \\right) = Z_{\\text{Vib}} \\times Z_{\\text{Mag}}.\n\\]",
    "crumbs": [
      "Chapters",
      "6. Systems of weakly interacting constituents"
    ]
  },
  {
    "objectID": "solutions_set_2.html",
    "href": "solutions_set_2.html",
    "title": "Solutions to problem set 2",
    "section": "",
    "text": "A group of \\(N = 4\\) dipoles may have energies of the form \\(E = E_0 + n \\hat{\\varepsilon}\\) with \\(n = 0, 1, 2, 3, 4\\), where \\(E_0\\) is the ground state energy of the system, and \\(\\hat{\\varepsilon} \\equiv 2mH\\) is the spacing between the two levels, in the energy-level diagram for a single dipole; \\(n\\) is the number of dipoles in excited states. The microstates associated with the 5 energy macrostates (indexed by \\(n = 0 \\dots 5\\)) are as follows (using \\(\\uparrow\\) to signify a dipole in its aligned, low energy state; and \\(\\downarrow\\) to signify a dipole in its excited state):\n\n\n\n\n\n\n\n\nMacrostate\nMicrostates\nWeight \\(\\Omega\\)\n\n\n\n\n\\(n = 0\\)\n\\(\\uparrow \\uparrow \\uparrow \\uparrow\\)\n1\n\n\n\\(n = 1\\)\n\\(\\uparrow \\uparrow \\uparrow \\downarrow\\:\\:\\) and three further permutations\n4\n\n\n\\(n = 2\\)\n\\(\\uparrow \\uparrow \\downarrow \\downarrow\\)\n\n\n\n\n\\(\\uparrow \\downarrow \\uparrow \\downarrow\\)\n\n\n\n\n\\(\\uparrow \\downarrow \\downarrow \\uparrow\\)\n\n\n\n\n\\(\\downarrow \\uparrow \\uparrow \\downarrow\\)\n\n\n\n\n\\(\\downarrow \\uparrow \\downarrow \\uparrow\\)\n6\n\n\n\n\\(\\downarrow \\downarrow \\uparrow \\uparrow\\)\n\n\n\n\\(n = 3\\)\n\\(\\uparrow \\downarrow \\downarrow \\downarrow\\:\\:\\) and three further permutations\n4\n\n\n\\(n = 4\\)\n\\(\\downarrow \\downarrow \\downarrow \\downarrow\\)\n1\n\n\n\nThe first signs of the peak in the weight function (destined to be sharply focused on \\(n/N = 1/2\\) at large \\(N\\)) are already apparent.\n\n\n\nThe argument is given in full in chapter 2 which also plots the function in fig 4.\n\n\n\nConsider the function \\(s(y)\\). We have\n\\[\ns'(y) = -\\ln y + \\ln(1 - y)\n\\]\nwhich is zero at \\(y = y^* \\equiv \\frac{1}{2}\\). This locates the maximum in the function \\(s\\). For \\(y\\) near \\(y^*\\) we can write\n\\[\ns(y) \\simeq s(y^*) + s'(y) \\bigg|_{y=y^*} (y - y^*) + \\frac{1}{2} s''(y) \\bigg|_{y=y^*} (y - y^*)^2 + \\dots\n\\]\n\\[\n= s(y^*) + \\frac{1}{2} \\left[ \\frac{-1}{y^*} - \\frac{1}{1 - y^*} \\right] (y - y^*)^2\n\\]\n\\[\n= s(y^*) - \\frac{(y - y^*)^2}{2y^*(1 - y^*)}\n\\]\nIt follows that\n\\[\n\\ln \\Omega(E = 0, n_L) \\simeq N s(y^*) - \\frac{N (y - y^*)^2}{2y^*(1 - y^*)}\n\\]\nwhence exponentiating and substituting \\(y^*=\\frac{1}{2}\\)\n\\[\n\\Omega(E = 0, n_L) \\simeq \\Omega_{\\text{max}} e^{-2N(y - y^*)^2}\n\\]\nwith \\(\\Omega_{\\text{max}} \\equiv e^{Ns(y^*)} = \\Omega(E = 0, n_L = N/4)\\).\nThe weight function is thus a Gaussian peaked at \\(n_L = N/4\\). The peak is very sharp because of the factor of \\(N\\) in the exponential. The width of the peak may be estimated by the value of \\(\\Delta y \\equiv y - y^*\\) at which the argument of the exponential is of order unity. This occurs at \\(\\Delta y \\sim 1/(2N)^{1/2}\\). Thus the width is of order \\(1/\\sqrt{N}\\). This is the same behavior as we saw in the coin tossing example in section 1.4. It expresses a general truth about macroscopic quantities (in a sense, their defining feature): they have values which are sharply defined on the scale of their means.\n\n\n\nThe energy of an ideal gas is a function of its temperature, and only its temperature. Since the energy doesn’t change (the system is isolated) neither does the temperature. Thus the irreversible process described takes the gas from the thermodynamic state \\(T, V\\) to the thermodynamic state \\(T, 2V\\). We can find the entropy change by integrating the infinitesimal entropy changes occurring along any path we choose connecting these two states. One such path is a path of constant energy (and therefore temperature) in which, therefore,\n\\[\n0 = dE = TdS - PdV\n\\]\nat each stage. Then\n\\[\n\\Delta S \\equiv S_f - S_i = \\int_i^f dS = \\int_i^f \\frac{PdV}{T} = \\int_i^f \\frac{RdV}{V} = R \\ln \\frac{V_f}{V_i} = R \\ln 2\n\\]\nwhere we have made use of the ideal gas equation of state \\(PV=RT\\), for one mole of gas, with \\(R\\) the gas constant.\nNow consider the statistical mechanics perspective. If, as stated, the number of distinct states available to a single particle is proportional to the volume \\(V\\) in which it is contained, the number of distinct microstates of \\(N\\) particles is proportional to \\(V^N\\). Thus\n\\[\n\\Delta S \\equiv S_f - S_i = k \\ln \\Omega_f - k \\ln \\Omega_i = k \\ln \\frac{\\Omega_f}{\\Omega_i} = k \\ln \\frac{(2V)^N}{V^N} = Nk \\ln 2 = R \\ln 2\n\\] for \\(N = N_A\\) particles. The two calculations agree!\n\n\n\n\nThe ice cube has a size of \\(10 \\, \\text{cm}^3\\).\nIts density is \\(1 \\, \\text{g/cm}^3\\).\nThe latent heat of melting of ice is \\(3.34 \\times 10^5 \\, \\text{J/kg}\\).\n\nThe relation from thermodynamics for entropy change is:\n\\[\n\\Delta S = \\frac{Q}{T}\n\\]\nThe mass of the ice cube is given by the volume and density. Thus the mass is:\n\\[\n\\text{mass} = \\text{density} \\times \\text{volume} = 1 \\, \\text{g/cm}^3 \\times 10 \\, \\text{cm}^3 = 10 \\, \\text{g} = 0.01 \\, \\text{kg}\n\\]\nThe heat required to melt the ice cube, \\(Q\\), is calculated using the latent heat of melting of ice (\\(L = 3.34 \\times 10^5 \\, \\text{J/kg}\\)):\n\\[\nQ = \\text{mass} \\times L = 0.01 \\, \\text{kg} \\times 3.34 \\times 10^5 \\, \\text{J/kg} = 3.34 \\times 10^3 \\, \\text{J}\n\\]\nThe entropy change \\(\\Delta S\\) is given by the formula:\n\\[\n\\Delta S = \\frac{Q}{T}\n\\]\nWhere \\(T = 273 \\, \\text{K}\\) (since \\(0^\\circ C = 273 \\, \\text{K}\\)):\n\\[\n\\Delta S = \\frac{3.34 \\times 10^3 \\, \\text{J}}{273 \\, \\text{K}} \\approx 12.24 \\, \\text{J/K}\n\\]\nThe Planck relation connects entropy change with the number of microstates. The number of microstates, \\(\\Omega\\), is related to entropy by:\n\\[\n\\Delta S = k \\ln\\left(\\frac{\\Omega_{\\text{final}}}{\\Omega_{\\text{initial}}}\\right)\n\\]\n\\[\n12.24 \\, \\text{J/K} = (1.38 \\times 10^{-23} \\, \\text{J/K}) \\ln\\left(\\frac{\\Omega_{\\text{liquid}}}{\\Omega_{\\text{solid}}}\\right)\n\\]\nSolving for the ratio of microstates:\n\\[\n\\ln\\left(\\frac{\\Omega_{\\text{liquid}}}{\\Omega_{\\text{solid}}}\\right) \\approx \\frac{12.24}{1.38 \\times 10^{-23}} \\approx 8.87 \\times 10^{23}\n\\]\nTherefore:\n\\[\n\\frac{\\Omega_{\\text{liquid}}}{\\Omega_{\\text{solid}}} \\approx e^{8.87 \\times 10^{23}}\n\\]\nThis is an astronomically large number, indicating that the liquid phase has vastly more microstates than the solid phase.\n\n\n\nFirst let’s check that the stated low temperature structure matches the stated proportions of the atoms. Each cubic unit cell contributes effectively 1 corner site (each of the 8 corner sites is ‘shared’ amongst 8 adjacent cells) and 3 face-centre sites (each of the 6 face-centre sites is shared between 2 cells). Thus there are 3 times as many face centre sites (occupied by Cu atoms in the low-\\(T\\) arrangement) as there are cube-corner sites (occupied by Au atoms at low \\(T\\)), matching the stated proportions of Cu and Au atoms.\nIn a solid of \\(N\\) atoms (and \\(N\\) sites) there will thus be \\(n_1 = \\frac{3N}{4}\\) Cu atoms (and face-centre sites) and \\(n_2 = \\frac{N}{4}\\) Au atoms (and cube-corner sites).\nThe reference to ‘ways of thinking’ about atoms reflects the choice we have to make: do we say that two identical atoms (ie two atoms of the same species) are distinguishable or indistinguishable?\nThis is not always an empty issue. Here however we shall see that the choice we make has no effect on the final prediction for the entropy difference.\nSuppose, first, that atoms are distinguishable: we can tell the difference between one Au atom and another (and between one Cu atom and another). In the high \\(T\\) (disordered) structure we can arrange the \\(N = n_1 + n_2\\) different atoms amongst the \\(N\\) sites in\n\\[\\Omega_d = N!\\]\n(DISTINGUISHABLE)\ndistinct ways. In the low \\(T\\) (ordered) structure we can arrange the \\(n_1\\) Cu atoms amongst their (face-centre) sites in \\(n_1!\\) ways and the \\(n_2\\) Au atoms amongst their (cube-corner) sites in \\(n_2!\\) ways. Since each of the possible Cu arrangements can be paired with each of the possible Au arrangements, the total number of distinct arrangements is the product of these two factors, so that\n\\[\\Omega_o = n_1! n_2! = \\left(\\frac{3N}{4}\\right)!\\left(\\frac{N}{4}\\right)!\\]\n(DISTINGUISHABLE)\nNow suppose that the atoms are indistinguishable: we can’t tell the difference between members of the same species. We must then replace the above assignment of \\(\\Omega_d\\) by\n\\[\\Omega_d = \\frac{N!}{n_1! n_2!} = \\binom{N}{n_1}\\]\n(INDISTINGUISHABLE)\nwhich we recognise as the number of distinct ways of labelling the \\(N\\) sites as ‘Cu’ or ‘Au’, given \\(n_1\\) Cu labels, and \\(n_2 = N - n_1\\) Au labels. For the ordered arrangement we have simply\n\\[\\Omega_o = 1\\]\n(INDISTINGUISHABLE)\nsince all the face-centre sites now have ‘Cu’ labels and all the corner sites have ‘Au’ labels, and there are no choices to be made.\nNow consider the difference between the entropy of the ordered and disordered arrangements. We have\n\\[\n\\Delta S = S_d - S_o = k \\ln \\left( \\frac{\\Omega_d}{\\Omega_o} \\right)\n\\]\nInspection of the two sets of results shows that, whichever perspective we adopt\n\\[\n\\Delta S = k \\ln \\left( \\frac{N!}{n_1! n_2!} \\right)\n\\]\nThen appealing to the Stirling formula we find\n\\[\n\\frac{\\Delta S}{k} = N \\ln N - N - n_1 \\ln n_1 + n_1 - n_2 \\ln n_2 + n_2\n\\]\n\\[\n= N \\ln N - \\frac{3N}{4} \\ln \\frac{3N}{4} - \\frac{N}{4} \\ln \\frac{N}{4}\n\\]\n\\[\n= -N \\left( \\frac{3}{4} \\ln \\frac{3}{4} + \\frac{1}{4} \\ln \\frac{1}{4} \\right)\n\\]\nor, for one mole (setting \\(N = N_A\\) and \\(R \\equiv N_A k\\)),\n\\[\n\\Delta S = R \\left( \\ln 4 - \\frac{3}{4} \\ln 3 \\right) = 4.67 \\, \\mathrm{J \\, mol^{-1} \\, K^{-1}}\n\\]\nMeasurement of the heat capacity of this alloy as a function of \\(T\\) (how is this relevant?) shows that, in fact, between the low and high temperature regions the entropy increases by \\(\\Delta S \\simeq 3.96 \\, \\mathrm{J \\, mol^{-1} \\, K^{-1}}\\), in reasonable accord with the calculation."
  },
  {
    "objectID": "solutions_set_2.html#microstates-and-macrostates",
    "href": "solutions_set_2.html#microstates-and-macrostates",
    "title": "Solutions to problem set 2",
    "section": "",
    "text": "A group of \\(N = 4\\) dipoles may have energies of the form \\(E = E_0 + n \\hat{\\varepsilon}\\) with \\(n = 0, 1, 2, 3, 4\\), where \\(E_0\\) is the ground state energy of the system, and \\(\\hat{\\varepsilon} \\equiv 2mH\\) is the spacing between the two levels, in the energy-level diagram for a single dipole; \\(n\\) is the number of dipoles in excited states. The microstates associated with the 5 energy macrostates (indexed by \\(n = 0 \\dots 5\\)) are as follows (using \\(\\uparrow\\) to signify a dipole in its aligned, low energy state; and \\(\\downarrow\\) to signify a dipole in its excited state):\n\n\n\n\n\n\n\n\nMacrostate\nMicrostates\nWeight \\(\\Omega\\)\n\n\n\n\n\\(n = 0\\)\n\\(\\uparrow \\uparrow \\uparrow \\uparrow\\)\n1\n\n\n\\(n = 1\\)\n\\(\\uparrow \\uparrow \\uparrow \\downarrow\\:\\:\\) and three further permutations\n4\n\n\n\\(n = 2\\)\n\\(\\uparrow \\uparrow \\downarrow \\downarrow\\)\n\n\n\n\n\\(\\uparrow \\downarrow \\uparrow \\downarrow\\)\n\n\n\n\n\\(\\uparrow \\downarrow \\downarrow \\uparrow\\)\n\n\n\n\n\\(\\downarrow \\uparrow \\uparrow \\downarrow\\)\n\n\n\n\n\\(\\downarrow \\uparrow \\downarrow \\uparrow\\)\n6\n\n\n\n\\(\\downarrow \\downarrow \\uparrow \\uparrow\\)\n\n\n\n\\(n = 3\\)\n\\(\\uparrow \\downarrow \\downarrow \\downarrow\\:\\:\\) and three further permutations\n4\n\n\n\\(n = 4\\)\n\\(\\downarrow \\downarrow \\downarrow \\downarrow\\)\n1\n\n\n\nThe first signs of the peak in the weight function (destined to be sharply focused on \\(n/N = 1/2\\) at large \\(N\\)) are already apparent."
  },
  {
    "objectID": "solutions_set_2.html#exploring-the-weight-function-for-the-magnet",
    "href": "solutions_set_2.html#exploring-the-weight-function-for-the-magnet",
    "title": "Solutions to problem set 2",
    "section": "",
    "text": "The argument is given in full in chapter 2 which also plots the function in fig 4."
  },
  {
    "objectID": "solutions_set_2.html#sharpness-of-maximum-of-weight-function",
    "href": "solutions_set_2.html#sharpness-of-maximum-of-weight-function",
    "title": "Solutions to problem set 2",
    "section": "",
    "text": "Consider the function \\(s(y)\\). We have\n\\[\ns'(y) = -\\ln y + \\ln(1 - y)\n\\]\nwhich is zero at \\(y = y^* \\equiv \\frac{1}{2}\\). This locates the maximum in the function \\(s\\). For \\(y\\) near \\(y^*\\) we can write\n\\[\ns(y) \\simeq s(y^*) + s'(y) \\bigg|_{y=y^*} (y - y^*) + \\frac{1}{2} s''(y) \\bigg|_{y=y^*} (y - y^*)^2 + \\dots\n\\]\n\\[\n= s(y^*) + \\frac{1}{2} \\left[ \\frac{-1}{y^*} - \\frac{1}{1 - y^*} \\right] (y - y^*)^2\n\\]\n\\[\n= s(y^*) - \\frac{(y - y^*)^2}{2y^*(1 - y^*)}\n\\]\nIt follows that\n\\[\n\\ln \\Omega(E = 0, n_L) \\simeq N s(y^*) - \\frac{N (y - y^*)^2}{2y^*(1 - y^*)}\n\\]\nwhence exponentiating and substituting \\(y^*=\\frac{1}{2}\\)\n\\[\n\\Omega(E = 0, n_L) \\simeq \\Omega_{\\text{max}} e^{-2N(y - y^*)^2}\n\\]\nwith \\(\\Omega_{\\text{max}} \\equiv e^{Ns(y^*)} = \\Omega(E = 0, n_L = N/4)\\).\nThe weight function is thus a Gaussian peaked at \\(n_L = N/4\\). The peak is very sharp because of the factor of \\(N\\) in the exponential. The width of the peak may be estimated by the value of \\(\\Delta y \\equiv y - y^*\\) at which the argument of the exponential is of order unity. This occurs at \\(\\Delta y \\sim 1/(2N)^{1/2}\\). Thus the width is of order \\(1/\\sqrt{N}\\). This is the same behavior as we saw in the coin tossing example in section 1.4. It expresses a general truth about macroscopic quantities (in a sense, their defining feature): they have values which are sharply defined on the scale of their means."
  },
  {
    "objectID": "solutions_set_2.html#two-ways-of-calculating-entropy-changes",
    "href": "solutions_set_2.html#two-ways-of-calculating-entropy-changes",
    "title": "Solutions to problem set 2",
    "section": "",
    "text": "The energy of an ideal gas is a function of its temperature, and only its temperature. Since the energy doesn’t change (the system is isolated) neither does the temperature. Thus the irreversible process described takes the gas from the thermodynamic state \\(T, V\\) to the thermodynamic state \\(T, 2V\\). We can find the entropy change by integrating the infinitesimal entropy changes occurring along any path we choose connecting these two states. One such path is a path of constant energy (and therefore temperature) in which, therefore,\n\\[\n0 = dE = TdS - PdV\n\\]\nat each stage. Then\n\\[\n\\Delta S \\equiv S_f - S_i = \\int_i^f dS = \\int_i^f \\frac{PdV}{T} = \\int_i^f \\frac{RdV}{V} = R \\ln \\frac{V_f}{V_i} = R \\ln 2\n\\]\nwhere we have made use of the ideal gas equation of state \\(PV=RT\\), for one mole of gas, with \\(R\\) the gas constant.\nNow consider the statistical mechanics perspective. If, as stated, the number of distinct states available to a single particle is proportional to the volume \\(V\\) in which it is contained, the number of distinct microstates of \\(N\\) particles is proportional to \\(V^N\\). Thus\n\\[\n\\Delta S \\equiv S_f - S_i = k \\ln \\Omega_f - k \\ln \\Omega_i = k \\ln \\frac{\\Omega_f}{\\Omega_i} = k \\ln \\frac{(2V)^N}{V^N} = Nk \\ln 2 = R \\ln 2\n\\] for \\(N = N_A\\) particles. The two calculations agree!"
  },
  {
    "objectID": "solutions_set_2.html#entropy-change-due-to-a-phase-transition-melting-ice",
    "href": "solutions_set_2.html#entropy-change-due-to-a-phase-transition-melting-ice",
    "title": "Solutions to problem set 2",
    "section": "",
    "text": "The ice cube has a size of \\(10 \\, \\text{cm}^3\\).\nIts density is \\(1 \\, \\text{g/cm}^3\\).\nThe latent heat of melting of ice is \\(3.34 \\times 10^5 \\, \\text{J/kg}\\).\n\nThe relation from thermodynamics for entropy change is:\n\\[\n\\Delta S = \\frac{Q}{T}\n\\]\nThe mass of the ice cube is given by the volume and density. Thus the mass is:\n\\[\n\\text{mass} = \\text{density} \\times \\text{volume} = 1 \\, \\text{g/cm}^3 \\times 10 \\, \\text{cm}^3 = 10 \\, \\text{g} = 0.01 \\, \\text{kg}\n\\]\nThe heat required to melt the ice cube, \\(Q\\), is calculated using the latent heat of melting of ice (\\(L = 3.34 \\times 10^5 \\, \\text{J/kg}\\)):\n\\[\nQ = \\text{mass} \\times L = 0.01 \\, \\text{kg} \\times 3.34 \\times 10^5 \\, \\text{J/kg} = 3.34 \\times 10^3 \\, \\text{J}\n\\]\nThe entropy change \\(\\Delta S\\) is given by the formula:\n\\[\n\\Delta S = \\frac{Q}{T}\n\\]\nWhere \\(T = 273 \\, \\text{K}\\) (since \\(0^\\circ C = 273 \\, \\text{K}\\)):\n\\[\n\\Delta S = \\frac{3.34 \\times 10^3 \\, \\text{J}}{273 \\, \\text{K}} \\approx 12.24 \\, \\text{J/K}\n\\]\nThe Planck relation connects entropy change with the number of microstates. The number of microstates, \\(\\Omega\\), is related to entropy by:\n\\[\n\\Delta S = k \\ln\\left(\\frac{\\Omega_{\\text{final}}}{\\Omega_{\\text{initial}}}\\right)\n\\]\n\\[\n12.24 \\, \\text{J/K} = (1.38 \\times 10^{-23} \\, \\text{J/K}) \\ln\\left(\\frac{\\Omega_{\\text{liquid}}}{\\Omega_{\\text{solid}}}\\right)\n\\]\nSolving for the ratio of microstates:\n\\[\n\\ln\\left(\\frac{\\Omega_{\\text{liquid}}}{\\Omega_{\\text{solid}}}\\right) \\approx \\frac{12.24}{1.38 \\times 10^{-23}} \\approx 8.87 \\times 10^{23}\n\\]\nTherefore:\n\\[\n\\frac{\\Omega_{\\text{liquid}}}{\\Omega_{\\text{solid}}} \\approx e^{8.87 \\times 10^{23}}\n\\]\nThis is an astronomically large number, indicating that the liquid phase has vastly more microstates than the solid phase."
  },
  {
    "objectID": "solutions_set_2.html#entropy-change-due-to-a-phase-transition",
    "href": "solutions_set_2.html#entropy-change-due-to-a-phase-transition",
    "title": "Solutions to problem set 2",
    "section": "",
    "text": "First let’s check that the stated low temperature structure matches the stated proportions of the atoms. Each cubic unit cell contributes effectively 1 corner site (each of the 8 corner sites is ‘shared’ amongst 8 adjacent cells) and 3 face-centre sites (each of the 6 face-centre sites is shared between 2 cells). Thus there are 3 times as many face centre sites (occupied by Cu atoms in the low-\\(T\\) arrangement) as there are cube-corner sites (occupied by Au atoms at low \\(T\\)), matching the stated proportions of Cu and Au atoms.\nIn a solid of \\(N\\) atoms (and \\(N\\) sites) there will thus be \\(n_1 = \\frac{3N}{4}\\) Cu atoms (and face-centre sites) and \\(n_2 = \\frac{N}{4}\\) Au atoms (and cube-corner sites).\nThe reference to ‘ways of thinking’ about atoms reflects the choice we have to make: do we say that two identical atoms (ie two atoms of the same species) are distinguishable or indistinguishable?\nThis is not always an empty issue. Here however we shall see that the choice we make has no effect on the final prediction for the entropy difference.\nSuppose, first, that atoms are distinguishable: we can tell the difference between one Au atom and another (and between one Cu atom and another). In the high \\(T\\) (disordered) structure we can arrange the \\(N = n_1 + n_2\\) different atoms amongst the \\(N\\) sites in\n\\[\\Omega_d = N!\\]\n(DISTINGUISHABLE)\ndistinct ways. In the low \\(T\\) (ordered) structure we can arrange the \\(n_1\\) Cu atoms amongst their (face-centre) sites in \\(n_1!\\) ways and the \\(n_2\\) Au atoms amongst their (cube-corner) sites in \\(n_2!\\) ways. Since each of the possible Cu arrangements can be paired with each of the possible Au arrangements, the total number of distinct arrangements is the product of these two factors, so that\n\\[\\Omega_o = n_1! n_2! = \\left(\\frac{3N}{4}\\right)!\\left(\\frac{N}{4}\\right)!\\]\n(DISTINGUISHABLE)\nNow suppose that the atoms are indistinguishable: we can’t tell the difference between members of the same species. We must then replace the above assignment of \\(\\Omega_d\\) by\n\\[\\Omega_d = \\frac{N!}{n_1! n_2!} = \\binom{N}{n_1}\\]\n(INDISTINGUISHABLE)\nwhich we recognise as the number of distinct ways of labelling the \\(N\\) sites as ‘Cu’ or ‘Au’, given \\(n_1\\) Cu labels, and \\(n_2 = N - n_1\\) Au labels. For the ordered arrangement we have simply\n\\[\\Omega_o = 1\\]\n(INDISTINGUISHABLE)\nsince all the face-centre sites now have ‘Cu’ labels and all the corner sites have ‘Au’ labels, and there are no choices to be made.\nNow consider the difference between the entropy of the ordered and disordered arrangements. We have\n\\[\n\\Delta S = S_d - S_o = k \\ln \\left( \\frac{\\Omega_d}{\\Omega_o} \\right)\n\\]\nInspection of the two sets of results shows that, whichever perspective we adopt\n\\[\n\\Delta S = k \\ln \\left( \\frac{N!}{n_1! n_2!} \\right)\n\\]\nThen appealing to the Stirling formula we find\n\\[\n\\frac{\\Delta S}{k} = N \\ln N - N - n_1 \\ln n_1 + n_1 - n_2 \\ln n_2 + n_2\n\\]\n\\[\n= N \\ln N - \\frac{3N}{4} \\ln \\frac{3N}{4} - \\frac{N}{4} \\ln \\frac{N}{4}\n\\]\n\\[\n= -N \\left( \\frac{3}{4} \\ln \\frac{3}{4} + \\frac{1}{4} \\ln \\frac{1}{4} \\right)\n\\]\nor, for one mole (setting \\(N = N_A\\) and \\(R \\equiv N_A k\\)),\n\\[\n\\Delta S = R \\left( \\ln 4 - \\frac{3}{4} \\ln 3 \\right) = 4.67 \\, \\mathrm{J \\, mol^{-1} \\, K^{-1}}\n\\]\nMeasurement of the heat capacity of this alloy as a function of \\(T\\) (how is this relevant?) shows that, in fact, between the low and high temperature regions the entropy increases by \\(\\Delta S \\simeq 3.96 \\, \\mathrm{J \\, mol^{-1} \\, K^{-1}}\\), in reasonable accord with the calculation."
  },
  {
    "objectID": "course-texts.html",
    "href": "course-texts.html",
    "title": "Course Texts",
    "section": "",
    "text": "The notes that I provide are skeletons of the course to be fleshed out by your own reading. Pointers to relevant texts appear in the lecture notes. There are several paper copies of each of the following texts in the Physics library. All except the book by F. Mandl are also available as a pdf through the links to the library collection below.\n\nThe main text (an oldie but a goodie!) for this course is:\n\nF. Mandl: Statistical Physics.\n\nA more modern take, including hands-on simulations is:\n\nH. Gould and J. Tobochnik: Statistical and Thermal Physics with computer applications.\n\nA book that is good for some aspects of the course is\n\nS. Blundell and K. Blundell: Concepts in Thermal Physics\n\nAnother older staple book at the right level is\n\nR. Baierlein: Thermal Physics",
    "crumbs": [
      "Course texts"
    ]
  },
  {
    "objectID": "solutions_set_1.html",
    "href": "solutions_set_1.html",
    "title": "Solutions to problem set 1",
    "section": "",
    "text": "The binomial distribution of probabilities is given by\n\\[\np_n = {{N}\\choose{n}} p^n q^{(N-n)} \\quad \\text{where} \\quad {{N}\\choose{n}} \\equiv \\frac{N!}{(N-n)! n!}\n\\]\nis the binomial coefficient giving the number of distinct ways of sorting \\(N\\) objects into two piles containing \\(n\\) and \\(N-n\\) respectively.\nThe distribution approaches a Gaussian for large enough \\(N\\) (even \\(N \\simeq 10^2\\) is large enough for this correspondence to be good).\nThe most important things to know about this distribution (apart from what it means) are its mean and variance, which are given by\n\\[\n\\bar{n} \\equiv \\sum_{n=0}^N n p_n = Np \\quad \\text{(mean)}\n\\]\nand\n\\[\n\\Delta n^2 \\equiv \\sum_{n=0}^N (n - \\bar{n})^2 p_n = Npq \\quad \\text{(variance)}\n\\]\nThe standard deviation (which measures the width of the distribution) is identified as the square root of the variance. It is important to know these results!",
    "crumbs": [
      "Solutions",
      "Week 20: Solutions set 1"
    ]
  },
  {
    "objectID": "solutions_set_1.html#properties-of-the-binomial-distribution",
    "href": "solutions_set_1.html#properties-of-the-binomial-distribution",
    "title": "Solutions to problem set 1",
    "section": "",
    "text": "The binomial distribution of probabilities is given by\n\\[\np_n = {{N}\\choose{n}} p^n q^{(N-n)} \\quad \\text{where} \\quad {{N}\\choose{n}} \\equiv \\frac{N!}{(N-n)! n!}\n\\]\nis the binomial coefficient giving the number of distinct ways of sorting \\(N\\) objects into two piles containing \\(n\\) and \\(N-n\\) respectively.\nThe distribution approaches a Gaussian for large enough \\(N\\) (even \\(N \\simeq 10^2\\) is large enough for this correspondence to be good).\nThe most important things to know about this distribution (apart from what it means) are its mean and variance, which are given by\n\\[\n\\bar{n} \\equiv \\sum_{n=0}^N n p_n = Np \\quad \\text{(mean)}\n\\]\nand\n\\[\n\\Delta n^2 \\equiv \\sum_{n=0}^N (n - \\bar{n})^2 p_n = Npq \\quad \\text{(variance)}\n\\]\nThe standard deviation (which measures the width of the distribution) is identified as the square root of the variance. It is important to know these results!",
    "crumbs": [
      "Solutions",
      "Week 20: Solutions set 1"
    ]
  },
  {
    "objectID": "solutions_set_1.html#thinking-about-probabilities",
    "href": "solutions_set_1.html#thinking-about-probabilities",
    "title": "Solutions to problem set 1",
    "section": "1.2 Thinking about probabilities",
    "text": "1.2 Thinking about probabilities\n1. The probability of not seeing the event in a single trial is \\(1 - p\\). Therefore, the probability of not seeing the event in all \\(N\\) trials (i.e., zero occurrences of the event) is:\n\\[\nP(\\text{no event}) = (1 - p)^N\n\\]\nThus, the probability of seeing at least one instance of the event is the complement of this:\n\\[\nP(\\text{at least one event}) = 1 - (1 - p)^N\n\\]\n2. For small values of \\(p\\) and large \\(N\\), the expression \\((1 - p)^N\\) can be approximated using the exponential function. Specifically, for small \\(p\\), we use the approximation \\((1 - p) \\approx e^{-p}\\). Applying this to \\((1 - p)^N\\), we get:\n\\[\n(1 - p)^N \\approx \\exp(-Np)\n\\]\nThus, the probability of seeing at least one instance becomes:\n\\[\nP(\\text{at least one event}) \\approx 1 - \\exp(-Np)\n\\]\nThis approximation works well when \\(Np\\) is small, meaning when either \\(p\\) is small or \\(N\\) is large but \\(p\\) remains small enough that the product \\(Np\\) is not large.\n3. \\(Np\\) approximates the expected number of events (mean of the binomial distribution). For small probabilities, \\(Np\\) can be used to estimate the likelihood of observing at least one event. This is particularly effective when \\(Np\\) is small because:\n\nWhen \\(p\\) is very small and \\(N\\) is reasonably large, the distribution becomes approximately Poissonian with mean \\(Np\\), and the probability of at least one event is close to \\(Np\\).\nWhen \\(Np\\) is small, the expression \\(1 - \\exp(-Np) \\approx Np\\), as \\(\\exp(-Np)\\) can be expanded for small \\(Np\\).\n\nHence, \\(Np\\) is a good approximation for the probability of at least one event when \\(p\\) is small and \\(Np\\) is small.",
    "crumbs": [
      "Solutions",
      "Week 20: Solutions set 1"
    ]
  },
  {
    "objectID": "solutions_set_1.html#the-gaussian-distribution",
    "href": "solutions_set_1.html#the-gaussian-distribution",
    "title": "Solutions to problem set 1",
    "section": "1.3 The Gaussian distribution",
    "text": "1.3 The Gaussian distribution\nStart by writing the logarithm of the binomial probability \\(P_n\\):\n\\[\ns(n) = \\ln P_n = \\ln N! - \\ln (N-n)! - \\ln n! + n \\ln p + (N - n) \\ln (1-p)\n\\]\nStirling’s approximation is given by:\n\\[\n\\ln N! \\simeq N \\ln N - N\n\\]\nUsing this for \\(N!\\), \\((N-n)!\\), and \\(n!\\), we get:\n\\[\n\\ln N! \\simeq N \\ln N - N\n\\] \\[\n\\ln (N - n)! \\simeq (N - n) \\ln (N - n) - (N - n)\n\\] \\[\n\\ln n! \\simeq n \\ln n - n\n\\]\nThus\n\\[\ns(n) = \\left( N \\ln N - N \\right) - \\left( (N-n) \\ln (N-n) - (N-n) \\right) - \\left( n \\ln n - n \\right) + n \\ln p + (N - n) \\ln (1 - p)\n\\]\nSimplifying:\n\\[\ns(n) \\simeq n \\ln p + (N - n) \\ln (1 - p) + N \\ln N - (N - n) \\ln (N - n) - n \\ln n\n\\]\nNext we compute the first and second derivatives of \\(s(n)\\) with respect to \\(n\\).\n\\[\ns'(n) = \\frac{\\partial}{\\partial n} \\left[ n \\ln p + (N - n) \\ln (1 - p) + N \\ln N - (N - n) \\ln (N - n) - n \\ln n \\right]\n\\]\nDifferentiating term by term:\n\\[\ns'(n) = \\ln p - \\ln (1 - p) + \\ln (N - n) - \\ln n\n\\]\nThen\n\\[\ns''(n) = \\frac{\\partial}{\\partial n} \\left( \\ln p - \\ln (1 - p) + \\ln (N - n) - \\ln n \\right)\n\\] so\n\\[\ns''(n) = -\\frac{1}{N - n} - \\frac{1}{n} = -\\frac{N}{n(N - n)}\n\\]\nThe question invites us to perform a Taylor Expansion in \\(x = n - Np\\). We expand \\(s(n)\\) to second order around \\(n = Np\\):\n\\[\ns(n) \\simeq s(Np) + s'(Np)(n - Np) + \\frac{1}{2} s''(Np) (n - Np)^2\n\\]\nAt \\(n = Np\\), the first derivative \\(s'(Np) = 0\\), so the linear term vanishes. Thus, the second-order expansion is:\n\\[\ns(n) \\simeq s(Np) + \\frac{1}{2} s''(Np) (n - Np)^2\n\\]\nSubstitute \\(s''(Np) = -\\frac{1}{Np(1 - p)}\\):\n\\[\ns(n) \\simeq s(Np) - \\frac{(n - Np)^2}{2N(1 - p)p}\n\\]\nFinally, we can exponentiate both sides to recover \\(P_n\\):\n\\[\nP_n \\simeq P_{Np} \\exp\\left( - \\frac{(n - Np)^2}{2N(1 - p)p} \\right)\n\\] showing that the binomial distribution is approximated by a Gaussian distribution.",
    "crumbs": [
      "Solutions",
      "Week 20: Solutions set 1"
    ]
  },
  {
    "objectID": "solutions_set_1.html#the-random-walk",
    "href": "solutions_set_1.html#the-random-walk",
    "title": "Solutions to problem set 1",
    "section": "1.4 The random walk",
    "text": "1.4 The random walk\nDenote by \\(n_R\\) the number of steps taken to the right (the homeward direction, say). The number taken to the left is then \\(n_L = N - n_R\\). The displacement \\(d\\) from the starting point may thus be identified as\n\\[\nd = (n_R - n_L)a = (2n_R - N)a\n\\]\nNow a step to the right occurs on each “trial” (i.e. some step) with probability \\(p = 1/2\\). Thus the distribution of the number \\(n_R\\) of steps to the right is binomial with mean and standard deviation\n\\[\n\\overline{n_R} = Np = N/2 \\quad \\text{and} \\quad \\sqrt{\\overline{\\Delta n_R^2}} = \\sqrt{Npq} = \\sqrt{N/4}\n\\]\nThe mean and standard deviation of the displacement follow as\n\\[\n\\bar{d} = (2\\overline{n_R} - N)a = 0\n\\]\nand, since \\(\\Delta d \\equiv d - \\bar{d} = 2\\Delta n_R a\\),\n\\[\n\\sqrt{\\overline{\\Delta d^2}} = 2a \\sqrt{\\overline{\\Delta n_R^2}} = \\sqrt{Na}\n\\]\nThe typical distance from the origin thus increases as the square root of the number of steps taken.\nThough couched in whimsical language this little calculation has a wide range of applications (it is the basic model of a diffusive process) and can be extended and refined in many interesting ways (look up “Random Walk” on the internet!). One thing you might try is extending the above argument to a biased walk in which a step to the right is more likely than a step to the left.",
    "crumbs": [
      "Solutions",
      "Week 20: Solutions set 1"
    ]
  },
  {
    "objectID": "solutions_set_1.html#contrasting-one-and-many",
    "href": "solutions_set_1.html#contrasting-one-and-many",
    "title": "Solutions to problem set 1",
    "section": "1.5 Contrasting one and many",
    "text": "1.5 Contrasting one and many\nThe mean energy of a single particle is\n\\[\n\\bar{\\varepsilon} = \\sum_{i=1}^{2} p_i \\varepsilon_i\n\\]\nwhere (in this case) \\(p_1 = p_2 = \\frac{1}{2}\\). Thus\n\\[\n\\bar{\\varepsilon} = 0 \\times \\frac{1}{2} + \\hat{\\varepsilon} \\times \\frac{1}{2} = \\frac{\\hat{\\varepsilon}}{2}\n\\]\nThe variance of the energy of a single particle is\n\\[\n\\overline{(\\Delta \\varepsilon)^2} = \\sum_{i=1}^{2} p_i (\\varepsilon_i - \\bar{\\varepsilon})^2 = \\left( - \\frac{\\hat{\\varepsilon}}{2} \\right)^2 \\times \\frac{1}{2} + \\left( \\frac{\\hat{\\varepsilon}}{2} \\right)^2 \\times \\frac{1}{2} = \\frac{\\hat{\\varepsilon}^2}{4}\n\\]\nSo the standard deviation of the energy of one particle is\n\\[\n\\left[ \\overline{(\\Delta \\varepsilon)^2 }\\right]^{1/2} = \\frac{\\hat{\\varepsilon}}{2}\n\\]\nThe energy of \\(N\\) particles can be written as \\(E = n \\hat{\\varepsilon}\\) where \\(n\\) is the number of particles in excited states. Then, appealing to the binomial distribution (with \\(p = q = \\frac{1}{2}\\)),\n\\[\n\\overline{E} = \\overline{n} \\hat{\\varepsilon} = N p \\hat{\\varepsilon} = N \\frac{\\hat{\\varepsilon}}{2}\n\\]\nand since the particle are independent, that variances add\n\\[\n\\overline{(\\Delta E)^2} = \\overline{(\\Delta n)^2 }\\hat{\\varepsilon}^2 = N p q \\hat{\\varepsilon}^2 = N \\frac{\\hat{\\varepsilon}^2}{4}\n\\]\nso that\n\\[\n\\left[ \\overline{(\\Delta E)^2} \\right]^{1/2} = \\sqrt{N} \\frac{\\hat{\\varepsilon}}{2}\n\\]\nThe most pertinent comment is that while the standard deviation of the energy of a single particle is “significant” on the scale of its mean\n\\[\n\\frac{\\left[\\overline{ (\\Delta \\varepsilon)^2} \\right]^{1/2}}{\\bar{\\varepsilon}} = 1\n\\]\nthe standard deviation of the energy of the system as a whole is vanishingly small on the scale of its mean\n\\[\n\\frac{\\left[ \\overline{(\\Delta E)^2 }\\right]^{1/2}}{E} = \\frac{1}{\\sqrt{N}}\n\\]\nThis is a simple instance of a fundamental principle: physical properties are ever more sharply defined the more independent contributions they comprise.",
    "crumbs": [
      "Solutions",
      "Week 20: Solutions set 1"
    ]
  },
  {
    "objectID": "solutions_set_1.html#practically-never",
    "href": "solutions_set_1.html#practically-never",
    "title": "Solutions to problem set 1",
    "section": "1.6 Practically never",
    "text": "1.6 Practically never\nThe information supplied suggests that the distribution of the molecules over the two halves of the box will be refreshed roughly every \\(\\tau \\sim 10^3 \\, \\text{s}\\). Suppose we look through \\(\\mathcal{N}\\) such arrangements, at intervals of \\(\\tau\\). In each case we will have probability \\(p_N\\) of finding the ‘special’ arrangement. Thus in \\(\\mathcal{N}\\) occasions the mean number giving the special arrangement is \\(p_N \\mathcal{N}\\) (the mean of the binomial distribution).\nTo give ourselves a reasonable chance of encountering such an arrangement, we need to have \\(\\mathcal{N}\\) such that \\(p_N \\mathcal{N} \\sim 1\\). Thus we need \\(\\mathcal{N} \\sim 1 / p_N\\), which will require a time \\(\\mathcal{N} \\tau \\sim \\tau / p_N\\), which is something like \\(10^{(2 \\times 10^{23})} \\, \\text{s}\\) (the value assigned to \\(\\tau\\) gets lost!!). By comparison, the age of the universe is \\(\\sim 10^{17} \\, \\text{s}\\).",
    "crumbs": [
      "Solutions",
      "Week 20: Solutions set 1"
    ]
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1. Introduction",
    "section": "",
    "text": "Thermal Physics encompasses all parts of physics where ideas of temperature and entropy come into play. As we shall see this implies the properties of macroscopic systems with a large number of microscopic constituents.\nMacroscopic Approach (Classical Thermodynamics)\n\ndeals with macroscopic variables i.e. variables that do not refer to any microscopic details\ninput is phenomenological laws e.g. equation of state \\(P(V)\\)\noutput is general relations between macroscopic variables\nadvantage is the generality of the approach\n\nMicroscopic Approach (Statistical Mechanics)\n\nstarts from a microscopic description and seeks to explain macroscopic properties.\ninput is a microscopic model of a given system i.e. what we believe is an adequate microscopic description\noutput is predictions for macroscopic properties and behaviour.\nadvantage of the approach is that it yields predictions for a given system which can be compared to experiment thus allowing refinement of the microscopic model which in turn deepens our understanding of the system.\n\n\n\n\n\n\n\nThe key idea that connects the microscopic and macroscopic approaches is that due to the large number of consituents, macroscopic quantities are precisely defined even though the microscopic specification is not precise.\n\n\n\nAims of course\n\nTo formulate the ideas of statistical mechanics\nTo give microscopic understanding of entropy & second law of thermodynamics which you met in Year 1 Properties of Matter (PoM)\nTo derive results you have met in PoM such as equipartition and the Boltzmann factor\nTo explore the role of indistinguishability and quantum effects in many body systems",
    "crumbs": [
      "Chapters",
      "1. Introduction"
    ]
  },
  {
    "objectID": "chapter1.html#what-is-statistical-mechanics",
    "href": "chapter1.html#what-is-statistical-mechanics",
    "title": "1. Introduction",
    "section": "",
    "text": "Thermal Physics encompasses all parts of physics where ideas of temperature and entropy come into play. As we shall see this implies the properties of macroscopic systems with a large number of microscopic constituents.\nMacroscopic Approach (Classical Thermodynamics)\n\ndeals with macroscopic variables i.e. variables that do not refer to any microscopic details\ninput is phenomenological laws e.g. equation of state \\(P(V)\\)\noutput is general relations between macroscopic variables\nadvantage is the generality of the approach\n\nMicroscopic Approach (Statistical Mechanics)\n\nstarts from a microscopic description and seeks to explain macroscopic properties.\ninput is a microscopic model of a given system i.e. what we believe is an adequate microscopic description\noutput is predictions for macroscopic properties and behaviour.\nadvantage of the approach is that it yields predictions for a given system which can be compared to experiment thus allowing refinement of the microscopic model which in turn deepens our understanding of the system.\n\n\n\n\n\n\n\nThe key idea that connects the microscopic and macroscopic approaches is that due to the large number of consituents, macroscopic quantities are precisely defined even though the microscopic specification is not precise.\n\n\n\nAims of course\n\nTo formulate the ideas of statistical mechanics\nTo give microscopic understanding of entropy & second law of thermodynamics which you met in Year 1 Properties of Matter (PoM)\nTo derive results you have met in PoM such as equipartition and the Boltzmann factor\nTo explore the role of indistinguishability and quantum effects in many body systems",
    "crumbs": [
      "Chapters",
      "1. Introduction"
    ]
  },
  {
    "objectID": "chapter1.html#microscopic-approaches",
    "href": "chapter1.html#microscopic-approaches",
    "title": "1. Introduction",
    "section": "1.2 Microscopic approaches",
    "text": "1.2 Microscopic approaches\nExtreme philosophy: If we know microscopic laws e.g. Newtonian mechanics; quantum mechanics etc then we just specify the initial conditions and solve all the equations to determine what happens!\nProblems with this approach:\n\nAnalytically intractible\neven on a powerful computer one can only deal with a limited number of consituents say \\(N \\sim 10^5\\), whereas Avogadro’s number is \\(N_A=\\sim 6 \\times 10^{23}\\).\nsystem may not even be deterministic! i.e. small change in initial conditions leads to completely different final states\n\nInstead we will use the simpler and more powerful statistical formulation\n\nOnly aim to make probabilistic statements e.g. the Maxwell probability distribution for the velocities of molecules in an ideal gas (see PoM) doesn’t tell us about trajectories of individual gas molecules. However it does give us all we need to know to deduce macroscopic properties such as the pressure of the gas.\nIt is a very natural and economical approach i.e. we only introduce as much detail as we need (or want) in the microscopic description and we do not waste time calculating unnecessary details. Therefore we can do the calculations analytically (i.e. on pen and paper).\nIt uses the large number \\(N\\) of microscopic constituents to advantage\nWhy the name ‘statistical mechanics’?\n\nThe microscopic model is the ‘mechanics’ e.g. quantum, Newtonian or even more primitive (but adequate) descriptions\n‘statistical’ refers to the statistical description of the resulting behaviour of a system with a large number of microscopic constituents\n\n\nStatistical mechanics was founded in the late nineteenth century with the work of Maxwell, Boltzmann, Gibbs …on classical systems. It was developed hand in hand with quantum mechanics in the twentieth century by Einstein, Bose, Dirac, Fermi . . . It remains an active and constantly developing research area in the 21st century. For example the techniques of statistical mechanics are applied to neural modelling, the study of traffic flow, economic and social systems… in fact any system with a large number of constituents.\nMore detailed comparison of the two approaches (taking example of a gas):\n\n\n\n\n\n\n\n\n\nThermodynamics\nStatistical Mechanics\n\n\n\n\nQuantity of interest\nMacroscopic properties (eg. \\(P,V,T,C_P, C_V\\))\nMicroscopic properties (eg: molecular speeds)\n\n\nStrategy\nAvoid microscopic model\nBuild on microscopic model\n\n\nStrengths\nGenerality of results\nProvides way of refining microscopic understanding\n\n\nWeaknesses\nNo understanding of system-specific features. Conceptually opaque.\nRequires additional input (the model). Requires additional techniques (probability theory; classical and quantum mechanics)",
    "crumbs": [
      "Chapters",
      "1. Introduction"
    ]
  },
  {
    "objectID": "chapter1.html#probability",
    "href": "chapter1.html#probability",
    "title": "1. Introduction",
    "section": "1.3 Probability",
    "text": "1.3 Probability\nProbabilitic concepts are central to statistical mechanics. You should have a working knowledge of probability theory from your programme so far. We shall not explicitly revise this material in lectures, so refer to you probability notes as needed. For convenience and reference we summarise below some of the main results on which we shall draw.\n\n\n\n\n\n\nExpand to review key concepts of probability\n\n\n\n\n\nFor further reading see chapter 3 of Gould and Tobochnik\nDefinitions\n\nfrequency definition: the probability \\(P\\) of an event in a trial is given by \\[\nP = \\lim_{N\\to\\infty}\\frac{n}{N}\n\\] where \\(n\\) is the number of occurrences in \\(N\\) trials.\ndegree of belief: probability is a quantitative measure of our degree of belief that something will occur e.g. if there are \\(q\\) possible outcomes of a trial (recall that a trial is some procedure where we can measure the outcome), and we have no reason to favour any one outcome over any other, then we would assign probability \\(1/q\\) to each outcome. Tossing a coin would correspond to \\(q = 2\\) and rolling a die to \\(q = 6\\).\n\nThankfully both definitions lead to the same numerical values for probability.\nRules of Probability\nFor a trial with \\(q\\) possible outcomes let the probability of outcome \\(i\\) be \\(P(i)\\)\n\nnormalisation \\(\\sum_{i=1}^q P(i) = 1\\)\n\\(P (i \\text{ or } j) = P (i) + P (j)\\) (for mutually exclusive outcomes \\(i, j\\)).\nWe can also consider ‘compound events’. For example consider two trials then\n\n\\[\nP(i \\text{ in trial $1$ and $j$ in trial $2$}) = P(i)P(j)\n\\] (for outcomes of trial \\(1\\) and trial \\(2\\) independent)\nProbability distributions\nIf we associate a numerical value \\(x_i\\) to event \\(i\\) then the probability distribution for the ‘random variable’ \\(x\\) is\n\\[\nP(i)= \\text{Probability that } x=x_i\n\\]\nProbability density functions\nIn many case the outcome of an event is described by a continuous variable \\(x\\). Then the probability density function \\(P(x)\\) is defined by \\[\nP (x) dx = \\text{Probability that outcome lies in the range } x \\text{ to } x + dx\n\\] and is normalised according to\n\\[\n\\int_{-\\infty}^\\infty P(x)dx=1\n\\] (N.B. often the probability density function is simply called the probability distribution.)\nAverages\nThe Probability distribution contains the complete information about the trial. However the moments (mean, variance . . . ) can give the important features. They are defined as\n\\[\\begin{eqnarray*}\n\\text{mean}~~\\overline{x} &=& \\sum_{i=1}^q P(i)x_i ~\\text{ or}~ \\int_{-\\infty}^\\infty P(x)xdx\\\\\n\\text{variance}~~\\overline{\\Delta x^2} &=& \\sum_{i=1}^q P(i)(x_i-\\overline{x})^2 ~\\text{ or}~ \\int_{-\\infty}^\\infty P(x)(x-\\overline{x})^2\\\\\ni.e. ~~\\overline{\\Delta x^2}&=& \\overline{(x-\\overline{x})^2}=\\overline{x^2}-\\overline{x}^2\n\\end{eqnarray*}\\]\nMore generally the average of a function \\(f(x)\\) is given by\n\\[\n\\overline {f(x)}=\\sum_{i=1}^qP(i)f(x_i)~~\\text{ or}~~\\int_{-\\infty}^\\infty P(x)f(x)dx\n\\]\nBasic distributions\nFor completeness (and for the mathematically inclined) the content below includes some derivations of the functional forms of distributions; you are not expected to know these derivations for this course.\nThe binomial distribution\nFirst let us review the concepts of permutations and combinations. If we have \\(N\\) distinguishable objects—say numbered balls, or a pack of cards—then if we select \\(n\\) objects from \\(N\\) (without replacement) how many different possible outcomes are there?\nThe answer is known as the number of permutations of \\(n\\) from \\(N\\) and is equal to \\[\n\\frac{N!}{(N-n)!} = \\frac{N(N−1)(N−2)\\ldots(1)}{(N-n)(N-n-1)\\ldots(1)} =N(N−1)(N−2)...(N−n+1).\n\\]\nTo see this think of the number of possibilities for the first object (\\(= N\\)) then multiply by the number of possibilities for the second \\((N − 1)\\) and keep going until the nth object for which there are (\\(N − n + 1\\)) possibilities.\nAlso recall that \\(0! = 1\\), so that the number of permuations of \\(N\\) from \\(N\\) is \\(N!\\)\nIn this counting scheme we in fact count as different outcomes the selection of the same objects but in different order e.g. clearly in the number of permutations of \\(N\\) from \\(N\\) we always choose the same \\(N\\) objects but the number of possible orders is \\(N!\\)\nNow if we only count as distinct outcomes those events where a different set of objects is selected we are interested in the number of combinations of \\(n\\) from \\(N\\). To obtain the number of combinations we have to divide the number of permutations of \\(n\\) from \\(N\\) by \\(n!\\) (the number of permutations of the n selected objects). This results in \\[\n\\text{number of combinations of $n$ from } N = \\frac{N!}{(N − n)! n!}= {N\\choose n}\n\\]\nwhere \\({N \\choose n}\\) is known as a binomial coefficient and is pronounced ‘\\(N\\) choose \\(n\\)’. (N.B. There are many other symbols that are used for the number of combinations.)\nNow we are ready to write down the binomial distribution. We consider \\(N\\) trials in each of which an event can occur with probability p. The probability of observing precisely \\(n\\) events in \\(N\\) trials is given by the Binomial distribution \\[\n\\boxed{P_n = {N\\choose n}p^n(1−p)^{N−n}}\n\\] The binomial coefficient gives the number of ways of choosing the \\(n\\) trials where an event occurs out of the total of \\(N\\) trials.\nThe factor \\(p^n(1 − p)^{N−n}\\) is the probability that in the \\(n\\) chosen trials an event occurs and in the rest of the trials an event does not occur. (This factor is the same whatever the \\(n\\) chosen trials). This argument may seem somewhat obscure at first but should with familiarity become second nature.\nNote that for the Binomial distribution \\[\n\\boxed{\\overline{n}=Np~\\text{  and  }~ \\overline{\\Delta n^2}=Np(1−p)}\n\\]\nIt should be pointed out that the factorials involved in combinatorial calculations soon become very large numberse.g. for \\(N =0,1,2\\dots\\) one can check that \\(N!\\) is \\(1,1,2,6,24,120,720\\ldots\\) and already \\(15! \\sim 10^{12}\\).\nA very useful approximation for the factorial function is given by the Stirling approximation\n\\[\n\\ln(N !) = N \\ln N − N + \\frac{1}{2}\\ln(2\\pi N ) + O(1/N )\n\\] We see that for \\(N\\) large\n\\[\n\\boxed{\\ln(N !) \\simeq N\\ln N − N}\n\\]\nPoisson distribution\nAs an application of Stirling’s approximation let us consider the binomial probabilities for \\(N \\gg n\\). Then we find \\[\\begin{eqnarray*}\n\\ln {N\\choose n} &=& \\ln N!−\\ln(N−n)!−\\ln n!\\\\\n&\\simeq &  N\\ln N−N−(N−n)\\ln(N−n)+(N−n)−\\ln n!\\\\\n&\\simeq &  n\\ln N−\\ln n!\n\\end{eqnarray*}\\] In the calculation we used Stirling’s approximation for \\(\\ln N !\\) and \\(\\ln(N − n)!\\) and made use of the fact that for \\(N \\gg n\\), \\(\\ln(N −n) \\simeq \\ln N −n/N\\). Note that we have only kept the highest order terms. Thus, re-exponentiating the logarithm, we see that \\[\n{N\\choose n}\\simeq\\frac{N^n}{n!}\n\\] Now consider\n\\[(1−p)^{N−n} =\\exp[(N−n)\\ln(1−p)]\\simeq \\exp(−Np)\n\\] where we have used \\(\\ln(1−p) \\simeq −p\\) for \\(p \\ll 1\\) and \\(N −n \\simeq N\\). (More strictly we require \\(N\\to\\infty, p\\to 0\\) such that \\(Np=\\overline{n}\\) is finite.) Under these conditions we find when we put the above results together that \\[\n\\boxed{P_n = (N p)^n \\frac{\\exp(−N p)}{n!} = (\\overline{n}^n) \\frac{\\exp(−\\overline{n})}{n!}}\n\\]\nThis is known as the Poisson distribution and one can check using the power series representation of the exponential function that \\(\\sum_{n=1}^NP_n=1\\). One also finds\n\\[\n\\boxed{\n\\begin{aligned}\n\\overline{n}          &= \\sum_{n=1}^NnP_n=Np\\\\\n\\overline{\\Delta n^2} &= \\sum_{n=1}^N(n-\\overline{n})^2P_n=Np\n\\end{aligned}}\n\\] ie. the mean and variance are equal.\nBasically the Poisson distribution is used when there are a large number of trials in each of which an event is very unlikely but overall one expects a finite number of events.\nGaussian Distribution\nHere we consider the limit of the binomial distribution where \\(N\\) is large and so is \\(Np\\). Therefore we expect that for \\(P_n\\) to be non-vanishingly small, we need \\(n\\) of the same order as \\(Np\\)\nThus we use Stirling’s approximation on binomial distribution to show that\n\\[\ns(n) \\equiv \\ln P_n \\simeq n\\ln p+(N −n)\\ln(1−p) +N \\ln N −(N −n)\\ln(N −n)−n\\ln n\n\\] for large \\(N,n\\). It is straightforward to show that\n\\[\\begin{eqnarray*}\ns^\\prime (n) &=& \\ln p−\\ln(1−p)+\\ln(N −n)−\\ln n\\\\\ns^{\\prime\\prime}(n) &=& − \\frac{1}{N-n}-\\frac{1}{n}\n\\end{eqnarray*}\\] You should notice that \\(s(n)\\) is a maximum at \\(n = Np\\) i.e. at the mean value.\nNow expand in powers of \\(x = n − N p\\), i.e. make a Taylor expansion around the mean value: \\[\ns(x)=s(Np)+xs^\\prime(Np)+ \\frac{x^2}{2}s^\\prime(Np)\\ldots\n\\]\nNote that since \\(s\\) is maximised at the mean value the first non-zero term in the expansion is in \\(x^2\\) and\n\\[\ns(x)\\simeq s(NP)=\\frac{x^2}{2N(1-p)p}\n\\] When we return to \\(P_n = \\exp s(n)\\) we find\n\\[\nP_n\\simeq P_{Np}\\exp{ \\left(\\frac{−(n−Np)^2}{2N(1−p)p}\\right)}\n\\] In order to determine the constant \\(P_{N_p}\\) which serves to normalise the distribution we can make the approximation\n\\[\n1=\\sum_{n-0}^NP_m\\simeq\\int_{-\\infty}^\\infty P_{Np}\\exp{\\left(\\frac{−(x^2}{2N(1−p)p}\\right)}dx\n\\] where extending the limits in this way produces no error since the additional contributions are vanishingly small.\nWe then invoke a standard integral you should be familiar with \\[\n\\int_{-\\infty}^\\infty \\exp{\\left(\\frac{−(x^2}{2\\sigma^2}\\right)}dx=\\sqrt{2\\pi\\sigma^2}.\n\\] From this we deduce the correct normalisation for the Gaussian approximation to the binomial distribution as\n\\[\n\\boxed{P_n\\simeq \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{\\left(\\frac{−(n-Np)^2}{2\\sigma^2}\\right)}}\n\\] where \\(\\sigma^2=Np(1-p)\\).\nNote that if \\(|n−Np| \\gg N^{1/2}\\) the probability becomes vanishingly small due to the argument of the exponential becoming very large.\nThe Gaussian distribution is often referred to as a ‘Normal distribution’.",
    "crumbs": [
      "Chapters",
      "1. Introduction"
    ]
  },
  {
    "objectID": "chapter1.html#simplicity-at-large-n-a-toy-example",
    "href": "chapter1.html#simplicity-at-large-n-a-toy-example",
    "title": "1. Introduction",
    "section": "1.4 Simplicity at large \\(N\\): a toy example",
    "text": "1.4 Simplicity at large \\(N\\): a toy example\nSuppose that \\(N\\) coins are tossed; we denote by \\(n\\) the number of heads obtained. We will determine the probability distribution \\(p_n\\) (ie the set of probabilities \\(p_0,p_1\\ldots,p_N\\)) characterising the possible outcomes.\nDenote by \\(p\\) the probability that a head results from a single toss; then \\(q\\equiv 1-p\\) gives the corresponding probability for a tail.\nThere are many distinct ways of getting \\(n\\) heads from \\(N\\) tosses, differing according to which tosses give heads. It should be clear that each one of these distinct ways has the same probability, namely \\(p^nq^{N-n}\\), and recalling your probability and statistics lectures, the number of distinct ways in which we can get \\(n\\) heads from \\(N\\) tosses is given by the binomial coefficient:\n\\[\n{N\\choose n}\\equiv \\frac{N!}{(N-n)!n!}\n\\]\nIt follows that\n\\[\\begin{eqnarray*}\n   p_n & \\equiv &  \\text {number of distinct ways of obtaining $n$ heads}\\\\\n       & & \\times   \\text{probability of any specific way of getting $n$ heads}\\\\[2mm]\n   & = & {N\\choose n}p^nq^{N-m}\n\\end{eqnarray*}\\]\nThis is the binomial distribution of probabilities. The distribution is characterised by two key parameters, the mean \\(\\overline{n}\\) and the variance \\(\\overline{\\Delta n^2}\\). Remind yourself from your probability and statistics lectures that \\(\\overline{n}\\equiv\\sum_{n=0}^Nnp_n=Np\\), and \\(\\overline{\\Delta n^2}\\equiv\\sum_{n=0}^N(n-\\overline{n})^2p_n=Npq\\).\nIn the present context, setting \\(p=q=\\frac{1}{2}\\) (for an unbiased coin) and defining \\(f\\equiv n/N\\), the fraction of tosses giving heads, we have\n\\[\n\\overline{f}=\\frac{\\overline{n}}{N}=p=\\frac{1}{2}\n\\]\nand\n\\[\n(\\overline{\\Delta f^2})^{1/2}\\equiv\\frac{(\\overline{\\Delta n^2})^{1/2}}{N}=\\left(\\frac{pq}{N}\\right)^{1/2}=\\frac{1}{2N^{1/2}}\n\\tag{1}\\]\nEquation 1 shows that the typical deviation of \\(f\\) from its mean value is vanishingly small (it is \\(O(N^{-1/2}\\)) for \\(N\\) large). Thus for large \\(N\\) we can be very sure that \\(f\\) will always effectively coincide with its mean (cf Figure 1 (b)). The virtual certainty that comes from dealing with large numbers is one of the distinctive features of statistical physics.\n\n\n\n\n\n\n\n\n\n\n\n(a) For \\(N=50\\) tosses, we can be reasonably sure that \\(f\\) will be close to \\(0.5\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) For \\(N=10^{26}\\) tosses We can be absolutely sure that \\(f\\) will be indistinguishable from \\(0.5\\)\n\n\n\n\n\n\n\nFigure 1: Left: a probability histogram of the fraction of heads \\(f\\) obtained from repeated trials of \\(N=50\\) coin tosses. Right: for a vary large number \\(N=10^{26}\\) tosses the histogram narrows essentially to a \\(\\delta\\) function.\n\n\n\nYou can investigate for yourself the effect of changing the number of coin tosses \\(N\\) in this python simulation (copy it into your favourite Python runtime environment):\n\n\nCode\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef flip_coin(n):\n    \"\"\"Simulate flipping a fair coin n times.\"\"\"\n    outcomes = [random.choice(['H', 'T']) for _ in range(n)]\n    heads_count = outcomes.count('H')\n    return heads_count / n\n\ndef simulate_coin_flips(N, M):\n    \"\"\"Simulate flipping a fair coin N times and repeat M times.\"\"\"\n    fractions = [flip_coin(N) for _ in range(M)]\n    return fractions\n\ndef plot_probability_histogram(fractions):\n    \"\"\"Plot a histogram of the probabilities of the fractions of heads.\"\"\"\n    weights = np.ones_like(fractions) / len(fractions)\n    plt.hist(fractions, bins=np.linspace(0, 1.0, 50), weights=weights, edgecolor='black')\n    plt.xlabel('Fraction of heads')\n    plt.ylabel('Probability')\n    plt.xticks(np.arange(0, 1, 0.1))\n    plt.xlim(0.1,0.9)\n    plt.title('Probability Histogram of Fraction of Heads in {} Coin Flips'.format(N))\n    plt.grid(True)\n    plt.show()\n\n#if __name__ == \"__main__\":\n    N = int(input(\"Enter the number of times to flip the coin each time: \"))\n    M = int(input(\"Enter the number of times to repeat the simulation: \"))\n    fractions = simulate_coin_flips(N, M)\n    plot_probability_histogram(fractions)",
    "crumbs": [
      "Chapters",
      "1. Introduction"
    ]
  },
  {
    "objectID": "solutions_set_4.html",
    "href": "solutions_set_4.html",
    "title": "Solutions to problem set 4",
    "section": "",
    "text": "The analysis is given in chapter 8. For high \\(T\\), in the sense that \\(\\hbar \\omega / kT \\ll 1\\), we can write\n\\[\ne^{\\hbar \\omega / kT} \\simeq 1 + \\frac{\\hbar \\omega}{kT} + O\\left( \\frac{\\hbar \\omega}{kT} \\right)^2\n\\]\nso that\n\\[\n\\bar{n} = \\frac{1}{1 + \\hbar \\omega / kT + \\dots - 1} = \\frac{kT}{\\hbar \\omega} \\gg 1\n\\]\nwhence \\(\\overline{\\varepsilon} \\simeq kT\\). The disappearance of \\(\\hbar\\) manifests the fact that this result is “classical”: it can be established by arguments which make no reference to the underlying quantum theory of the harmonic oscillator; they capture the high \\(T\\) behaviour correctly, but fail to predict the different behaviour that sets in, typically, as \\(kT\\) becomes small enough to “resolve” the discreteness of the energy level structure.\nThis result is an example of the Equipartition Theorem.\n\n\n\nThe partition function for the 1d oscillator is (Chapter 8.2)\n\\[\nZ^{d=1}(1) = \\frac{e^{-x/2}}{1 - e^{-x}} \\quad \\text{with} \\quad x = \\frac{\\hbar \\omega}{kT}\n\\]\nThe partition function for \\(3N\\) 1d oscillators (or \\(N\\) 3d oscillators) is\n\\[\nZ = \\left[ Z^{d=1}(1) \\right]^{3N}\n\\]\nThe mean energy of one 1d oscillator is\n\\[\n\\overline{\\varepsilon}^{d=1} = \\left[ \\frac{1}{e^x - 1} + \\frac{1}{2} \\right] \\hbar \\omega\n\\]\nand the mean energy of \\(3N\\) such oscillators is\n\\[\n\\overline{E} = 3N \\left[ \\frac{1}{e^x - 1} + \\frac{1}{2} \\right] \\hbar \\omega\n\\]\nAppealing to the general result (Key Point 11)\n\\[\nS = k \\ln Z + \\frac{\\bar{E}}{T}\n\\]\nWe then have\n\\[\nS = 3Nk \\left( \\ln \\left[ \\frac{e^{-x/2}}{1 - e^{-x}} \\right] + \\left[ \\frac{1}{e^x - 1} + \\frac{1}{2} \\right] \\frac{\\hbar \\omega}{kT} \\right)\n\\]\nSimplifying,\n\\[\nS = 3Nk \\left( \\frac{x}{e^x - 1} - \\ln \\left[ 1 - e^{-x} \\right] \\right)\n\\]\nFor low \\(T\\) (\\(x \\gg 1\\)) we have\n\\[\n\\frac{x}{e^x - 1} \\simeq xe^{-x} \\quad \\text{and} \\quad \\ln \\left[ 1 - e^{-x} \\right] \\simeq -e^{-x}\n\\]\nThe first of these terms dominates the second, so that\n\\[\nS \\simeq 3Nkxe^{-x}\n\\]\nwhich vanishes exponentially (as \\(e^{-\\hbar \\omega / kT}\\)) as \\(T \\to 0\\).\nFor high \\(T\\) (\\(x \\ll 1\\)),\n\\[\n\\frac{x}{e^x - 1} \\simeq 1 \\quad \\text{and} \\quad \\ln \\left[ 1 - e^{-x} \\right] \\simeq \\ln x\n\\]\nThis time the second term is the dominant one, so that\n\\[\nS \\simeq -3Nk \\ln x = 3Nk \\ln \\left( \\frac{kT}{\\hbar \\omega} \\right)\n\\]\nIn contrast to the magnet, the vibrational entropy continues to grow with increasing \\(T\\) (as \\(\\ln T\\)), manifesting the fact that the oscillators explore ever higher parts of their energy-level ladder as the temperature grows. Correspondingly, the heat capacity\n\\[\nC_V \\equiv T \\left( \\frac{\\partial S}{\\partial T} \\right)_V = T \\frac{\\partial}{\\partial T} \\left( 3Nk \\ln T \\right) = 3Nk\n\\]\nThis is the Dulong and Petit law; the limiting non-zero value for \\(C_V\\) is to be compared with the result for the magnet, where the heat capacity vanishes as \\(T \\to \\infty\\).\n\n\n\nAccording to the result established chapter 10.3, the entropy of one mole (\\(N = N_A\\)) of ideal gas in volume \\(V\\) at temperature \\(T\\) is\n\\[\nS(T) = R \\left[ \\ln \\left( \\frac{2\\pi MkT}{h^2} \\right)^{\\frac{3}{2}} - \\ln \\left( \\frac{N}{V} \\right) + \\frac{5}{2} \\right]\n\\]\nor\n\\[\nS = R \\left[ \\frac{3}{2} \\ln T - \\ln \\left( \\frac{N}{V} \\right) + \\frac{5}{2} + \\frac{3}{2} \\ln \\left( \\frac{2\\pi Mk}{h^2} \\right) \\right]\n\\]\nNow, setting \\(M = 6.62 \\times 10^{-26} \\, \\text{kg}\\), \\(kT_b = 1.205 \\times 10^{-21} \\, \\text{J}\\), and \\(V/N \\equiv V/N_A = kT/P = 1.18 \\times 10^{-26} \\, \\text{m}^3\\) (with \\(P = 1.013 \\times 10^5 \\, \\text{Nm}^{-2}\\)), we find \\(S(T_b) \\simeq 129.2 \\, \\text{J mol}^{-1} \\, \\text{K}^{-1}\\), in excellent agreement with the measured value.\n\n\n\nThe solutions to the Schrödinger equation for a particle confined to 2 dimensions are of the form:\n\\[\n\\phi(x, y) = B \\sin(k_x x) \\sin(k_y y) \\quad \\text{with} \\quad k_x = \\frac{n_x \\pi}{L} \\quad k_y = \\frac{n_y \\pi}{L}\n\\]\nwhere \\(n_x, n_y = 1, 2, 3 \\dots\\). We define\n\\[\n\\Gamma(k) dk \\equiv \\text{number of solutions with k-values in the range} \\ k \\to k + dk\n\\]\nThis quantity is identifiable (refer to the figure in Appendix C) as the number of allowed points in \\(k\\)-space lying in the shell bounded by circles of radii \\(k\\) and \\(k + dk\\). This is\n\\[\n\\Gamma(k) dk = \\frac{2\\pi k dk}{4} \\cdot \\frac{1}{(\\pi / L)^2}\n\\]\nThe first factor is the area of one quadrant of the shell; the second factor is the reciprocal of the area of the space associated with each point. Reorganizing, and identifying \\(A = L^2\\), gives\n\\[\n\\Gamma(k) dk = \\frac{A k dk}{2\\pi}\n\\]\nRecalling the definition of the density of states,\n\\[\ng(\\epsilon) d\\epsilon \\equiv \\text{number of states with energies in the range} \\ \\epsilon \\to \\epsilon + d\\epsilon\n\\]\nwe identify\n\\[\ng(\\epsilon) d\\epsilon = \\Gamma(k) dk \\quad \\text{with} \\quad \\epsilon = \\frac{\\hbar^2 k^2}{2M} \\quad \\text{and} \\quad d\\epsilon = \\frac{\\hbar^2 k dk}{M}\n\\]\nThen\n\\[\ng(\\epsilon) d\\epsilon = \\frac{A}{2\\pi} \\cdot k dk = \\frac{A}{2\\pi} \\cdot \\frac{M}{\\hbar^2} d\\epsilon\n\\]\nso that\n\\[\ng(\\epsilon) = \\frac{AM}{2\\pi \\hbar^2}\n\\]\nwhich is to be contrasted with the result for 3d where \\(g(\\epsilon)\\) increases as \\(\\epsilon^{1/2}\\).\nThe 2d gas is not just a theorist’s fantasy: there are a range of physical systems (atoms absorbed onto substrates, electrons in “quantum wells”, \\(\\dots\\)) to which this theory is relevant.\n\n\n\nThe Fermi-Dirac distribution is defined by\n\\[\nf_+(\\epsilon) = \\frac{1}{e^{\\beta(\\epsilon - \\mu)} + 1}\n\\]\nIt gives the mean number, \\(\\bar{n}\\), of fermions in a given state of energy \\(\\epsilon\\). The distribution encodes the exclusion principle which asserts that one “state” (one solution to the single-particle Schrödinger equation) can accommodate at most one fermion.\nLet \\(p_o(\\epsilon)\\) denote the probability that a particular state of energy \\(\\epsilon\\) is occupied. The probability it is unoccupied (“empty”) is then \\(p_e(\\epsilon) = 1 - p_o(\\epsilon)\\). The mean number of fermions in this state can then be written as\n\\[\n\\bar{n} = 0 \\times p_e(\\epsilon) + 1 \\times p_o(\\epsilon) = p_o(\\epsilon)\n\\]\nwhence \\(f_+(\\epsilon) = p_o(\\epsilon)\\) as claimed. Note that, consistent with this identification, the function \\(f_+(\\epsilon)\\) lies between 0 and 1.\nThe relevant plots at \\(T=0\\) and low \\(T&gt;0\\) are shownin chapter 13.\n\n\n\nThe mean total energy of a free electron (fermi) gas can be written as a sum over states:\n\\[\n\\overline{E} = \\sum_i \\overline{n_i} \\epsilon_i\n\\]\nwhere \\(\\overline{n_i} \\equiv f_+(\\epsilon_i)\\) is the mean number of electrons in state \\(i\\) of energy \\(\\epsilon_i\\). The sum can be replaced by an integral, using the density of states:\n\\[\n\\overline{E} = \\int_0^{\\infty} d\\epsilon \\, g(\\epsilon) f_+(\\epsilon) \\epsilon\n\\]\nwhich is the expression quoted. At \\(T = 0\\), \\(f_+(\\epsilon)\\) assumes the simple form\n\\[\nf_+(\\epsilon) =\n\\begin{cases}\n1 & \\epsilon &lt; \\epsilon_f \\\\\n0 & \\epsilon &gt; \\epsilon_f\n\\end{cases}\n\\]\nand the energy assumes the form\n\\[\n\\overline{E}(T = 0) = \\int_0^{\\epsilon_f} d\\epsilon \\, g(\\epsilon) \\epsilon\n\\]\nIt is useful to set this equation alongside the equation which determines \\(\\epsilon_f \\equiv \\mu (T = 0)\\), namely\n\\[\nN = \\int_0^{\\epsilon_f} d\\epsilon \\, g(\\epsilon)\n\\]\nNow we need only recall the form of \\(g(\\epsilon)\\) from chapter 10, namely\n\\[\ng(\\epsilon) = \\tilde{D} V \\epsilon^{1/2}\n\\]\nThen, dividing the equation for \\(\\bar{E}\\) by the equation for \\(N\\) we obtain,\n\\[\n\\frac{\\overline{E}(T = 0)}{N} = \\frac{\\int_0^{\\epsilon_f} d\\epsilon \\, \\epsilon^{3/2}}{\\int_0^{\\epsilon_f} d\\epsilon \\, \\epsilon^{1/2}} = \\frac{3}{5} \\epsilon_f\n\\]\nfrom which the constant \\(\\tilde{D}\\) has conveniently disappeared. Thus\n\\[\n\\overline{E}(T = 0) = \\frac{3N}{5} \\epsilon_f\n\\]\n\n\n\nThe number density (number-per-unit-volume) of electrons in the Na conduction electron gas is\n\\[\nn_e \\equiv \\frac{N}{V} = \\frac{6.02 \\times 10^{23}}{23.7 \\times 10^{-6}} = 2.54 \\times 10^{28} \\, m^{-3}\n\\]\nThe Fermi energy is determined by (cf. Key Point 19)\n\\[\n\\epsilon_f = \\frac{\\hbar^2 k_f^2}{2M}\n\\]\nwhere\n\\[\nk_f \\equiv (3 n_e \\pi^2)^{1/3}\n\\]\nis the magnitude of the wavevector of electrons with the Fermi energy. Inserting the value of \\(n_e\\), and the free-electron mass, we find \\(\\epsilon_f = 3.15 \\, \\text{eV}\\), which is typical of simple metals. It is large compared to \\(kT\\) at any reasonable temperature."
  },
  {
    "objectID": "solutions_set_4.html#statistical-mechanics-of-the-1d-harmonic-oscillator",
    "href": "solutions_set_4.html#statistical-mechanics-of-the-1d-harmonic-oscillator",
    "title": "Solutions to problem set 4",
    "section": "",
    "text": "The analysis is given in chapter 8. For high \\(T\\), in the sense that \\(\\hbar \\omega / kT \\ll 1\\), we can write\n\\[\ne^{\\hbar \\omega / kT} \\simeq 1 + \\frac{\\hbar \\omega}{kT} + O\\left( \\frac{\\hbar \\omega}{kT} \\right)^2\n\\]\nso that\n\\[\n\\bar{n} = \\frac{1}{1 + \\hbar \\omega / kT + \\dots - 1} = \\frac{kT}{\\hbar \\omega} \\gg 1\n\\]\nwhence \\(\\overline{\\varepsilon} \\simeq kT\\). The disappearance of \\(\\hbar\\) manifests the fact that this result is “classical”: it can be established by arguments which make no reference to the underlying quantum theory of the harmonic oscillator; they capture the high \\(T\\) behaviour correctly, but fail to predict the different behaviour that sets in, typically, as \\(kT\\) becomes small enough to “resolve” the discreteness of the energy level structure.\nThis result is an example of the Equipartition Theorem."
  },
  {
    "objectID": "solutions_set_4.html#entropy-of-the-vibrating-solid",
    "href": "solutions_set_4.html#entropy-of-the-vibrating-solid",
    "title": "Solutions to problem set 4",
    "section": "",
    "text": "The partition function for the 1d oscillator is (Chapter 8.2)\n\\[\nZ^{d=1}(1) = \\frac{e^{-x/2}}{1 - e^{-x}} \\quad \\text{with} \\quad x = \\frac{\\hbar \\omega}{kT}\n\\]\nThe partition function for \\(3N\\) 1d oscillators (or \\(N\\) 3d oscillators) is\n\\[\nZ = \\left[ Z^{d=1}(1) \\right]^{3N}\n\\]\nThe mean energy of one 1d oscillator is\n\\[\n\\overline{\\varepsilon}^{d=1} = \\left[ \\frac{1}{e^x - 1} + \\frac{1}{2} \\right] \\hbar \\omega\n\\]\nand the mean energy of \\(3N\\) such oscillators is\n\\[\n\\overline{E} = 3N \\left[ \\frac{1}{e^x - 1} + \\frac{1}{2} \\right] \\hbar \\omega\n\\]\nAppealing to the general result (Key Point 11)\n\\[\nS = k \\ln Z + \\frac{\\bar{E}}{T}\n\\]\nWe then have\n\\[\nS = 3Nk \\left( \\ln \\left[ \\frac{e^{-x/2}}{1 - e^{-x}} \\right] + \\left[ \\frac{1}{e^x - 1} + \\frac{1}{2} \\right] \\frac{\\hbar \\omega}{kT} \\right)\n\\]\nSimplifying,\n\\[\nS = 3Nk \\left( \\frac{x}{e^x - 1} - \\ln \\left[ 1 - e^{-x} \\right] \\right)\n\\]\nFor low \\(T\\) (\\(x \\gg 1\\)) we have\n\\[\n\\frac{x}{e^x - 1} \\simeq xe^{-x} \\quad \\text{and} \\quad \\ln \\left[ 1 - e^{-x} \\right] \\simeq -e^{-x}\n\\]\nThe first of these terms dominates the second, so that\n\\[\nS \\simeq 3Nkxe^{-x}\n\\]\nwhich vanishes exponentially (as \\(e^{-\\hbar \\omega / kT}\\)) as \\(T \\to 0\\).\nFor high \\(T\\) (\\(x \\ll 1\\)),\n\\[\n\\frac{x}{e^x - 1} \\simeq 1 \\quad \\text{and} \\quad \\ln \\left[ 1 - e^{-x} \\right] \\simeq \\ln x\n\\]\nThis time the second term is the dominant one, so that\n\\[\nS \\simeq -3Nk \\ln x = 3Nk \\ln \\left( \\frac{kT}{\\hbar \\omega} \\right)\n\\]\nIn contrast to the magnet, the vibrational entropy continues to grow with increasing \\(T\\) (as \\(\\ln T\\)), manifesting the fact that the oscillators explore ever higher parts of their energy-level ladder as the temperature grows. Correspondingly, the heat capacity\n\\[\nC_V \\equiv T \\left( \\frac{\\partial S}{\\partial T} \\right)_V = T \\frac{\\partial}{\\partial T} \\left( 3Nk \\ln T \\right) = 3Nk\n\\]\nThis is the Dulong and Petit law; the limiting non-zero value for \\(C_V\\) is to be compared with the result for the magnet, where the heat capacity vanishes as \\(T \\to \\infty\\)."
  },
  {
    "objectID": "solutions_set_4.html#a-check-on-the-entropy-formula",
    "href": "solutions_set_4.html#a-check-on-the-entropy-formula",
    "title": "Solutions to problem set 4",
    "section": "",
    "text": "According to the result established chapter 10.3, the entropy of one mole (\\(N = N_A\\)) of ideal gas in volume \\(V\\) at temperature \\(T\\) is\n\\[\nS(T) = R \\left[ \\ln \\left( \\frac{2\\pi MkT}{h^2} \\right)^{\\frac{3}{2}} - \\ln \\left( \\frac{N}{V} \\right) + \\frac{5}{2} \\right]\n\\]\nor\n\\[\nS = R \\left[ \\frac{3}{2} \\ln T - \\ln \\left( \\frac{N}{V} \\right) + \\frac{5}{2} + \\frac{3}{2} \\ln \\left( \\frac{2\\pi Mk}{h^2} \\right) \\right]\n\\]\nNow, setting \\(M = 6.62 \\times 10^{-26} \\, \\text{kg}\\), \\(kT_b = 1.205 \\times 10^{-21} \\, \\text{J}\\), and \\(V/N \\equiv V/N_A = kT/P = 1.18 \\times 10^{-26} \\, \\text{m}^3\\) (with \\(P = 1.013 \\times 10^5 \\, \\text{Nm}^{-2}\\)), we find \\(S(T_b) \\simeq 129.2 \\, \\text{J mol}^{-1} \\, \\text{K}^{-1}\\), in excellent agreement with the measured value."
  },
  {
    "objectID": "solutions_set_4.html#density-of-states-for-a-2d-gas",
    "href": "solutions_set_4.html#density-of-states-for-a-2d-gas",
    "title": "Solutions to problem set 4",
    "section": "",
    "text": "The solutions to the Schrödinger equation for a particle confined to 2 dimensions are of the form:\n\\[\n\\phi(x, y) = B \\sin(k_x x) \\sin(k_y y) \\quad \\text{with} \\quad k_x = \\frac{n_x \\pi}{L} \\quad k_y = \\frac{n_y \\pi}{L}\n\\]\nwhere \\(n_x, n_y = 1, 2, 3 \\dots\\). We define\n\\[\n\\Gamma(k) dk \\equiv \\text{number of solutions with k-values in the range} \\ k \\to k + dk\n\\]\nThis quantity is identifiable (refer to the figure in Appendix C) as the number of allowed points in \\(k\\)-space lying in the shell bounded by circles of radii \\(k\\) and \\(k + dk\\). This is\n\\[\n\\Gamma(k) dk = \\frac{2\\pi k dk}{4} \\cdot \\frac{1}{(\\pi / L)^2}\n\\]\nThe first factor is the area of one quadrant of the shell; the second factor is the reciprocal of the area of the space associated with each point. Reorganizing, and identifying \\(A = L^2\\), gives\n\\[\n\\Gamma(k) dk = \\frac{A k dk}{2\\pi}\n\\]\nRecalling the definition of the density of states,\n\\[\ng(\\epsilon) d\\epsilon \\equiv \\text{number of states with energies in the range} \\ \\epsilon \\to \\epsilon + d\\epsilon\n\\]\nwe identify\n\\[\ng(\\epsilon) d\\epsilon = \\Gamma(k) dk \\quad \\text{with} \\quad \\epsilon = \\frac{\\hbar^2 k^2}{2M} \\quad \\text{and} \\quad d\\epsilon = \\frac{\\hbar^2 k dk}{M}\n\\]\nThen\n\\[\ng(\\epsilon) d\\epsilon = \\frac{A}{2\\pi} \\cdot k dk = \\frac{A}{2\\pi} \\cdot \\frac{M}{\\hbar^2} d\\epsilon\n\\]\nso that\n\\[\ng(\\epsilon) = \\frac{AM}{2\\pi \\hbar^2}\n\\]\nwhich is to be contrasted with the result for 3d where \\(g(\\epsilon)\\) increases as \\(\\epsilon^{1/2}\\).\nThe 2d gas is not just a theorist’s fantasy: there are a range of physical systems (atoms absorbed onto substrates, electrons in “quantum wells”, \\(\\dots\\)) to which this theory is relevant."
  },
  {
    "objectID": "solutions_set_4.html#the-fermi-dirac-distribution",
    "href": "solutions_set_4.html#the-fermi-dirac-distribution",
    "title": "Solutions to problem set 4",
    "section": "",
    "text": "The Fermi-Dirac distribution is defined by\n\\[\nf_+(\\epsilon) = \\frac{1}{e^{\\beta(\\epsilon - \\mu)} + 1}\n\\]\nIt gives the mean number, \\(\\bar{n}\\), of fermions in a given state of energy \\(\\epsilon\\). The distribution encodes the exclusion principle which asserts that one “state” (one solution to the single-particle Schrödinger equation) can accommodate at most one fermion.\nLet \\(p_o(\\epsilon)\\) denote the probability that a particular state of energy \\(\\epsilon\\) is occupied. The probability it is unoccupied (“empty”) is then \\(p_e(\\epsilon) = 1 - p_o(\\epsilon)\\). The mean number of fermions in this state can then be written as\n\\[\n\\bar{n} = 0 \\times p_e(\\epsilon) + 1 \\times p_o(\\epsilon) = p_o(\\epsilon)\n\\]\nwhence \\(f_+(\\epsilon) = p_o(\\epsilon)\\) as claimed. Note that, consistent with this identification, the function \\(f_+(\\epsilon)\\) lies between 0 and 1.\nThe relevant plots at \\(T=0\\) and low \\(T&gt;0\\) are shownin chapter 13."
  },
  {
    "objectID": "solutions_set_4.html#energy-of-the-electron-gas",
    "href": "solutions_set_4.html#energy-of-the-electron-gas",
    "title": "Solutions to problem set 4",
    "section": "",
    "text": "The mean total energy of a free electron (fermi) gas can be written as a sum over states:\n\\[\n\\overline{E} = \\sum_i \\overline{n_i} \\epsilon_i\n\\]\nwhere \\(\\overline{n_i} \\equiv f_+(\\epsilon_i)\\) is the mean number of electrons in state \\(i\\) of energy \\(\\epsilon_i\\). The sum can be replaced by an integral, using the density of states:\n\\[\n\\overline{E} = \\int_0^{\\infty} d\\epsilon \\, g(\\epsilon) f_+(\\epsilon) \\epsilon\n\\]\nwhich is the expression quoted. At \\(T = 0\\), \\(f_+(\\epsilon)\\) assumes the simple form\n\\[\nf_+(\\epsilon) =\n\\begin{cases}\n1 & \\epsilon &lt; \\epsilon_f \\\\\n0 & \\epsilon &gt; \\epsilon_f\n\\end{cases}\n\\]\nand the energy assumes the form\n\\[\n\\overline{E}(T = 0) = \\int_0^{\\epsilon_f} d\\epsilon \\, g(\\epsilon) \\epsilon\n\\]\nIt is useful to set this equation alongside the equation which determines \\(\\epsilon_f \\equiv \\mu (T = 0)\\), namely\n\\[\nN = \\int_0^{\\epsilon_f} d\\epsilon \\, g(\\epsilon)\n\\]\nNow we need only recall the form of \\(g(\\epsilon)\\) from chapter 10, namely\n\\[\ng(\\epsilon) = \\tilde{D} V \\epsilon^{1/2}\n\\]\nThen, dividing the equation for \\(\\bar{E}\\) by the equation for \\(N\\) we obtain,\n\\[\n\\frac{\\overline{E}(T = 0)}{N} = \\frac{\\int_0^{\\epsilon_f} d\\epsilon \\, \\epsilon^{3/2}}{\\int_0^{\\epsilon_f} d\\epsilon \\, \\epsilon^{1/2}} = \\frac{3}{5} \\epsilon_f\n\\]\nfrom which the constant \\(\\tilde{D}\\) has conveniently disappeared. Thus\n\\[\n\\overline{E}(T = 0) = \\frac{3N}{5} \\epsilon_f\n\\]"
  },
  {
    "objectID": "solutions_set_4.html#ideal-fermi-gas-model-of-conduction-electrons-in-na",
    "href": "solutions_set_4.html#ideal-fermi-gas-model-of-conduction-electrons-in-na",
    "title": "Solutions to problem set 4",
    "section": "",
    "text": "The number density (number-per-unit-volume) of electrons in the Na conduction electron gas is\n\\[\nn_e \\equiv \\frac{N}{V} = \\frac{6.02 \\times 10^{23}}{23.7 \\times 10^{-6}} = 2.54 \\times 10^{28} \\, m^{-3}\n\\]\nThe Fermi energy is determined by (cf. Key Point 19)\n\\[\n\\epsilon_f = \\frac{\\hbar^2 k_f^2}{2M}\n\\]\nwhere\n\\[\nk_f \\equiv (3 n_e \\pi^2)^{1/3}\n\\]\nis the magnitude of the wavevector of electrons with the Fermi energy. Inserting the value of \\(n_e\\), and the free-electron mass, we find \\(\\epsilon_f = 3.15 \\, \\text{eV}\\), which is typical of simple metals. It is large compared to \\(kT\\) at any reasonable temperature."
  },
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "3. Entropy, equilibrium, and the second law",
    "section": "",
    "text": "See Mandl chapter 2\n\n\nWe can now formulate the axiomatic foundation of statistical physics:\n\n\n\n\n\n\nKey Point 5: The principle of equal a priori probabilities:\n\n\n\nAn isolated system is equally likely to be found in any one of its allowed microstates.\n\n\nAlthough we state it here as a postulate, it can be justified through the degree of belief view of probabilities: our microscopic model prescribes the available microstates which is the most detailed description of the state we can (or are prepared) to offer. Since we have no further information we have to assume that each of these microstates is equally likely. Recalling that the weight of a macrostate is the number of corresponding microstates, a direct consequence of key point 5 is\n\n\n\n\n\n\nKey Point 6:\n\n\n\nThe probability that an isolated system, of energy \\(E\\), will be found in a macrostate \\(N,E,\\{\\alpha\\}\\) is proportional to the weight \\(\\Omega(N,E,\\{\\alpha\\})\\), ie the number of microstates associated with the macrostate \\(N,E,\\{\\alpha\\}\\).\n\n\nIn other words we can think of the microstate as the outcome of a trial (measuring which microstate our system is in) and the number of outcomes corresponding to a macrostate is given by the weight. Thus the probability of being in a macrostate is proportional to the weight. Since different macrostates have different weights they will have different probabilities.\nWe saw that since weight functions are typically exponentially large in \\(N\\), the maximum for the weight function is extremely sharp. This means with macrostates in which the (free) macroscopic variables maximise the weight function are overwhelming more probable than other possible values. This was illustrated in chapter 2 where we plotted the weight function of the model magnet link to image and link to image.\nThe probability that an isolated system will be found in macrostate \\(N, E, \\{\\alpha\\}\\) is proportional to the weight of macrostate \\(\\Omega(N, E, \\{\\alpha\\})\\). From the sharpening of the weight functions we deduce another key point\n\n\n\n\n\n\nKey Point 7:\n\n\n\nThe equilibrium values \\(\\{\\alpha^\\star\\}\\) of the macroscopic properties \\(\\{\\alpha\\}\\) of an isolated system are those that maximise the weight function \\(\\Omega(N,E,\\{\\alpha\\})\\).\n\n\nThus while, in principle, macroscopic properties \\(\\Omega(N,E,\\{\\alpha\\})\\) are free to take up a wide range of values, in practice they are sharply defined so that equilibrium is characterised by macroscopic properties unique to the equilibrium state that is singled out by the constraints.\nSaying that a thermodynamic system is ‘in equilibrium’ is saying (essentially) that enough time has elapsed since the last change in constraints for the system to find its way to the most probable macrostate consistent with those constraints.\nA consequence of this, together with KP4, is that\n\n\n\n\n\n\nKey Point 8:\n\n\n\nThe equilibrium entropy of an isolated system of \\(N\\) constituents with energy \\(E\\) is \\(S(N,E)=k\\ln\\Omega(N,E,\\{\\alpha^\\star\\})\\)\n\n\n\n\n\nTo illustrate the second law we consider the model magnet of the previous chapter: \\(N\\) atoms (magnetic dipoles), energy \\(E\\) and \\(n\\) atoms in excited states. Let us consider starting from an initial condition where all the excited dipoles are in the left hand half of the system and none in the right. One can think of inserting a thermally insulating wall to ensure that no energy can be transferred between the two sub-systems.\nNow when we remove the wall the system finds itself in a macrostate which is far from the most probable. The system will evolve, by more and more of the excited dipoles appearing in the right hand half, through macrostates which have greater and greater weights. Finally it reaches the macrostate where there are equal numbers of excited dipoles in the left and right hand sides and which is the macrostate with the maximal weight. Of course when we say equal we mean equal to within O(\\(N^{1/2}\\)).\nIn effect there has been a flow of energy from the left hand side to the right hand side of the system until equilibrium was reached. A related example is of a gas starting with all molecules in the left hand half of the system. Similar considerations apply to that system although now there is a flow of particles or mass from left to right.\nWe summarise with a statement not quite worthy of being a key point, as we will not pursue dynamics much further, but which is nevertheless worth a box\n\n\n\n\n\n\nAn isolated system evolves through macrostates with greater and greater weight until it reaches the macrostate with the largest weight and stays there.\n\n\n\nNow considering the entropy, which is by Planck’s relation proportional to the logarithm of the weight we see that\nThe entropy of an isolated system can only increase\nwhich is a statement of the second law of thermodynamics. Thus on the approach to equilibrium, the entropy rises to its equilibrium value \\(S(N,E)=k\\ln\\Omega(N,E,\\{\\alpha^\\star\\})\\), whereafter it remains constant. Moreover we now understand why the law holds only in a statistical sense. In the example of the ideal gas there is nothing in Newton’s laws to prevent all the molecules moving into the left hand side, which would mean a decrease in entropy. However we know statistically that this will basically never happen.\n\n\n\nEntropy is often referred to as a measure of disorder. For this statement to be meaningful we have to know what we mean by ordered and disordered.\nBasically the idea is that when something is ‘disordered’ it may be so in many different ways, whereas ‘order’ places many restrictions on the ways things can exist.\nConsider the example of a bedroom. For a bedroom to be tidy i.e. ordered, everything has to be in its correct place, and there are not many ways to place ones possessions and clothes correctly in the room. However in an untidy, i.e. disordered room, there are many, many different ways for things to be scattered about.\nAnother example is problem question 2.6 where a order/disorder transition is discussed. There copper and gold atoms are arranged in a crystal lattice. In the ordered phase the copper atoms occupy certain positions and the gold atoms occupy other positions. This is an orderly arrangement compared to the disordered phase where any atom can occupy any of the positions. In that question entropy differences are calculated.\nQuestion 2.5 highlights that a liquid (water) is more disordered than a solid (ice).\n\n\n\nConsider dividing our system (whether it be the model magnet or ideal gas) in half i.e. into two subsystems 1 and 2, each with number \\(N_i\\) of constituents and containing energy \\(E_i\\) where \\(i = 1, 2\\). For example, in we divide our array of \\(N\\) magnetic atoms into two sub-systems of fixed \\(N_1 = N_2 = N/2\\) which are in thermal contact (they exchange energy). The fixed total energy corresponds to a total of say \\(n = N/2\\) excited dipoles. The weight for the macrostate where \\(n_1\\) of the dipoles in the left half are excited is a product of weight functions for the two halves\n\\[\n\\Omega (N, E, n_1) = \\Omega_1(N_1, E_1, n_1)\\Omega_2(N_2, E_2,n − n_1)\n\\] This factorisation of the weight function into two terms from each subsystem is quite general. Thus using Planck’s relation we see that for the two sub-systems\n\\[\nS(N, E, n_1) = S_1(N_1, E_1, n_1) + S_2(N_2, E_2, n_2).\n\\]\nThis property of the entropy being the sum of the entropy of the subsystems is quite general and is referred to as additivity of the entropy i.e. the entropy of a composite system is the sum of entropies of macroscopic component subsystems. In particular in an equilibrium system, where the energies of the two sub systems are \\(E_1, E_2 =\nE_1^\\star, E_2^\\star\\)\n\\[\nS(N,E)=S(N_1,E_1^\\star)+S(N_2,E_2^\\star)\n\\]\n\n\n\nWe now analyse the condition for equilibrium more mathematically. For our two systems in thermal equilibrium we have\n\\[\nS(E) = S_1(E_1) + S_2(E_2)\\:\\:\\:\\:\\:\\:\\: E = E_1 + E_2.\n\\] where we have been lazy and dropped the \\(Ns\\) since we take them to be fixed.\nWe wish to consider the effect of changing \\(E_1\\) to \\(E_1 + dE_1\\) which implies changing \\(E_2\\) by \\(dE_2 = −dE_1\\), since total \\(E\\) is fixed.\nNow consider the effect on the entropy \\[\\begin{eqnarray*}\ndS &=& dS_1+dS_2\\\\\n\\: &=& \\left(\\frac{\\partial S_1}{\\partial E_1}\\right) dE_1+\\left(\\frac{\\partial S_2}{\\partial E_2}\\right) dE_2\\\\\n\\:  &=&  \\left(\\frac{\\partial S_1}{\\partial E_1}-\\frac{\\partial S_2}{\\partial E_2}\\right) dE_1\n\\end{eqnarray*}\\]\nNow since the entropy is an extremum (i.e. it should be maximised) \\(dS\\) must be \\(0\\). Therefore we deduce \\[\n\\frac{\\partial S_1}{\\partial E_1}=\\frac{\\partial S_2}{\\partial E_2}\n\\] Recalling the zeroth law of thermodynamics, which states that two systems in thermal equilibrium must share the same temperature, and comparing with the condition for equilibrium that we have derived, we deduce that \\[\n\\frac{\\partial S_1}{\\partial E_1} = \\text{some~function ~of~}T\n\\] In fact the correct identification is\n\n\n\n\n\n\nKey Point 9:\n\n\n\n\\[\\frac{1}{T}=\\frac{\\partial S}{\\partial E}\\]\n\n\n(To understand why this particular function of \\(T\\) is the correct one will need the ideas of free energy minimisation that we shall meet later and in question 3.5)\nWe now ask what happens when we put two systems at different temperatures \\(T_1\\) and \\(T_2\\) in thermal contact. In this case we expect from the second law that \\(dS &gt; 0\\).\nUsing the identification of key point 9 we find \\[\ndS=\\left(\\frac{1}{T_1}-\\frac{1}{T_2}\\right)dE_1\n\\]\nSo if \\(T_1 &gt;T_2\\) we must have \\(dE_1\\) −ve for \\(dS\\) to be \\(+ve\\) and we see energy (or heat) flows from the hotter to the colder system.\nOne can generalises the arguments of this subsection to the consideration of systems free to exchange volume (e.g. a balloon) or particles (e.g. a liquid and gas in coexistence).",
    "crumbs": [
      "Chapters",
      "3. Entropy, equilibrium, and the second Law"
    ]
  },
  {
    "objectID": "chapter3.html#principle-of-equal-a-priori-probabilities-and-its-consequences",
    "href": "chapter3.html#principle-of-equal-a-priori-probabilities-and-its-consequences",
    "title": "3. Entropy, equilibrium, and the second law",
    "section": "",
    "text": "We can now formulate the axiomatic foundation of statistical physics:\n\n\n\n\n\n\nKey Point 5: The principle of equal a priori probabilities:\n\n\n\nAn isolated system is equally likely to be found in any one of its allowed microstates.\n\n\nAlthough we state it here as a postulate, it can be justified through the degree of belief view of probabilities: our microscopic model prescribes the available microstates which is the most detailed description of the state we can (or are prepared) to offer. Since we have no further information we have to assume that each of these microstates is equally likely. Recalling that the weight of a macrostate is the number of corresponding microstates, a direct consequence of key point 5 is\n\n\n\n\n\n\nKey Point 6:\n\n\n\nThe probability that an isolated system, of energy \\(E\\), will be found in a macrostate \\(N,E,\\{\\alpha\\}\\) is proportional to the weight \\(\\Omega(N,E,\\{\\alpha\\})\\), ie the number of microstates associated with the macrostate \\(N,E,\\{\\alpha\\}\\).\n\n\nIn other words we can think of the microstate as the outcome of a trial (measuring which microstate our system is in) and the number of outcomes corresponding to a macrostate is given by the weight. Thus the probability of being in a macrostate is proportional to the weight. Since different macrostates have different weights they will have different probabilities.\nWe saw that since weight functions are typically exponentially large in \\(N\\), the maximum for the weight function is extremely sharp. This means with macrostates in which the (free) macroscopic variables maximise the weight function are overwhelming more probable than other possible values. This was illustrated in chapter 2 where we plotted the weight function of the model magnet link to image and link to image.\nThe probability that an isolated system will be found in macrostate \\(N, E, \\{\\alpha\\}\\) is proportional to the weight of macrostate \\(\\Omega(N, E, \\{\\alpha\\})\\). From the sharpening of the weight functions we deduce another key point\n\n\n\n\n\n\nKey Point 7:\n\n\n\nThe equilibrium values \\(\\{\\alpha^\\star\\}\\) of the macroscopic properties \\(\\{\\alpha\\}\\) of an isolated system are those that maximise the weight function \\(\\Omega(N,E,\\{\\alpha\\})\\).\n\n\nThus while, in principle, macroscopic properties \\(\\Omega(N,E,\\{\\alpha\\})\\) are free to take up a wide range of values, in practice they are sharply defined so that equilibrium is characterised by macroscopic properties unique to the equilibrium state that is singled out by the constraints.\nSaying that a thermodynamic system is ‘in equilibrium’ is saying (essentially) that enough time has elapsed since the last change in constraints for the system to find its way to the most probable macrostate consistent with those constraints.\nA consequence of this, together with KP4, is that\n\n\n\n\n\n\nKey Point 8:\n\n\n\nThe equilibrium entropy of an isolated system of \\(N\\) constituents with energy \\(E\\) is \\(S(N,E)=k\\ln\\Omega(N,E,\\{\\alpha^\\star\\})\\)",
    "crumbs": [
      "Chapters",
      "3. Entropy, equilibrium, and the second Law"
    ]
  },
  {
    "objectID": "chapter3.html#the-second-law",
    "href": "chapter3.html#the-second-law",
    "title": "3. Entropy, equilibrium, and the second law",
    "section": "",
    "text": "To illustrate the second law we consider the model magnet of the previous chapter: \\(N\\) atoms (magnetic dipoles), energy \\(E\\) and \\(n\\) atoms in excited states. Let us consider starting from an initial condition where all the excited dipoles are in the left hand half of the system and none in the right. One can think of inserting a thermally insulating wall to ensure that no energy can be transferred between the two sub-systems.\nNow when we remove the wall the system finds itself in a macrostate which is far from the most probable. The system will evolve, by more and more of the excited dipoles appearing in the right hand half, through macrostates which have greater and greater weights. Finally it reaches the macrostate where there are equal numbers of excited dipoles in the left and right hand sides and which is the macrostate with the maximal weight. Of course when we say equal we mean equal to within O(\\(N^{1/2}\\)).\nIn effect there has been a flow of energy from the left hand side to the right hand side of the system until equilibrium was reached. A related example is of a gas starting with all molecules in the left hand half of the system. Similar considerations apply to that system although now there is a flow of particles or mass from left to right.\nWe summarise with a statement not quite worthy of being a key point, as we will not pursue dynamics much further, but which is nevertheless worth a box\n\n\n\n\n\n\nAn isolated system evolves through macrostates with greater and greater weight until it reaches the macrostate with the largest weight and stays there.\n\n\n\nNow considering the entropy, which is by Planck’s relation proportional to the logarithm of the weight we see that\nThe entropy of an isolated system can only increase\nwhich is a statement of the second law of thermodynamics. Thus on the approach to equilibrium, the entropy rises to its equilibrium value \\(S(N,E)=k\\ln\\Omega(N,E,\\{\\alpha^\\star\\})\\), whereafter it remains constant. Moreover we now understand why the law holds only in a statistical sense. In the example of the ideal gas there is nothing in Newton’s laws to prevent all the molecules moving into the left hand side, which would mean a decrease in entropy. However we know statistically that this will basically never happen.",
    "crumbs": [
      "Chapters",
      "3. Entropy, equilibrium, and the second Law"
    ]
  },
  {
    "objectID": "chapter3.html#entropy-and-disorder",
    "href": "chapter3.html#entropy-and-disorder",
    "title": "3. Entropy, equilibrium, and the second law",
    "section": "",
    "text": "Entropy is often referred to as a measure of disorder. For this statement to be meaningful we have to know what we mean by ordered and disordered.\nBasically the idea is that when something is ‘disordered’ it may be so in many different ways, whereas ‘order’ places many restrictions on the ways things can exist.\nConsider the example of a bedroom. For a bedroom to be tidy i.e. ordered, everything has to be in its correct place, and there are not many ways to place ones possessions and clothes correctly in the room. However in an untidy, i.e. disordered room, there are many, many different ways for things to be scattered about.\nAnother example is problem question 2.6 where a order/disorder transition is discussed. There copper and gold atoms are arranged in a crystal lattice. In the ordered phase the copper atoms occupy certain positions and the gold atoms occupy other positions. This is an orderly arrangement compared to the disordered phase where any atom can occupy any of the positions. In that question entropy differences are calculated.\nQuestion 2.5 highlights that a liquid (water) is more disordered than a solid (ice).",
    "crumbs": [
      "Chapters",
      "3. Entropy, equilibrium, and the second Law"
    ]
  },
  {
    "objectID": "chapter3.html#additivity-of-entropy",
    "href": "chapter3.html#additivity-of-entropy",
    "title": "3. Entropy, equilibrium, and the second law",
    "section": "",
    "text": "Consider dividing our system (whether it be the model magnet or ideal gas) in half i.e. into two subsystems 1 and 2, each with number \\(N_i\\) of constituents and containing energy \\(E_i\\) where \\(i = 1, 2\\). For example, in we divide our array of \\(N\\) magnetic atoms into two sub-systems of fixed \\(N_1 = N_2 = N/2\\) which are in thermal contact (they exchange energy). The fixed total energy corresponds to a total of say \\(n = N/2\\) excited dipoles. The weight for the macrostate where \\(n_1\\) of the dipoles in the left half are excited is a product of weight functions for the two halves\n\\[\n\\Omega (N, E, n_1) = \\Omega_1(N_1, E_1, n_1)\\Omega_2(N_2, E_2,n − n_1)\n\\] This factorisation of the weight function into two terms from each subsystem is quite general. Thus using Planck’s relation we see that for the two sub-systems\n\\[\nS(N, E, n_1) = S_1(N_1, E_1, n_1) + S_2(N_2, E_2, n_2).\n\\]\nThis property of the entropy being the sum of the entropy of the subsystems is quite general and is referred to as additivity of the entropy i.e. the entropy of a composite system is the sum of entropies of macroscopic component subsystems. In particular in an equilibrium system, where the energies of the two sub systems are \\(E_1, E_2 =\nE_1^\\star, E_2^\\star\\)\n\\[\nS(N,E)=S(N_1,E_1^\\star)+S(N_2,E_2^\\star)\n\\]",
    "crumbs": [
      "Chapters",
      "3. Entropy, equilibrium, and the second Law"
    ]
  },
  {
    "objectID": "chapter3.html#entropy-and-temperature",
    "href": "chapter3.html#entropy-and-temperature",
    "title": "3. Entropy, equilibrium, and the second law",
    "section": "",
    "text": "We now analyse the condition for equilibrium more mathematically. For our two systems in thermal equilibrium we have\n\\[\nS(E) = S_1(E_1) + S_2(E_2)\\:\\:\\:\\:\\:\\:\\: E = E_1 + E_2.\n\\] where we have been lazy and dropped the \\(Ns\\) since we take them to be fixed.\nWe wish to consider the effect of changing \\(E_1\\) to \\(E_1 + dE_1\\) which implies changing \\(E_2\\) by \\(dE_2 = −dE_1\\), since total \\(E\\) is fixed.\nNow consider the effect on the entropy \\[\\begin{eqnarray*}\ndS &=& dS_1+dS_2\\\\\n\\: &=& \\left(\\frac{\\partial S_1}{\\partial E_1}\\right) dE_1+\\left(\\frac{\\partial S_2}{\\partial E_2}\\right) dE_2\\\\\n\\:  &=&  \\left(\\frac{\\partial S_1}{\\partial E_1}-\\frac{\\partial S_2}{\\partial E_2}\\right) dE_1\n\\end{eqnarray*}\\]\nNow since the entropy is an extremum (i.e. it should be maximised) \\(dS\\) must be \\(0\\). Therefore we deduce \\[\n\\frac{\\partial S_1}{\\partial E_1}=\\frac{\\partial S_2}{\\partial E_2}\n\\] Recalling the zeroth law of thermodynamics, which states that two systems in thermal equilibrium must share the same temperature, and comparing with the condition for equilibrium that we have derived, we deduce that \\[\n\\frac{\\partial S_1}{\\partial E_1} = \\text{some~function ~of~}T\n\\] In fact the correct identification is\n\n\n\n\n\n\nKey Point 9:\n\n\n\n\\[\\frac{1}{T}=\\frac{\\partial S}{\\partial E}\\]\n\n\n(To understand why this particular function of \\(T\\) is the correct one will need the ideas of free energy minimisation that we shall meet later and in question 3.5)\nWe now ask what happens when we put two systems at different temperatures \\(T_1\\) and \\(T_2\\) in thermal contact. In this case we expect from the second law that \\(dS &gt; 0\\).\nUsing the identification of key point 9 we find \\[\ndS=\\left(\\frac{1}{T_1}-\\frac{1}{T_2}\\right)dE_1\n\\]\nSo if \\(T_1 &gt;T_2\\) we must have \\(dE_1\\) −ve for \\(dS\\) to be \\(+ve\\) and we see energy (or heat) flows from the hotter to the colder system.\nOne can generalises the arguments of this subsection to the consideration of systems free to exchange volume (e.g. a balloon) or particles (e.g. a liquid and gas in coexistence).",
    "crumbs": [
      "Chapters",
      "3. Entropy, equilibrium, and the second Law"
    ]
  },
  {
    "objectID": "Problem_set_1.html",
    "href": "Problem_set_1.html",
    "title": "Revision of probability and probability distributions - Problem set 1",
    "section": "",
    "text": "Write down the form of the binomial distribution \\(P_n\\) giving the probability of observing precisely \\(n\\) instances of an event in a total of \\(N\\) trials each of which may give that event with probability \\(p\\).\nWrite down expressions for the mean \\(\\bar{n} \\equiv \\sum_{n=0}^{N} n P_n\\) and variance \\(\\Delta n^2 \\equiv \\sum_{n=0}^{N} (n - \\bar{n})^2 P_n\\) of this distribution.\n\n\n\n\nWhat is the probability that in \\(N\\) trials, each with probability of an event \\(p\\), one sees at least one instance of an event?\nWhen is \\(1 - \\exp(-Np)\\) a good approximation to this probability?\nWhen is \\(Np\\) a good approximation to this probability?\n\n\n\n\nConsider the binomial distribution discussed above. Use Stirling’s approximation to show that\n\\[\ns(n) \\equiv \\ln P_n \\simeq n \\ln p + (N - n) \\ln (1 - p) + N \\ln N - (N - n) \\ln (N - n) - n \\ln n\n\\]\nfor large \\(N, n\\).\nShow that\n\\[\ns'(n) = \\ln p - \\ln (1 - p) + \\ln (N - n) - \\ln n\n\\]\n\\[\ns''(n) = - \\frac{N}{n (N - n)}\n\\]\nNow make a Taylor expansion to second order in \\(x = n - Np\\) to give\n\\[\ns(x) \\simeq s(Np) - \\frac{x^2}{2N(1 - p)p}\n\\]\n(why is there no linear term in \\(x\\)?).\nThus deduce\n\\[\nP_n \\simeq P_{Np} \\exp\\left( - \\frac{(n - Np)^2}{2N(1 - p)p} \\right)\n\\]\n\n\n\nAn inebriated customer exits a pub and lurches from one lamp post to another on his way home; at each lamp post he pauses for reflection before starting out again. Each time he starts he is equally likely to move towards or away from home. If the posts are separated by a distance \\(a\\), find the mean and the standard deviation of his displacement \\(d\\) from the starting point, after \\(N\\) steps (i.e. encounters with a lamppost).\nIn fact the random walk plays a key role in our understanding of how randomness affects our lives. See “The Drunkards walk” by L. Mlodinow\n\n\n\nConsider a system of \\(N\\) particles, each of which can exist in either of two states, of energies \\(0\\) and \\(\\hat{\\epsilon}\\). Suppose that each particle is equally likely to be in either of its two states. Write down expressions for:\n\nthe mean energy of one particle, and its standard deviation;\nthe mean energy of the system, and its standard deviation.\n\nComment.\n\n\n\nConsider a cubic box of side \\(L = 1\\)m containing \\(N = 6 \\times 10^{23}\\) molecules. Estimate the probability of finding all the molecules in the left-hand half of the box.\nHow long might you expect to wait before observing one of the special arrangements whose probability you have just estimated.\n[Help: Under typical conditions it might take a molecule of the order of \\(10^3\\) s to diffuse from one half of the box to the other.]",
    "crumbs": [
      "Problems",
      "Week 20: Problem set 1"
    ]
  },
  {
    "objectID": "Problem_set_1.html#properties-of-the-binomial-distribution",
    "href": "Problem_set_1.html#properties-of-the-binomial-distribution",
    "title": "Revision of probability and probability distributions - Problem set 1",
    "section": "",
    "text": "Write down the form of the binomial distribution \\(P_n\\) giving the probability of observing precisely \\(n\\) instances of an event in a total of \\(N\\) trials each of which may give that event with probability \\(p\\).\nWrite down expressions for the mean \\(\\bar{n} \\equiv \\sum_{n=0}^{N} n P_n\\) and variance \\(\\Delta n^2 \\equiv \\sum_{n=0}^{N} (n - \\bar{n})^2 P_n\\) of this distribution.",
    "crumbs": [
      "Problems",
      "Week 20: Problem set 1"
    ]
  },
  {
    "objectID": "Problem_set_1.html#thinking-about-probabilities",
    "href": "Problem_set_1.html#thinking-about-probabilities",
    "title": "Revision of probability and probability distributions - Problem set 1",
    "section": "",
    "text": "What is the probability that in \\(N\\) trials, each with probability of an event \\(p\\), one sees at least one instance of an event?\nWhen is \\(1 - \\exp(-Np)\\) a good approximation to this probability?\nWhen is \\(Np\\) a good approximation to this probability?",
    "crumbs": [
      "Problems",
      "Week 20: Problem set 1"
    ]
  },
  {
    "objectID": "Problem_set_1.html#the-gaussian-distribution",
    "href": "Problem_set_1.html#the-gaussian-distribution",
    "title": "Revision of probability and probability distributions - Problem set 1",
    "section": "",
    "text": "Consider the binomial distribution discussed above. Use Stirling’s approximation to show that\n\\[\ns(n) \\equiv \\ln P_n \\simeq n \\ln p + (N - n) \\ln (1 - p) + N \\ln N - (N - n) \\ln (N - n) - n \\ln n\n\\]\nfor large \\(N, n\\).\nShow that\n\\[\ns'(n) = \\ln p - \\ln (1 - p) + \\ln (N - n) - \\ln n\n\\]\n\\[\ns''(n) = - \\frac{N}{n (N - n)}\n\\]\nNow make a Taylor expansion to second order in \\(x = n - Np\\) to give\n\\[\ns(x) \\simeq s(Np) - \\frac{x^2}{2N(1 - p)p}\n\\]\n(why is there no linear term in \\(x\\)?).\nThus deduce\n\\[\nP_n \\simeq P_{Np} \\exp\\left( - \\frac{(n - Np)^2}{2N(1 - p)p} \\right)\n\\]",
    "crumbs": [
      "Problems",
      "Week 20: Problem set 1"
    ]
  },
  {
    "objectID": "Problem_set_1.html#the-random-walk",
    "href": "Problem_set_1.html#the-random-walk",
    "title": "Revision of probability and probability distributions - Problem set 1",
    "section": "",
    "text": "An inebriated customer exits a pub and lurches from one lamp post to another on his way home; at each lamp post he pauses for reflection before starting out again. Each time he starts he is equally likely to move towards or away from home. If the posts are separated by a distance \\(a\\), find the mean and the standard deviation of his displacement \\(d\\) from the starting point, after \\(N\\) steps (i.e. encounters with a lamppost).\nIn fact the random walk plays a key role in our understanding of how randomness affects our lives. See “The Drunkards walk” by L. Mlodinow",
    "crumbs": [
      "Problems",
      "Week 20: Problem set 1"
    ]
  },
  {
    "objectID": "Problem_set_1.html#contrasting-one-and-many",
    "href": "Problem_set_1.html#contrasting-one-and-many",
    "title": "Revision of probability and probability distributions - Problem set 1",
    "section": "",
    "text": "Consider a system of \\(N\\) particles, each of which can exist in either of two states, of energies \\(0\\) and \\(\\hat{\\epsilon}\\). Suppose that each particle is equally likely to be in either of its two states. Write down expressions for:\n\nthe mean energy of one particle, and its standard deviation;\nthe mean energy of the system, and its standard deviation.\n\nComment.",
    "crumbs": [
      "Problems",
      "Week 20: Problem set 1"
    ]
  },
  {
    "objectID": "Problem_set_1.html#practically-never",
    "href": "Problem_set_1.html#practically-never",
    "title": "Revision of probability and probability distributions - Problem set 1",
    "section": "",
    "text": "Consider a cubic box of side \\(L = 1\\)m containing \\(N = 6 \\times 10^{23}\\) molecules. Estimate the probability of finding all the molecules in the left-hand half of the box.\nHow long might you expect to wait before observing one of the special arrangements whose probability you have just estimated.\n[Help: Under typical conditions it might take a molecule of the order of \\(10^3\\) s to diffuse from one half of the box to the other.]",
    "crumbs": [
      "Problems",
      "Week 20: Problem set 1"
    ]
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "2. Foundations: equilibrium of an isolated system",
    "section": "",
    "text": "Consider a macroscopic system (a portion of matter), containing an Avogadro-sized number \\(N\\) of ‘microscopic constituents’ or ‘particles’ which we shall take to be isolated from the rest of the world so that no energy (either in the form of heat or work) may enter or leave the system. The system thus has a constant total energy which we denote by \\(E\\).\n\n\n\n\n\n\nFigure 1: Schematic of a system isolated from its surroundings\n\n\n\nWe shall develop the statistical mechanics view of such a system. Specifically we shall establish the microscopic significance of entropy and temperature, and begin to see the simplifications due to the large value of \\(N\\).\nThe arguments will be general (applicable irrespective of what the system actually comprises). They will be illustrated with references to two models of real systems:\n\nthe ideal gas model\na simple model magnet\n\nThe ideal gas model is familiar from PoM. The model magnet that we consider here is defined as follows (we will discuss its physical origins more fully in chapters 6 and 7).\n\n\n\n\n\n\n\nIt comprises an array of \\(N\\) atoms (eg crystal) each with a magnetic dipole moment \\({\\mathbf m}\\) in an applied magnetic field \\({\\mathbf H}\\). The energy of a dipole resides entirely in its interaction with the field which is given by \\(\\epsilon = -{\\mathbf m} \\cdot {\\mathbf H}\\)\nIn view of quantum mechanics (we shall not discuss the details here), \\({\\mathbf m}\\) is either\n\nparallel to \\({\\mathbf H}\\) with energy \\(\\epsilon = -mH\\) (ground state)\nor antiparallel to \\({\\mathbf H}\\) with energy \\(\\epsilon = +mH\\) (excited state)\n\nEach magnetic moment then has the energy level diagram shown on the right with two levels.\n\n\n\n\n\n\n\n\n\n\nFigure 2: Energy-levels for one magnetic atom in a magnetic field in a simple model magnet\n\n\n\n\n\n\n\n\n\nSee Mandl Chapter 2\nThe terms microstate and macrostate constitute two different levels of description of a macroscopic system; we define them and explore their relationship.\n\n\n\n\n\n\nKey point 1\n\n\n\nA microstate is a complete specification of the state of the system according to the microscopic model.\n\n\nThus a microstate is the most detailed description of the state of the system we can provide and is dictated by the microscopic model.\nExamples:\n\nSpecifying the microstate of the model magnet means specifying the orientation of each of the \\(N\\) dipoles as in Figure 3 (a)– or equivalently specifying which of the two rungs of its energy level ladder (cf Figure 2) it occupies.\nSpecifying the microstate of the (ideal) gas means (if we choose to use classical language) specifying the positions and velocities of each and every molecule (cf. Figure 3 (b)).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) A microstate of a model magnet entails knowledge of the orientation (up or down) of each dipole. This is one possible microstate.\n\n\n\n\n\n\n\n\n\n\n\n(b) A microstate of a model gas entails knowledge of the positions (blue circles) and velocities (red arrows) of all the molecules. Here is one example.\n\n\n\n\n\n\n\nFigure 3\n\n\n\nThe microstate will change continually as the particles exchange energy with one another. For instance the molecules of a gas will continually be changing their position and their velocities as they collide with one another; The magnetic dipoles will undergo frequent transitions (hop from rung to rung) under the influence of their mutual interactions.\n\n\n\n\n\n\nKey point 2\n\n\n\nA macrostate is a limited description of the state of the system given by the values of macroscopic variables of interest\n\n\nTo understand this, we must first consider macroscopic variables.\n\nMacroscopic properties are properties reflecting aggregate behaviour of a large number of constituents.\nSome macroscopic properties are fixed by constraints e.g. total energy \\(E\\), and number \\(N\\) are fixed macroscopic properties in the isolated systems we are considering.\nOther macroscopic variables are free e.g. the number of ideal gas molecules in the left hand side of the box in Figure 3 (b) can take on different values, as can the magnetisation in some macroscopic portion of interest of the magnet.\n\nNow we see that a macrostate is a description which depends on what macroscopic properties we are interested in i.e. we have some freedom in choosing what are to be the macrostates. We adopt the notation of denoting the free macroscopic properties of interest (if any) \\(\\{\\alpha\\}\\) and we label macrostates by \\(N, E, \\{\\alpha\\}\\) (note that to lighten the notation \\(N\\) is often dropped but \\(E\\) is usually retained, however we shall retain both for the time being.)\n\n\n\n\n\n\nKey point 3\n\n\n\nTo any one macrostate there correspond in general very many microstates.\n\n\nThe number of microstates corresponding to a macrostate \\((N, E, \\{\\alpha\\})\\) is called the weight of the macrostate and is denoted \\(\\Omega(N, E, \\{\\alpha\\})\\) or more lazily \\(\\Omega(E, \\{\\alpha\\})\\).\nExample of model magnet:\nHere we illustrate the idea of microstates and macrostates. To begin with we do not consider any free macroscopic variables thus the macrostates are simply labelled by \\(N, E\\) (which are fixed). We denote by \\(n_i (i=1,2)\\) the number of dipoles in levels 1 (\\(\\epsilon=−mH\\)) and 2 (\\(\\epsilon=+mH)\\) (cf Figure 2). In fact \\(n_i\\) are determined by the constraints:\n\\[\nn_1+n_2=N,\\hspace{1cm} n_2-n_1=\\frac{E}{mH}\n\\]\nso that\n\\[\nn_1=\\frac{1}{2}(N-\\frac{E}{mH}),\\hspace{1cm}n_2=\\frac{1}{2}(N+\\frac{E}{mH})\n\\]\nAs we stated before a microstate is a complete specification of which state every dipole is in. We now calculate the number of microstates which correspond to the values of \\(n_2\\), \\(n_1 = N − n_2\\)\n\\[\n\\Omega(N,E)={N\\choose n_2}=\\frac{N!}{(N-n_2)!n_2!}=\\frac{N!}{n_1!n_2!}\n\\]\nTo lighten the notation, set \\(n_2=n, n_1=N-n\\), with \\(n\\equiv\\frac{N}{2}(1+\\frac{E}{NmH})\\) (the number of dipoles in the ‘excited’ state) we can then write\n\\[\n\\Omega(N,E)=\\frac{N!}{n!(N-n)!}\n\\tag{1}\\]\n\n\n\n\n\n\nIf you don’t follow this\n\n\n\n\n\nHere is the reasoning, just like that in chapter \\(1.4\\), but now couched in the language of dipoles rather than coins. Take a hatful of \\(N\\) dipoles; pick one which is to be in level \\(i = 1\\); this can be done in \\(N\\) ways; from the remaining pool of \\(N-1\\) pick a second destined for level \\(i = 1\\); this can be done in \\((N - 1)\\) ways. Repeating until you have picked all \\(n_1\\) required for level 1, you have a total of \\(N \\times (N - 1) \\times (N - 2)\\cdots  (N - n_1 + 1) = N!/(N - n_1)!\\) different hat-drawing-possibilities. All the rest of the dipoles must be in level \\(2\\), so there are no further choices to make. But now we must recognise that many of the ‘hat-drawing-possibilities’ actually lead to the same microstate. The microstate is defined by saying what state each dipole is in; so it doesn’t matter if dipole \\(1\\) is the first or the last to be selected for level \\(1\\); to allow for this ‘overcounting’ we must divide by \\(n_1\\)!, the number of hat drawing possibilities which assign \\(n_1\\) specified dipoles to level \\(1\\).\n\n\n\nExplicit example \\(N=3\\)\nHere we can label the macrostates by \\(n = 0, 1, 2, 3\\)\n\n\n\n\n\n\n\n\n\nMacrostate\nMicrostates\nWeight\n\n\n\n\n\\(n=0,E=-3mH\\)\n\\(\\uparrow\\uparrow\\uparrow\\)\n1\n\n\n\\(n=1,E=-mH\\)\n\\(\\downarrow\\uparrow\\uparrow\\)\n3\n\n\n\n\\(\\uparrow\\downarrow\\uparrow\\)\n\n\n\n\n\\(\\uparrow\\uparrow\\downarrow\\)\n\n\n\n\\(n=2,E=+mH\\)\n\\(\\downarrow\\downarrow\\uparrow\\)\n3\n\n\n\n\\(\\downarrow\\uparrow\\downarrow\\)\n\n\n\n\n\\(\\uparrow\\downarrow\\downarrow\\)\n\n\n\n\\(n=3,E=+3mH\\)\n\\(\\downarrow\\downarrow\\downarrow\\)\n1\n\n\n\n\n\n\n\n\n\n(Exercise: repeat for the case \\(N = 4\\).)\nTaking the logarithm of Equation 1 and invoking the Stirling approximation \\(\\ln N!\\approx N(\\ln N-1)\\), good for large \\(N\\), we can write\n\\[\\begin{eqnarray*}\n\\ln \\Omega(N, E) &=& \\ln N! - \\ln n! -\\ln (N - n)! \\\\\n&=& N \\ln N - N - n \\ln n + n - (N - n) \\ln(N - n) + (N - n)\\\\\n&=& N \\left[ \\ln N - \\frac{n}{N} \\ln n - \\left( 1 - \\frac{n}{N} \\right) \\ln(N - n) \\right]\\\\\n&=& N \\left[ -\\frac{n}{N} \\ln \\left( \\frac{n}{N} \\right) - \\left( 1 - \\frac{n}{N} \\right) \\ln \\left( 1 - \\frac{n}{N} \\right) \\right]\\\\\n\\end{eqnarray*}\\]\nwhere we have used a ‘trick’ to write:\n\\[\n\\ln N = -\\frac{n}{N} \\ln \\frac{1}{N} - \\left( 1 - \\frac{n}{N} \\right) \\ln \\frac{1}{N}\n\\]\nThus \\[\n\\frac{1}{N}\\ln\\Omega(N,E)=s(n/N)\n\\] where \\(s(x)=-(1-x)\\ln(1-x)-x\\ln x\\)\n\n\n\n\n\n\nFigure 4: Logarithm of the weight function, \\(\\ln\\Omega(N,E)/N\\), for the simple model magnet, plotted as a function of the fraction \\(n/N\\) of dipoles in excited states; \\(n\\) and \\(E\\) are related by \\(n=\\frac{N}{2}(1+E/NmH)\\).\n\n\n\nThe logarithm of \\(\\Omega (N,E)\\) is displayed in Figure 4. Note the key features:\n\nWhen \\(E\\) has its minimum value, \\(E= -NmH\\) (ie \\(n= 0\\)), then \\(\\ln\\Omega(N,E) = 0\\) and \\(\\Omega(N,E) = 1\\). There is only a single microstate associated with this macrostate.\nAs \\(E\\) increases from its minimum, \\(\\ln \\Omega(N,E)\\) increases steeply.\nFor \\(E=0\\) (ie. \\(n=N/2\\)) we have a maximum where \\(\\ln\\Omega(N,E)= N\\ln 2\\), and \\(\\Omega(N,E)= 2^N\\). In general the logarithm of \\(\\Omega\\) is proportional to the size of the system (we say that it is ‘extensive’)\nSince \\(\\Omega = \\exp(Ns(x))\\) the weight function is exponentially large in \\(N\\) but the logarithm is proportional to \\(N\\).\n\nThe scaling of \\(\\Omega(N,E)\\) with \\(N\\) is demonstrated by Figure 5 for systems of \\(N=2,4,8,16,32,64\\) dipoles. We see that the number of microstates for each energy increases strongly with \\(N\\) and \\(\\Omega(N,E)\\) gets narrower. This trend continues so that for Avogadro’s number of dipoles, the plot Figure 6 of \\(\\Omega(E)\\) is practically infinitely high and infinitesimally narrow. This is the key insight of this exercise: for large \\(N\\) there are overwhelmingly more microstates associated with the ‘equal-shares’ macrostate than there are associated with any significantly different macrostate. Problem 2.2 invites you to explore the properties of the weight function.\n\n\n\n\n\n\nFigure 5: Weight function \\(\\Omega(N,E)\\), for the simple model magnet comprising \\(N=2,4,8,16,32,64\\) dipoles, plotted as a function of the fraction \\(n/N\\) of dipoles in excited states; \\(n\\) and \\(E\\) are related by \\(n=\\frac{N}{2}(1+E/NmH)\\). Note the y-axis scales and the narrowing of the function with increasing \\(N\\).\n\n\n\nFor \\(N=10^{23}\\) dipoles, Figure 6 shows that the weight function is essentially a \\(\\delta\\) function\n\n\n\n\n\n\nFigure 6: For \\(N=10^{23}\\) dipoles the weight function \\(\\Omega(N,E)\\) is essentially a delta function.\n\n\n\nIn order to understand what we mean by free macroscopic variables \\(\\{\\alpha\\}\\) we consider as an example the number of dipoles that are in the excited state in the left hand side of our array of \\(N\\) dipoles. We denote this number by \\(n_L\\). This number is not fixed by our constraints; we only have to satisfy \\(n_R + n_L = n\\) where \\(n_R\\) is the number of excited dipoles in the right hand region.\nWe now label our macrostates by \\(N, E, n_L\\)\nWe can calculate the weight of a macrostate by combinatorics: in the left hand region we can choose the \\(n_L\\) excited dipoles from \\(N/2\\) and similarly in the right hand region \\(n_R = n - n_L\\) are chosen from \\(N/2\\). Thus\n\\[\n\\Omega(N,E,n_L)={N/2\\choose n_L}{N/2\\choose n-n_L}\n\\]\nIn the special case where \\(E=0\\), so \\(n_1=n_2=n=N/2\\) and \\(n_L=N/2-n_R\\), one can show using Stirling’s approximation (see question 2.3) that\n\\[\n\\frac{1}{N}\\ln\\Omega(N,0,n_L)\\simeq s(n_L/(N/2))\n\\]\nSince \\(n_L\\) is free, it is in principle possible for the system to move between different macrostates, moreover the different available macrostates have different weights. In particular we see that macrostates with \\(n_L \\simeq N/4\\) have huge weights compared to say \\(n_L \\simeq N/2\\). Question 2.3 explores this point\n\n\n\nWe have seen that the logarithm of the weight function is the quantity proportional to \\(N\\). We now state an important point relating this quantity to the entropy of a macrostate\n\n\n\n\n\n\nKey point 4\n\n\n\n\\[\nS(N,E,\\{\\alpha\\})=k\\ln\\Omega(N,E,\\{\\alpha\\})\n\\]\n\n\nwhere \\(k=1.381\\times10^{-23} J/K\\) is Boltzmann’s constant (often also written as \\(k_B\\)). The entropy \\(S(N,E,\\{\\alpha\\})\\) of a macrostate is defined by this relation which for the moment we consider as a postulate. The logarithmic connection between entropy and probability was formulated by Boltzmann (though the above form of the equation is due to Planck). We shall refer to it as the Planck equation.",
    "crumbs": [
      "Chapters",
      "2. Foundations: equilibrium of an isolated system"
    ]
  },
  {
    "objectID": "chapter2.html#overview-the-aims-of-this-chapter",
    "href": "chapter2.html#overview-the-aims-of-this-chapter",
    "title": "2. Foundations: equilibrium of an isolated system",
    "section": "",
    "text": "Consider a macroscopic system (a portion of matter), containing an Avogadro-sized number \\(N\\) of ‘microscopic constituents’ or ‘particles’ which we shall take to be isolated from the rest of the world so that no energy (either in the form of heat or work) may enter or leave the system. The system thus has a constant total energy which we denote by \\(E\\).\n\n\n\n\n\n\nFigure 1: Schematic of a system isolated from its surroundings\n\n\n\nWe shall develop the statistical mechanics view of such a system. Specifically we shall establish the microscopic significance of entropy and temperature, and begin to see the simplifications due to the large value of \\(N\\).\nThe arguments will be general (applicable irrespective of what the system actually comprises). They will be illustrated with references to two models of real systems:\n\nthe ideal gas model\na simple model magnet\n\nThe ideal gas model is familiar from PoM. The model magnet that we consider here is defined as follows (we will discuss its physical origins more fully in chapters 6 and 7).\n\n\n\n\n\n\n\nIt comprises an array of \\(N\\) atoms (eg crystal) each with a magnetic dipole moment \\({\\mathbf m}\\) in an applied magnetic field \\({\\mathbf H}\\). The energy of a dipole resides entirely in its interaction with the field which is given by \\(\\epsilon = -{\\mathbf m} \\cdot {\\mathbf H}\\)\nIn view of quantum mechanics (we shall not discuss the details here), \\({\\mathbf m}\\) is either\n\nparallel to \\({\\mathbf H}\\) with energy \\(\\epsilon = -mH\\) (ground state)\nor antiparallel to \\({\\mathbf H}\\) with energy \\(\\epsilon = +mH\\) (excited state)\n\nEach magnetic moment then has the energy level diagram shown on the right with two levels.\n\n\n\n\n\n\n\n\n\n\nFigure 2: Energy-levels for one magnetic atom in a magnetic field in a simple model magnet",
    "crumbs": [
      "Chapters",
      "2. Foundations: equilibrium of an isolated system"
    ]
  },
  {
    "objectID": "chapter2.html#microstates-and-macrostates",
    "href": "chapter2.html#microstates-and-macrostates",
    "title": "2. Foundations: equilibrium of an isolated system",
    "section": "",
    "text": "See Mandl Chapter 2\nThe terms microstate and macrostate constitute two different levels of description of a macroscopic system; we define them and explore their relationship.\n\n\n\n\n\n\nKey point 1\n\n\n\nA microstate is a complete specification of the state of the system according to the microscopic model.\n\n\nThus a microstate is the most detailed description of the state of the system we can provide and is dictated by the microscopic model.\nExamples:\n\nSpecifying the microstate of the model magnet means specifying the orientation of each of the \\(N\\) dipoles as in Figure 3 (a)– or equivalently specifying which of the two rungs of its energy level ladder (cf Figure 2) it occupies.\nSpecifying the microstate of the (ideal) gas means (if we choose to use classical language) specifying the positions and velocities of each and every molecule (cf. Figure 3 (b)).\n\n\n\n\n\n\n\n\n\n\n\n\n(a) A microstate of a model magnet entails knowledge of the orientation (up or down) of each dipole. This is one possible microstate.\n\n\n\n\n\n\n\n\n\n\n\n(b) A microstate of a model gas entails knowledge of the positions (blue circles) and velocities (red arrows) of all the molecules. Here is one example.\n\n\n\n\n\n\n\nFigure 3\n\n\n\nThe microstate will change continually as the particles exchange energy with one another. For instance the molecules of a gas will continually be changing their position and their velocities as they collide with one another; The magnetic dipoles will undergo frequent transitions (hop from rung to rung) under the influence of their mutual interactions.\n\n\n\n\n\n\nKey point 2\n\n\n\nA macrostate is a limited description of the state of the system given by the values of macroscopic variables of interest\n\n\nTo understand this, we must first consider macroscopic variables.\n\nMacroscopic properties are properties reflecting aggregate behaviour of a large number of constituents.\nSome macroscopic properties are fixed by constraints e.g. total energy \\(E\\), and number \\(N\\) are fixed macroscopic properties in the isolated systems we are considering.\nOther macroscopic variables are free e.g. the number of ideal gas molecules in the left hand side of the box in Figure 3 (b) can take on different values, as can the magnetisation in some macroscopic portion of interest of the magnet.\n\nNow we see that a macrostate is a description which depends on what macroscopic properties we are interested in i.e. we have some freedom in choosing what are to be the macrostates. We adopt the notation of denoting the free macroscopic properties of interest (if any) \\(\\{\\alpha\\}\\) and we label macrostates by \\(N, E, \\{\\alpha\\}\\) (note that to lighten the notation \\(N\\) is often dropped but \\(E\\) is usually retained, however we shall retain both for the time being.)\n\n\n\n\n\n\nKey point 3\n\n\n\nTo any one macrostate there correspond in general very many microstates.\n\n\nThe number of microstates corresponding to a macrostate \\((N, E, \\{\\alpha\\})\\) is called the weight of the macrostate and is denoted \\(\\Omega(N, E, \\{\\alpha\\})\\) or more lazily \\(\\Omega(E, \\{\\alpha\\})\\).\nExample of model magnet:\nHere we illustrate the idea of microstates and macrostates. To begin with we do not consider any free macroscopic variables thus the macrostates are simply labelled by \\(N, E\\) (which are fixed). We denote by \\(n_i (i=1,2)\\) the number of dipoles in levels 1 (\\(\\epsilon=−mH\\)) and 2 (\\(\\epsilon=+mH)\\) (cf Figure 2). In fact \\(n_i\\) are determined by the constraints:\n\\[\nn_1+n_2=N,\\hspace{1cm} n_2-n_1=\\frac{E}{mH}\n\\]\nso that\n\\[\nn_1=\\frac{1}{2}(N-\\frac{E}{mH}),\\hspace{1cm}n_2=\\frac{1}{2}(N+\\frac{E}{mH})\n\\]\nAs we stated before a microstate is a complete specification of which state every dipole is in. We now calculate the number of microstates which correspond to the values of \\(n_2\\), \\(n_1 = N − n_2\\)\n\\[\n\\Omega(N,E)={N\\choose n_2}=\\frac{N!}{(N-n_2)!n_2!}=\\frac{N!}{n_1!n_2!}\n\\]\nTo lighten the notation, set \\(n_2=n, n_1=N-n\\), with \\(n\\equiv\\frac{N}{2}(1+\\frac{E}{NmH})\\) (the number of dipoles in the ‘excited’ state) we can then write\n\\[\n\\Omega(N,E)=\\frac{N!}{n!(N-n)!}\n\\tag{1}\\]\n\n\n\n\n\n\nIf you don’t follow this\n\n\n\n\n\nHere is the reasoning, just like that in chapter \\(1.4\\), but now couched in the language of dipoles rather than coins. Take a hatful of \\(N\\) dipoles; pick one which is to be in level \\(i = 1\\); this can be done in \\(N\\) ways; from the remaining pool of \\(N-1\\) pick a second destined for level \\(i = 1\\); this can be done in \\((N - 1)\\) ways. Repeating until you have picked all \\(n_1\\) required for level 1, you have a total of \\(N \\times (N - 1) \\times (N - 2)\\cdots  (N - n_1 + 1) = N!/(N - n_1)!\\) different hat-drawing-possibilities. All the rest of the dipoles must be in level \\(2\\), so there are no further choices to make. But now we must recognise that many of the ‘hat-drawing-possibilities’ actually lead to the same microstate. The microstate is defined by saying what state each dipole is in; so it doesn’t matter if dipole \\(1\\) is the first or the last to be selected for level \\(1\\); to allow for this ‘overcounting’ we must divide by \\(n_1\\)!, the number of hat drawing possibilities which assign \\(n_1\\) specified dipoles to level \\(1\\).\n\n\n\nExplicit example \\(N=3\\)\nHere we can label the macrostates by \\(n = 0, 1, 2, 3\\)\n\n\n\n\n\n\n\n\n\nMacrostate\nMicrostates\nWeight\n\n\n\n\n\\(n=0,E=-3mH\\)\n\\(\\uparrow\\uparrow\\uparrow\\)\n1\n\n\n\\(n=1,E=-mH\\)\n\\(\\downarrow\\uparrow\\uparrow\\)\n3\n\n\n\n\\(\\uparrow\\downarrow\\uparrow\\)\n\n\n\n\n\\(\\uparrow\\uparrow\\downarrow\\)\n\n\n\n\\(n=2,E=+mH\\)\n\\(\\downarrow\\downarrow\\uparrow\\)\n3\n\n\n\n\\(\\downarrow\\uparrow\\downarrow\\)\n\n\n\n\n\\(\\uparrow\\downarrow\\downarrow\\)\n\n\n\n\\(n=3,E=+3mH\\)\n\\(\\downarrow\\downarrow\\downarrow\\)\n1\n\n\n\n\n\n\n\n\n\n(Exercise: repeat for the case \\(N = 4\\).)\nTaking the logarithm of Equation 1 and invoking the Stirling approximation \\(\\ln N!\\approx N(\\ln N-1)\\), good for large \\(N\\), we can write\n\\[\\begin{eqnarray*}\n\\ln \\Omega(N, E) &=& \\ln N! - \\ln n! -\\ln (N - n)! \\\\\n&=& N \\ln N - N - n \\ln n + n - (N - n) \\ln(N - n) + (N - n)\\\\\n&=& N \\left[ \\ln N - \\frac{n}{N} \\ln n - \\left( 1 - \\frac{n}{N} \\right) \\ln(N - n) \\right]\\\\\n&=& N \\left[ -\\frac{n}{N} \\ln \\left( \\frac{n}{N} \\right) - \\left( 1 - \\frac{n}{N} \\right) \\ln \\left( 1 - \\frac{n}{N} \\right) \\right]\\\\\n\\end{eqnarray*}\\]\nwhere we have used a ‘trick’ to write:\n\\[\n\\ln N = -\\frac{n}{N} \\ln \\frac{1}{N} - \\left( 1 - \\frac{n}{N} \\right) \\ln \\frac{1}{N}\n\\]\nThus \\[\n\\frac{1}{N}\\ln\\Omega(N,E)=s(n/N)\n\\] where \\(s(x)=-(1-x)\\ln(1-x)-x\\ln x\\)\n\n\n\n\n\n\nFigure 4: Logarithm of the weight function, \\(\\ln\\Omega(N,E)/N\\), for the simple model magnet, plotted as a function of the fraction \\(n/N\\) of dipoles in excited states; \\(n\\) and \\(E\\) are related by \\(n=\\frac{N}{2}(1+E/NmH)\\).\n\n\n\nThe logarithm of \\(\\Omega (N,E)\\) is displayed in Figure 4. Note the key features:\n\nWhen \\(E\\) has its minimum value, \\(E= -NmH\\) (ie \\(n= 0\\)), then \\(\\ln\\Omega(N,E) = 0\\) and \\(\\Omega(N,E) = 1\\). There is only a single microstate associated with this macrostate.\nAs \\(E\\) increases from its minimum, \\(\\ln \\Omega(N,E)\\) increases steeply.\nFor \\(E=0\\) (ie. \\(n=N/2\\)) we have a maximum where \\(\\ln\\Omega(N,E)= N\\ln 2\\), and \\(\\Omega(N,E)= 2^N\\). In general the logarithm of \\(\\Omega\\) is proportional to the size of the system (we say that it is ‘extensive’)\nSince \\(\\Omega = \\exp(Ns(x))\\) the weight function is exponentially large in \\(N\\) but the logarithm is proportional to \\(N\\).\n\nThe scaling of \\(\\Omega(N,E)\\) with \\(N\\) is demonstrated by Figure 5 for systems of \\(N=2,4,8,16,32,64\\) dipoles. We see that the number of microstates for each energy increases strongly with \\(N\\) and \\(\\Omega(N,E)\\) gets narrower. This trend continues so that for Avogadro’s number of dipoles, the plot Figure 6 of \\(\\Omega(E)\\) is practically infinitely high and infinitesimally narrow. This is the key insight of this exercise: for large \\(N\\) there are overwhelmingly more microstates associated with the ‘equal-shares’ macrostate than there are associated with any significantly different macrostate. Problem 2.2 invites you to explore the properties of the weight function.\n\n\n\n\n\n\nFigure 5: Weight function \\(\\Omega(N,E)\\), for the simple model magnet comprising \\(N=2,4,8,16,32,64\\) dipoles, plotted as a function of the fraction \\(n/N\\) of dipoles in excited states; \\(n\\) and \\(E\\) are related by \\(n=\\frac{N}{2}(1+E/NmH)\\). Note the y-axis scales and the narrowing of the function with increasing \\(N\\).\n\n\n\nFor \\(N=10^{23}\\) dipoles, Figure 6 shows that the weight function is essentially a \\(\\delta\\) function\n\n\n\n\n\n\nFigure 6: For \\(N=10^{23}\\) dipoles the weight function \\(\\Omega(N,E)\\) is essentially a delta function.\n\n\n\nIn order to understand what we mean by free macroscopic variables \\(\\{\\alpha\\}\\) we consider as an example the number of dipoles that are in the excited state in the left hand side of our array of \\(N\\) dipoles. We denote this number by \\(n_L\\). This number is not fixed by our constraints; we only have to satisfy \\(n_R + n_L = n\\) where \\(n_R\\) is the number of excited dipoles in the right hand region.\nWe now label our macrostates by \\(N, E, n_L\\)\nWe can calculate the weight of a macrostate by combinatorics: in the left hand region we can choose the \\(n_L\\) excited dipoles from \\(N/2\\) and similarly in the right hand region \\(n_R = n - n_L\\) are chosen from \\(N/2\\). Thus\n\\[\n\\Omega(N,E,n_L)={N/2\\choose n_L}{N/2\\choose n-n_L}\n\\]\nIn the special case where \\(E=0\\), so \\(n_1=n_2=n=N/2\\) and \\(n_L=N/2-n_R\\), one can show using Stirling’s approximation (see question 2.3) that\n\\[\n\\frac{1}{N}\\ln\\Omega(N,0,n_L)\\simeq s(n_L/(N/2))\n\\]\nSince \\(n_L\\) is free, it is in principle possible for the system to move between different macrostates, moreover the different available macrostates have different weights. In particular we see that macrostates with \\(n_L \\simeq N/4\\) have huge weights compared to say \\(n_L \\simeq N/2\\). Question 2.3 explores this point",
    "crumbs": [
      "Chapters",
      "2. Foundations: equilibrium of an isolated system"
    ]
  },
  {
    "objectID": "chapter2.html#significance-of-the-weight-function-entropy",
    "href": "chapter2.html#significance-of-the-weight-function-entropy",
    "title": "2. Foundations: equilibrium of an isolated system",
    "section": "",
    "text": "We have seen that the logarithm of the weight function is the quantity proportional to \\(N\\). We now state an important point relating this quantity to the entropy of a macrostate\n\n\n\n\n\n\nKey point 4\n\n\n\n\\[\nS(N,E,\\{\\alpha\\})=k\\ln\\Omega(N,E,\\{\\alpha\\})\n\\]\n\n\nwhere \\(k=1.381\\times10^{-23} J/K\\) is Boltzmann’s constant (often also written as \\(k_B\\)). The entropy \\(S(N,E,\\{\\alpha\\})\\) of a macrostate is defined by this relation which for the moment we consider as a postulate. The logarithmic connection between entropy and probability was formulated by Boltzmann (though the above form of the equation is due to Planck). We shall refer to it as the Planck equation.",
    "crumbs": [
      "Chapters",
      "2. Foundations: equilibrium of an isolated system"
    ]
  },
  {
    "objectID": "Problem_set_2.html",
    "href": "Problem_set_2.html",
    "title": "Microstates, Macrostates and the Planck relations - problems set 2",
    "section": "",
    "text": "Consider the model magnet of \\(N\\) dipoles (see chapter 2), each of which may exist in either of two states (orientations). For the case \\(N = 4\\), identify explicitly, and count, the different microstates associated with each possible energy macrostate (identified by each possible value of the system energy \\(E\\)).\n\n\n\nReview the analysis of the model magnet (see chapter 2), according to which the weight function \\(\\Omega(N, E)\\), giving the number of microstates of energy \\(E\\), satisfies, for large \\(N\\):\n\\[\n\\frac{1}{N} \\ln \\Omega(N, E) = s(x)\n\\]\nwhere \\(s(x) = -x \\ln x - (1 - x) \\ln (1 - x)\\),\nand \\(x \\equiv n/N\\), with \\(n\\) (which is fixed by \\(E\\)) the number of dipoles in excited states. Sketch \\(\\ln \\Omega\\) as a function of \\(n/N\\), and think about why it has this form. (See chapter 2 for hints on the sketch).\n\n\n\nFor the model magnet with \\(E = 0\\) (which has \\(n = N/2\\) dipoles in excited states), the weight function of a macrostate in which \\(n_L\\) dipoles in the left-hand half of the magnet are in their excited state satisfies:\n\\[\n\\frac{1}{N} \\ln \\Omega(N, E = 0, n_L) = s(y)\n\\]\nwhere \\(s\\) is the function defined in the previous question, and \\(y = n_L/(N/2)\\). Carry out a Taylor expansion, to show that around its maximum, the weight function can be written in the form:\n\\[\n\\Omega(E = 0, n_L) \\simeq \\Omega_{\\text{max}} e^{-2N(y - y^*)^2}\n\\]\nwhere \\(y^* = \\frac{1}{2}\\). Think about the implications for the values of \\(n_L\\) that are likely to be observed.\n\n\n\nOne mole of ideal gas is maintained by a partition in one half of a container of total volume \\(2V\\), which is thermodynamically isolated. The partition is removed, allowing the gas to expand freely to fill the entire volume. Calculate the entropy change by appeal to: - (a) thermodynamic arguments - (b) statistical mechanics arguments\n[Help: In (a) Use the first law of thermodynamics in differential form: dE=dQ+dW=TdS-PdV. In (b) you may assume that the number of states available to a single molecule of gas is proportional to the volume \\(V\\) it occupies, from which you should deduce that the number of microstates of \\(N\\) molecules in volume \\(V\\) is proportional to \\(V^N\\).]\n\n\n\nAn ice cube is melted slowly (i.e., reversibly) at \\(T = 0^\\circ C\\). How many times more microstates are associated with the liquid phase (water) than the solid phase (ice)?\nHelp: Calculate the energy required to melt the cube given that its size is \\(10 \\, \\text{cm}^3\\), its density is \\(1 \\, \\text{g/cm}^3\\), and the latent heat of melting of ice is \\(3.34 \\times 10^5 \\, \\text{J/kg}\\). Now use the relation from thermodynamics for entropy change along a reversible path at constant \\(T\\), \\(\\Delta S = \\frac{Q}{T}\\), to determine the entropy change. Then use the Planck relation.\n\n\n\nA metallic alloy comprises 25 atomic percent of Au and 75 atomic percent of Cu. At low temperatures the atoms arrange themselves in an ordered array comprising cubic cells in which the Au atoms occupy cube-corner sites, and the Cu atoms occupy the centres of the cube faces. At high enough temperatures the atoms are randomly arranged over corner and face-centre sites. For a sample containing \\(N\\) atoms in total, show that the number of microstates associated with the ordered (o) low-temperature arrangement (“macrostate”), \\(\\Omega_o\\), and the number associated with the disordered (d) high-temperature arrangement, \\(\\Omega_d\\), are\neither\n\\[\\Omega_d = N! \\quad \\text{and} \\quad \\Omega_o = (N/4)!(3N/4)!\\]\nor\n\\[\\Omega_d = \\frac{N!}{[(N/4)!(3N/4)!]} \\quad \\text{and} \\quad \\Omega_o = 1\\]\nwhere the “either … or” reflects two possible ways of thinking about atoms, which you should identify. Show that (irrespective of the “way of thinking”) the difference between the (molar) entropy of the two arrangements is\n\\[\\Delta S \\equiv S_d - S_o = R \\left( \\ln 4 - \\frac{3}{4} \\ln 3 \\right).\\]\nHelp: (1) Can you tell one Au atom from another? (2) Use Stirling’s formula.",
    "crumbs": [
      "Problems",
      "Week 21: Problem set 2"
    ]
  },
  {
    "objectID": "Problem_set_2.html#microstates-and-macrostates",
    "href": "Problem_set_2.html#microstates-and-macrostates",
    "title": "Microstates, Macrostates and the Planck relations - problems set 2",
    "section": "",
    "text": "Consider the model magnet of \\(N\\) dipoles (see chapter 2), each of which may exist in either of two states (orientations). For the case \\(N = 4\\), identify explicitly, and count, the different microstates associated with each possible energy macrostate (identified by each possible value of the system energy \\(E\\)).",
    "crumbs": [
      "Problems",
      "Week 21: Problem set 2"
    ]
  },
  {
    "objectID": "Problem_set_2.html#exploring-the-weight-function-for-the-magnet",
    "href": "Problem_set_2.html#exploring-the-weight-function-for-the-magnet",
    "title": "Microstates, Macrostates and the Planck relations - problems set 2",
    "section": "",
    "text": "Review the analysis of the model magnet (see chapter 2), according to which the weight function \\(\\Omega(N, E)\\), giving the number of microstates of energy \\(E\\), satisfies, for large \\(N\\):\n\\[\n\\frac{1}{N} \\ln \\Omega(N, E) = s(x)\n\\]\nwhere \\(s(x) = -x \\ln x - (1 - x) \\ln (1 - x)\\),\nand \\(x \\equiv n/N\\), with \\(n\\) (which is fixed by \\(E\\)) the number of dipoles in excited states. Sketch \\(\\ln \\Omega\\) as a function of \\(n/N\\), and think about why it has this form. (See chapter 2 for hints on the sketch).",
    "crumbs": [
      "Problems",
      "Week 21: Problem set 2"
    ]
  },
  {
    "objectID": "Problem_set_2.html#sharpness-of-maximum-of-weight-function",
    "href": "Problem_set_2.html#sharpness-of-maximum-of-weight-function",
    "title": "Microstates, Macrostates and the Planck relations - problems set 2",
    "section": "",
    "text": "For the model magnet with \\(E = 0\\) (which has \\(n = N/2\\) dipoles in excited states), the weight function of a macrostate in which \\(n_L\\) dipoles in the left-hand half of the magnet are in their excited state satisfies:\n\\[\n\\frac{1}{N} \\ln \\Omega(N, E = 0, n_L) = s(y)\n\\]\nwhere \\(s\\) is the function defined in the previous question, and \\(y = n_L/(N/2)\\). Carry out a Taylor expansion, to show that around its maximum, the weight function can be written in the form:\n\\[\n\\Omega(E = 0, n_L) \\simeq \\Omega_{\\text{max}} e^{-2N(y - y^*)^2}\n\\]\nwhere \\(y^* = \\frac{1}{2}\\). Think about the implications for the values of \\(n_L\\) that are likely to be observed.",
    "crumbs": [
      "Problems",
      "Week 21: Problem set 2"
    ]
  },
  {
    "objectID": "Problem_set_2.html#two-ways-of-calculating-entropy-changes",
    "href": "Problem_set_2.html#two-ways-of-calculating-entropy-changes",
    "title": "Microstates, Macrostates and the Planck relations - problems set 2",
    "section": "",
    "text": "One mole of ideal gas is maintained by a partition in one half of a container of total volume \\(2V\\), which is thermodynamically isolated. The partition is removed, allowing the gas to expand freely to fill the entire volume. Calculate the entropy change by appeal to: - (a) thermodynamic arguments - (b) statistical mechanics arguments\n[Help: In (a) Use the first law of thermodynamics in differential form: dE=dQ+dW=TdS-PdV. In (b) you may assume that the number of states available to a single molecule of gas is proportional to the volume \\(V\\) it occupies, from which you should deduce that the number of microstates of \\(N\\) molecules in volume \\(V\\) is proportional to \\(V^N\\).]",
    "crumbs": [
      "Problems",
      "Week 21: Problem set 2"
    ]
  },
  {
    "objectID": "Problem_set_2.html#entropy-change-due-to-a-phase-transition-melting-ice",
    "href": "Problem_set_2.html#entropy-change-due-to-a-phase-transition-melting-ice",
    "title": "Microstates, Macrostates and the Planck relations - problems set 2",
    "section": "",
    "text": "An ice cube is melted slowly (i.e., reversibly) at \\(T = 0^\\circ C\\). How many times more microstates are associated with the liquid phase (water) than the solid phase (ice)?\nHelp: Calculate the energy required to melt the cube given that its size is \\(10 \\, \\text{cm}^3\\), its density is \\(1 \\, \\text{g/cm}^3\\), and the latent heat of melting of ice is \\(3.34 \\times 10^5 \\, \\text{J/kg}\\). Now use the relation from thermodynamics for entropy change along a reversible path at constant \\(T\\), \\(\\Delta S = \\frac{Q}{T}\\), to determine the entropy change. Then use the Planck relation.",
    "crumbs": [
      "Problems",
      "Week 21: Problem set 2"
    ]
  },
  {
    "objectID": "Problem_set_2.html#entropy-change-due-to-a-phase-transition-orderdisorder-transition",
    "href": "Problem_set_2.html#entropy-change-due-to-a-phase-transition-orderdisorder-transition",
    "title": "Microstates, Macrostates and the Planck relations - problems set 2",
    "section": "",
    "text": "A metallic alloy comprises 25 atomic percent of Au and 75 atomic percent of Cu. At low temperatures the atoms arrange themselves in an ordered array comprising cubic cells in which the Au atoms occupy cube-corner sites, and the Cu atoms occupy the centres of the cube faces. At high enough temperatures the atoms are randomly arranged over corner and face-centre sites. For a sample containing \\(N\\) atoms in total, show that the number of microstates associated with the ordered (o) low-temperature arrangement (“macrostate”), \\(\\Omega_o\\), and the number associated with the disordered (d) high-temperature arrangement, \\(\\Omega_d\\), are\neither\n\\[\\Omega_d = N! \\quad \\text{and} \\quad \\Omega_o = (N/4)!(3N/4)!\\]\nor\n\\[\\Omega_d = \\frac{N!}{[(N/4)!(3N/4)!]} \\quad \\text{and} \\quad \\Omega_o = 1\\]\nwhere the “either … or” reflects two possible ways of thinking about atoms, which you should identify. Show that (irrespective of the “way of thinking”) the difference between the (molar) entropy of the two arrangements is\n\\[\\Delta S \\equiv S_d - S_o = R \\left( \\ln 4 - \\frac{3}{4} \\ln 3 \\right).\\]\nHelp: (1) Can you tell one Au atom from another? (2) Use Stirling’s formula.",
    "crumbs": [
      "Problems",
      "Week 21: Problem set 2"
    ]
  },
  {
    "objectID": "Problem_set_3.html",
    "href": "Problem_set_3.html",
    "title": "Temperature and the Boltzmann Distribution - Problem set 3",
    "section": "",
    "text": "Write down the Boltzmann distribution for a system in equilibrium with its environment at temperature \\(T\\). Explain physically why it is that a low energy microstate is intrinsically more probable than a high energy microstate.\n\n\n\nIn a system of weakly interacting particles, in equilibrium at temperature \\(T\\), each particle has access to two states with energy difference \\(\\epsilon_2 - \\epsilon_1 = 0.1 \\, \\text{eV}\\). At what temperature will \\(1/3\\) of the particles be found to have energy \\(\\epsilon_2\\)?\n\n\n\nNow rework the argument supposing that the state of energy \\(\\epsilon_2\\) is two-fold degenerate (i.e., that each particle has access to two states of energy \\(\\epsilon_2\\)).\n\n\n\nShow that, if the energy of a system can be written as a sum of contributions associated with different aspects (magnetic, vibrational \\(\\dots\\)) of its behavior, then the partition function can be written as a product of independent partition functions associated with each aspect. Why is this useful?\n\n\n\nShow that minimising the free energy with respect to energy gives a definition of temperature consistent with chapter 3.5\n\n\n\nReview the form the Boltzmann distribution takes for a system of weakly interacting particles. Consider a system of weakly interacting particles, each of which can access only a finite number of energy levels. Sketch plausible forms for the \\(T\\)-dependence of the entropy \\(S\\), the (mean) energy \\(\\bar{E}\\), and the heat capacity \\(C \\equiv \\frac{\\partial \\bar{E}}{\\partial T}\\)."
  },
  {
    "objectID": "Problem_set_3.html#knowing-and-understanding-the-boltzmann-distribution",
    "href": "Problem_set_3.html#knowing-and-understanding-the-boltzmann-distribution",
    "title": "Temperature and the Boltzmann Distribution - Problem set 3",
    "section": "",
    "text": "Write down the Boltzmann distribution for a system in equilibrium with its environment at temperature \\(T\\). Explain physically why it is that a low energy microstate is intrinsically more probable than a high energy microstate."
  },
  {
    "objectID": "Problem_set_3.html#some-numbers",
    "href": "Problem_set_3.html#some-numbers",
    "title": "Temperature and the Boltzmann Distribution - Problem set 3",
    "section": "",
    "text": "In a system of weakly interacting particles, in equilibrium at temperature \\(T\\), each particle has access to two states with energy difference \\(\\epsilon_2 - \\epsilon_1 = 0.1 \\, \\text{eV}\\). At what temperature will \\(1/3\\) of the particles be found to have energy \\(\\epsilon_2\\)?"
  },
  {
    "objectID": "Problem_set_3.html#and-some-thinking",
    "href": "Problem_set_3.html#and-some-thinking",
    "title": "Temperature and the Boltzmann Distribution - Problem set 3",
    "section": "",
    "text": "Now rework the argument supposing that the state of energy \\(\\epsilon_2\\) is two-fold degenerate (i.e., that each particle has access to two states of energy \\(\\epsilon_2\\))."
  },
  {
    "objectID": "Problem_set_3.html#how-to-divide-and-conquer",
    "href": "Problem_set_3.html#how-to-divide-and-conquer",
    "title": "Temperature and the Boltzmann Distribution - Problem set 3",
    "section": "",
    "text": "Show that, if the energy of a system can be written as a sum of contributions associated with different aspects (magnetic, vibrational \\(\\dots\\)) of its behavior, then the partition function can be written as a product of independent partition functions associated with each aspect. Why is this useful?"
  },
  {
    "objectID": "Problem_set_3.html#minimising-the-free-energy",
    "href": "Problem_set_3.html#minimising-the-free-energy",
    "title": "Temperature and the Boltzmann Distribution - Problem set 3",
    "section": "",
    "text": "Show that minimising the free energy with respect to energy gives a definition of temperature consistent with chapter 3.5"
  },
  {
    "objectID": "Problem_set_3.html#a-qualitative-thinking-exercise",
    "href": "Problem_set_3.html#a-qualitative-thinking-exercise",
    "title": "Temperature and the Boltzmann Distribution - Problem set 3",
    "section": "",
    "text": "Review the form the Boltzmann distribution takes for a system of weakly interacting particles. Consider a system of weakly interacting particles, each of which can access only a finite number of energy levels. Sketch plausible forms for the \\(T\\)-dependence of the entropy \\(S\\), the (mean) energy \\(\\bar{E}\\), and the heat capacity \\(C \\equiv \\frac{\\partial \\bar{E}}{\\partial T}\\)."
  },
  {
    "objectID": "chapter5.html",
    "href": "chapter5.html",
    "title": "5. Free energy minimisation",
    "section": "",
    "text": "See Mandl 2.6\n\n\nWe have seen that the energy of a system connected to a heat bath exhibits thermal fluctuations controlled by the Boltzmann distribution. Here we calcualte the mean and variance of these fluctuations. The mean energy is defined as usual as:\n\\[\n\\overline{E} = \\sum_i E_i P_i\n\\]\nwhere \\(P_i\\) is the probability of the system being in microstate \\(i\\). Inserting the Boltzmann distribution yields:\n\\[\n\\overline{E} = \\frac{1}{Z} \\sum_i E_i \\exp(-\\beta E_i) = -\\frac{1}{Z} \\sum_i \\frac{\\partial}{\\partial \\beta} \\exp(-\\beta E_i) = -\\frac{1}{Z} \\frac{\\partial Z}{\\partial \\beta}\n\\]\nNote the trick of using the derivative to bring down \\(E_i\\) from the argument of the exponential. A further manipulation to note is:\n\\[\n\\frac{\\partial \\ln Z}{\\partial \\beta} = \\frac{1}{Z} \\frac{\\partial Z}{\\partial \\beta}\n\\]\nand we find:\n\\[\n\\overline{E} = -\\frac{\\partial \\ln Z}{\\partial \\beta} = -\\frac{dT}{d\\beta} \\frac{\\partial \\ln Z}{\\partial T} = kT^2 \\frac{\\partial \\ln Z}{\\partial T}\n\\]\nNow we consider fluctuations of the energy. The variance is:\n\\[\n\\overline{(\\Delta E)^2} = \\overline{(E - \\overline{E})^2} = \\overline{E^2} - \\overline{E}^2\n\\]\nIt turns out that this variance is related to the heat capacity \\(C\\) defined by \\[\nC \\equiv \\frac{\\partial \\overline{E}}{\\partial T} = \\frac{d\\beta}{dT} \\frac{\\partial \\overline{E}}{\\partial \\beta} = -\\frac{1}{kT^2} \\frac{\\partial \\overline{E}}{\\partial \\beta}\n\\]\nThe relationship (derived below) is \\[\nC = -\\frac{1}{kT^2} \\left[\\overline{E^2}- \\overline{E}^2\\right] = \\frac{(\\overline{\\Delta E)^2}}{kT^2}\n\\]\n\n\n\n\n\n\nOpen to see derivation\n\n\n\n\n\nIn order to demonstrate the relation of the energy fluctuations to the heat capacity, we write out the final expression for \\(C\\) above using the Boltzmann distribution:\n\\[\nC = -\\frac{1}{kT^2} \\frac{\\partial}{\\partial \\beta} \\left(\\frac{\\sum_i E_i \\exp(-\\beta E_i)}{Z}\\right) = -\\frac{1}{kT^2} \\left[-\\frac{\\sum_i E_i \\exp(-\\beta E_i)}{Z^2} \\frac{\\partial Z}{\\partial \\beta} - \\frac{\\sum_i E_i^2 \\exp(-\\beta E_i)}{Z}\\right]\n\\]\nwhich recalling:\n\\[\n\\sum_i E_i \\exp(-\\beta E_i) = \\overline{E} = -\\frac{1}{Z} \\frac{\\partial Z}{\\partial \\beta}\n\\]\n\\[\n\\sum_i E_i^2 \\exp(-\\beta E_i) = \\overline{E^2}\n\\]\nyields:\n\\[\nC = \\frac{1}{kT^2} \\left[\\overline{E^2}- \\overline{E}^2\\right] = \\frac{(\\overline{\\Delta E)^2}}{kT^2}\n\\]\n\n\n\nIt is interesting to pause and think what this means. The heat capacity can be thought of as the response of a macroscopic variable (the energy) to changing an external variable (the temperature). Thus, the response is related to the fluctuations of the macroscopic variable.\nSince \\(E\\) is proportional to \\(N\\), so will be \\(C = \\partial \\overline{E}/\\partial T\\). Therefore:\n\\[\n\\overline{(\\Delta E)^2} \\propto N\n\\]\nand the fractional deviation:\n\\[\n\\frac{\\left(\\overline{\\Delta E)^2}\\right)^{1/2}}{\\overline{E}} \\propto \\frac{1}{\\sqrt{N}}\n\\]\nFrom this, we see that for a system in equilibrium with a heat bath, although the energy is a free macroscopic variable, it takes on a sharply defined value \\(\\overline{E}\\). A consequence is that a large system in equilibrium with a heat bath should have essentially the same behavior as a large isolated system with fixed energy \\(\\overline{E}\\).\n\n\n\nSince, in the Boltzmann distribution, the energy is a free macroscopic variable, we can label the macrostates of the system by their energy, and the weight of the macrostate of energy \\(E\\) is \\(\\Omega(E)\\), the number of microstates with energy \\(E\\).\nAs in chapter 3, we can use the Planck relation to determine the entropy of a macrostate:\n\\[\nS(E) = k \\ln \\Omega(E)\n\\]\nWe now return to the question of the sharpness of \\(E\\). Let us consider again:\n\\[\n\\overline{E} = \\frac{1}{Z} \\sum_i E_i \\exp(-\\beta E_i)\n\\]\nGenerally, for a system of large \\(N\\), we expect many of the microstates to have the same energy. Therefore, rather than sum over all microstates, it is more convenient to sum over the possible values of the energy:\n\\[\n\\overline{E} = \\frac{1}{Z} \\sum_E \\Omega(E) E \\exp(-\\beta E)\n\\]\nTo understand this, realize that we are simply grouping together microstates with the same energy \\(E\\) of which there are \\(\\Omega(E)\\).\nThe same idea of grouping microstates with the same energy gives the probability of the system having energy \\(E\\):\n\\[\nP(E) = \\frac{1}{Z} \\Omega(E) \\exp(-\\beta E)\n\\]\nSince we expect \\(E \\sim N\\), \\(\\exp(-\\beta E)\\) is a sharply decreasing function. Also, \\(\\Omega(E)\\) is exponentially large in \\(N\\) and \\(\\Omega\\) is a sharply increasing function of \\(E\\). The two conspire to give as a product a sharply peaked probability distribution around \\(\\overline{E}\\). This is plotted in Figure 1\n\n\n\n\n\n\nFigure 1: Sharply peaked probability distribution of the energy\n\n\n\nTo go further, we rewrite \\(P(E)\\) and use the Planck relation:\n\\[\nP(E) = \\frac{1}{Z} \\exp\\left(-\\beta\\left(E - \\frac{\\ln \\Omega}{\\beta}\\right)\\right) = \\frac{1}{Z} \\exp(-\\beta(E - TS)) = \\frac{1}{Z} \\exp(-\\beta F)\n\\]\nwhere:\n\\[\nF(E) \\equiv E - TS(E)\n\\]\n\\(F\\) is the Helmholtz free energy, which is often referred to in statistical mechanics simply as the free energy.\nNow the peak of \\(P(E)\\) occurs when the argument of the exponential is maximized. Thus, we see that the equilibrium value of the energy \\(E\\) is determined by minimizing the free energy. This is a profound and general concept:\n\n\n\n\n\n\nKey Point 11:\n\n\n\nThe equilibrium values of the macroscopic properties \\(E, \\{\\alpha\\}\\) of a system in equilibrium are such as to minimize the free energy \\(F(E, \\{\\alpha\\}) \\equiv E - TS(E, \\{\\alpha\\})\\).\n\n\nWe write equilibrium values as \\(S(T)\\), \\(F(T)\\), etc.\n\n\n\nFirst, consider low \\(T\\): here \\(F\\) is dominated by \\(E\\), therefore minimizing \\(F\\) corresponds to minimizing \\(E\\) (exactly so for \\(T = 0\\)).\nNow consider high \\(T\\): here \\(F\\) is dominated by \\(S\\), therefore minimizing \\(F\\) corresponds to maximizing \\(S\\) (exactly so for \\(T = \\infty\\)).\nThus, one has a competition between energy and entropy, the arbiter of which is temperature.\nHowever, it should always be borne in mind that equilibrium is such as to maximize the entropy of the universe under the constraint that energy is conserved. For example, in an isolated system, the energy is constant, therefore we just maximize the entropy. For a system in equilibrium with its environment, the energy is free to vary. Thus, the minimization of the free energy for a system in equilibrium tells us the best way to distribute energy between the system and the universe such that the entropy of the universe is maximized.\nWe now begin to understand the different phases in question 2.5:\n\nAt low temperatures, ice forms. The crystal structure is energetically favorable and releases energy (latent heat of fusion) to the environment, which maximizes the entropy of the universe, i.e., from the viewpoint of the system, energy considerations win.\nAt higher temperatures, storing more energy in the system gives the liquid phase (water), which has a high entropy and allows the entropy of the universe to be maximized. From the point of view of the system, entropy wins.\n\n\n\n\nLet us first write \\(Z\\) as\n\\[\nZ = \\sum_E \\Omega(E) \\exp(-\\beta E)\n\\]\nIn the same way as \\(P(E)\\) is an extremely sharp function, the summand is very sharply peaked at \\(\\overline{E}\\) and of width \\(N^{1/2}\\) about the peak. Therefore, we can write\n\\[\nZ \\approx \\Omega(\\overline{E}) \\exp(-\\beta \\overline{E}) \\times O(N^{1/2})\n\\]\n\\[\n= \\exp(-\\beta F(T)) \\times O(N^{1/2})\n\\]\n(since \\(F(T) = F(\\overline{E})\\) for a large system in equilibrium). Thus\n\\[\n\\ln Z = -\\beta F(T) + O(\\ln N^{1/2})\n\\]\nWe see the last term can be ignored since we are in the ‘large \\(N\\)’ regime where \\(N \\gg \\ln N\\). Thus, the equilibrium value of the free energy\n\\[\nF(T) = \\overline{E} - TS(T) = -kT \\ln Z\n\\]\nThis is a very important formula.\nAlso, the expression can be written in terms of the entropy\n\\[\nS(T) = k \\ln Z + \\frac{\\overline{E}}{T}\n\\]\nTherefore, for a system in equilibrium at temperature \\(T\\), all the thermodynamic properties can be obtained from \\(Z\\) from the following formulae:\n\n\n\n\n\n\nKey Point 12:\n\n\n\n\\[\nF = -kT \\ln Z, \\quad \\overline{E} = kT^2 \\frac{\\partial}{\\partial T} \\ln Z, \\quad S = k \\ln Z + \\frac{\\overline{E}}{T}\n\\]",
    "crumbs": [
      "Chapters",
      "5. Free energy minimisation"
    ]
  },
  {
    "objectID": "chapter5.html#energy-and-its-fluctuations",
    "href": "chapter5.html#energy-and-its-fluctuations",
    "title": "5. Free energy minimisation",
    "section": "",
    "text": "We have seen that the energy of a system connected to a heat bath exhibits thermal fluctuations controlled by the Boltzmann distribution. Here we calcualte the mean and variance of these fluctuations. The mean energy is defined as usual as:\n\\[\n\\overline{E} = \\sum_i E_i P_i\n\\]\nwhere \\(P_i\\) is the probability of the system being in microstate \\(i\\). Inserting the Boltzmann distribution yields:\n\\[\n\\overline{E} = \\frac{1}{Z} \\sum_i E_i \\exp(-\\beta E_i) = -\\frac{1}{Z} \\sum_i \\frac{\\partial}{\\partial \\beta} \\exp(-\\beta E_i) = -\\frac{1}{Z} \\frac{\\partial Z}{\\partial \\beta}\n\\]\nNote the trick of using the derivative to bring down \\(E_i\\) from the argument of the exponential. A further manipulation to note is:\n\\[\n\\frac{\\partial \\ln Z}{\\partial \\beta} = \\frac{1}{Z} \\frac{\\partial Z}{\\partial \\beta}\n\\]\nand we find:\n\\[\n\\overline{E} = -\\frac{\\partial \\ln Z}{\\partial \\beta} = -\\frac{dT}{d\\beta} \\frac{\\partial \\ln Z}{\\partial T} = kT^2 \\frac{\\partial \\ln Z}{\\partial T}\n\\]\nNow we consider fluctuations of the energy. The variance is:\n\\[\n\\overline{(\\Delta E)^2} = \\overline{(E - \\overline{E})^2} = \\overline{E^2} - \\overline{E}^2\n\\]\nIt turns out that this variance is related to the heat capacity \\(C\\) defined by \\[\nC \\equiv \\frac{\\partial \\overline{E}}{\\partial T} = \\frac{d\\beta}{dT} \\frac{\\partial \\overline{E}}{\\partial \\beta} = -\\frac{1}{kT^2} \\frac{\\partial \\overline{E}}{\\partial \\beta}\n\\]\nThe relationship (derived below) is \\[\nC = -\\frac{1}{kT^2} \\left[\\overline{E^2}- \\overline{E}^2\\right] = \\frac{(\\overline{\\Delta E)^2}}{kT^2}\n\\]\n\n\n\n\n\n\nOpen to see derivation\n\n\n\n\n\nIn order to demonstrate the relation of the energy fluctuations to the heat capacity, we write out the final expression for \\(C\\) above using the Boltzmann distribution:\n\\[\nC = -\\frac{1}{kT^2} \\frac{\\partial}{\\partial \\beta} \\left(\\frac{\\sum_i E_i \\exp(-\\beta E_i)}{Z}\\right) = -\\frac{1}{kT^2} \\left[-\\frac{\\sum_i E_i \\exp(-\\beta E_i)}{Z^2} \\frac{\\partial Z}{\\partial \\beta} - \\frac{\\sum_i E_i^2 \\exp(-\\beta E_i)}{Z}\\right]\n\\]\nwhich recalling:\n\\[\n\\sum_i E_i \\exp(-\\beta E_i) = \\overline{E} = -\\frac{1}{Z} \\frac{\\partial Z}{\\partial \\beta}\n\\]\n\\[\n\\sum_i E_i^2 \\exp(-\\beta E_i) = \\overline{E^2}\n\\]\nyields:\n\\[\nC = \\frac{1}{kT^2} \\left[\\overline{E^2}- \\overline{E}^2\\right] = \\frac{(\\overline{\\Delta E)^2}}{kT^2}\n\\]\n\n\n\nIt is interesting to pause and think what this means. The heat capacity can be thought of as the response of a macroscopic variable (the energy) to changing an external variable (the temperature). Thus, the response is related to the fluctuations of the macroscopic variable.\nSince \\(E\\) is proportional to \\(N\\), so will be \\(C = \\partial \\overline{E}/\\partial T\\). Therefore:\n\\[\n\\overline{(\\Delta E)^2} \\propto N\n\\]\nand the fractional deviation:\n\\[\n\\frac{\\left(\\overline{\\Delta E)^2}\\right)^{1/2}}{\\overline{E}} \\propto \\frac{1}{\\sqrt{N}}\n\\]\nFrom this, we see that for a system in equilibrium with a heat bath, although the energy is a free macroscopic variable, it takes on a sharply defined value \\(\\overline{E}\\). A consequence is that a large system in equilibrium with a heat bath should have essentially the same behavior as a large isolated system with fixed energy \\(\\overline{E}\\).",
    "crumbs": [
      "Chapters",
      "5. Free energy minimisation"
    ]
  },
  {
    "objectID": "chapter5.html#energy-distribution-entropy-and-free-energy",
    "href": "chapter5.html#energy-distribution-entropy-and-free-energy",
    "title": "5. Free energy minimisation",
    "section": "",
    "text": "Since, in the Boltzmann distribution, the energy is a free macroscopic variable, we can label the macrostates of the system by their energy, and the weight of the macrostate of energy \\(E\\) is \\(\\Omega(E)\\), the number of microstates with energy \\(E\\).\nAs in chapter 3, we can use the Planck relation to determine the entropy of a macrostate:\n\\[\nS(E) = k \\ln \\Omega(E)\n\\]\nWe now return to the question of the sharpness of \\(E\\). Let us consider again:\n\\[\n\\overline{E} = \\frac{1}{Z} \\sum_i E_i \\exp(-\\beta E_i)\n\\]\nGenerally, for a system of large \\(N\\), we expect many of the microstates to have the same energy. Therefore, rather than sum over all microstates, it is more convenient to sum over the possible values of the energy:\n\\[\n\\overline{E} = \\frac{1}{Z} \\sum_E \\Omega(E) E \\exp(-\\beta E)\n\\]\nTo understand this, realize that we are simply grouping together microstates with the same energy \\(E\\) of which there are \\(\\Omega(E)\\).\nThe same idea of grouping microstates with the same energy gives the probability of the system having energy \\(E\\):\n\\[\nP(E) = \\frac{1}{Z} \\Omega(E) \\exp(-\\beta E)\n\\]\nSince we expect \\(E \\sim N\\), \\(\\exp(-\\beta E)\\) is a sharply decreasing function. Also, \\(\\Omega(E)\\) is exponentially large in \\(N\\) and \\(\\Omega\\) is a sharply increasing function of \\(E\\). The two conspire to give as a product a sharply peaked probability distribution around \\(\\overline{E}\\). This is plotted in Figure 1\n\n\n\n\n\n\nFigure 1: Sharply peaked probability distribution of the energy\n\n\n\nTo go further, we rewrite \\(P(E)\\) and use the Planck relation:\n\\[\nP(E) = \\frac{1}{Z} \\exp\\left(-\\beta\\left(E - \\frac{\\ln \\Omega}{\\beta}\\right)\\right) = \\frac{1}{Z} \\exp(-\\beta(E - TS)) = \\frac{1}{Z} \\exp(-\\beta F)\n\\]\nwhere:\n\\[\nF(E) \\equiv E - TS(E)\n\\]\n\\(F\\) is the Helmholtz free energy, which is often referred to in statistical mechanics simply as the free energy.\nNow the peak of \\(P(E)\\) occurs when the argument of the exponential is maximized. Thus, we see that the equilibrium value of the energy \\(E\\) is determined by minimizing the free energy. This is a profound and general concept:\n\n\n\n\n\n\nKey Point 11:\n\n\n\nThe equilibrium values of the macroscopic properties \\(E, \\{\\alpha\\}\\) of a system in equilibrium are such as to minimize the free energy \\(F(E, \\{\\alpha\\}) \\equiv E - TS(E, \\{\\alpha\\})\\).\n\n\nWe write equilibrium values as \\(S(T)\\), \\(F(T)\\), etc.",
    "crumbs": [
      "Chapters",
      "5. Free energy minimisation"
    ]
  },
  {
    "objectID": "chapter5.html#free-energy-the-energyentropy-competition",
    "href": "chapter5.html#free-energy-the-energyentropy-competition",
    "title": "5. Free energy minimisation",
    "section": "",
    "text": "First, consider low \\(T\\): here \\(F\\) is dominated by \\(E\\), therefore minimizing \\(F\\) corresponds to minimizing \\(E\\) (exactly so for \\(T = 0\\)).\nNow consider high \\(T\\): here \\(F\\) is dominated by \\(S\\), therefore minimizing \\(F\\) corresponds to maximizing \\(S\\) (exactly so for \\(T = \\infty\\)).\nThus, one has a competition between energy and entropy, the arbiter of which is temperature.\nHowever, it should always be borne in mind that equilibrium is such as to maximize the entropy of the universe under the constraint that energy is conserved. For example, in an isolated system, the energy is constant, therefore we just maximize the entropy. For a system in equilibrium with its environment, the energy is free to vary. Thus, the minimization of the free energy for a system in equilibrium tells us the best way to distribute energy between the system and the universe such that the entropy of the universe is maximized.\nWe now begin to understand the different phases in question 2.5:\n\nAt low temperatures, ice forms. The crystal structure is energetically favorable and releases energy (latent heat of fusion) to the environment, which maximizes the entropy of the universe, i.e., from the viewpoint of the system, energy considerations win.\nAt higher temperatures, storing more energy in the system gives the liquid phase (water), which has a high entropy and allows the entropy of the universe to be maximized. From the point of view of the system, entropy wins.",
    "crumbs": [
      "Chapters",
      "5. Free energy minimisation"
    ]
  },
  {
    "objectID": "chapter5.html#utility-of-the-partition-function",
    "href": "chapter5.html#utility-of-the-partition-function",
    "title": "5. Free energy minimisation",
    "section": "",
    "text": "Let us first write \\(Z\\) as\n\\[\nZ = \\sum_E \\Omega(E) \\exp(-\\beta E)\n\\]\nIn the same way as \\(P(E)\\) is an extremely sharp function, the summand is very sharply peaked at \\(\\overline{E}\\) and of width \\(N^{1/2}\\) about the peak. Therefore, we can write\n\\[\nZ \\approx \\Omega(\\overline{E}) \\exp(-\\beta \\overline{E}) \\times O(N^{1/2})\n\\]\n\\[\n= \\exp(-\\beta F(T)) \\times O(N^{1/2})\n\\]\n(since \\(F(T) = F(\\overline{E})\\) for a large system in equilibrium). Thus\n\\[\n\\ln Z = -\\beta F(T) + O(\\ln N^{1/2})\n\\]\nWe see the last term can be ignored since we are in the ‘large \\(N\\)’ regime where \\(N \\gg \\ln N\\). Thus, the equilibrium value of the free energy\n\\[\nF(T) = \\overline{E} - TS(T) = -kT \\ln Z\n\\]\nThis is a very important formula.\nAlso, the expression can be written in terms of the entropy\n\\[\nS(T) = k \\ln Z + \\frac{\\overline{E}}{T}\n\\]\nTherefore, for a system in equilibrium at temperature \\(T\\), all the thermodynamic properties can be obtained from \\(Z\\) from the following formulae:\n\n\n\n\n\n\nKey Point 12:\n\n\n\n\\[\nF = -kT \\ln Z, \\quad \\overline{E} = kT^2 \\frac{\\partial}{\\partial T} \\ln Z, \\quad S = k \\ln Z + \\frac{\\overline{E}}{T}\n\\]",
    "crumbs": [
      "Chapters",
      "5. Free energy minimisation"
    ]
  },
  {
    "objectID": "chapter4.html",
    "href": "chapter4.html",
    "title": "4 The Boltzmann distribution",
    "section": "",
    "text": "See Mandl chapter 2.5 and Blundell & Blundell chapter 20\nPreviously we have considered an isolated system of fixed \\(N, E\\). Here we consider systems with fixed \\(N\\) but instead of \\(E\\) being fixed, the system is at some fixed temperature \\(T\\).\nTo be specific our system is in equilibrium with a heat bath (or heat reservior) at constant \\(T\\). The zeroth law of thermodynamics then implies our system is at temperature \\(T\\) and the thermal contact with the reservoir means that energy can be exchanged. Therefore the energy of our system is not fixed and the system can explore microstates of different energy. We expect microstates of the same energy to have the same probabilities (we have no reason to believe otherwise). However since we are at fixed \\(T\\) we might think that microstates of different energy should have different probabilities. In the following we substantiate this.",
    "crumbs": [
      "Chapters",
      "4. The Boltzmann distribution"
    ]
  },
  {
    "objectID": "chapter4.html#boltzmann-distribution-for-a-single-particle-system",
    "href": "chapter4.html#boltzmann-distribution-for-a-single-particle-system",
    "title": "4 The Boltzmann distribution",
    "section": "4.2 Boltzmann distribution for a single particle system",
    "text": "4.2 Boltzmann distribution for a single particle system\nAs a simple application of the Boltzmann distribution, let us consider a single magnetic dipole in equilibrium with a heat bath. (This is discussed in Baierlein 5.3 but with some differences in notation.)\nRecall that a single dipole has two microstates \\(\\downarrow, \\uparrow\\) with energies \\(mH, -mH\\).\nA simple application of the Boltzmann distribution gives:\n\\[\nP(\\downarrow) = \\frac{\\exp\\left(-\\frac{mH}{kT}\\right)}{Z}, \\quad P(\\uparrow) = \\frac{\\exp\\left(\\frac{mH}{kT}\\right)}{Z}\n\\]\nwhere the partition function \\(Z\\) is given by:\n\\[\nZ = \\exp\\left(-\\frac{mH}{kT}\\right) + \\exp\\left(\\frac{mH}{kT}\\right) = 2\\cosh\\left(\\frac{mH}{kT}\\right)\n\\]\nThe average energy of the dipole is given by:\n\\[\\begin{eqnarray*}\n\\overline{E} &=& \\sum_i E_i P_i \\\\\n&=& E(\\downarrow)P(\\downarrow)+E(\\uparrow)P(\\uparrow)\\\\\n&= &\\frac{1}{Z}\\left[ mH \\exp\\left(-\\frac{mH}{kT}\\right)-mH \\exp\\left(\\frac{mH}{kT}\\right) \\right] \\\\\n&=& -mH \\frac{\\sinh\\left(\\frac{mH}{kT}\\right)}{\\cosh\\left(\\frac{mH}{kT}\\right)}\\\\\n&=& -mH \\tanh\\left(\\frac{mH}{kT}\\right)\n\\end{eqnarray*}\\]\nTo see the temperature dependence of \\(\\bar{E}\\), we plot it as a function of \\(y = \\frac{kT}{mH}\\):\n\\[\n\\overline{E} = -mH \\tanh\\left(\\frac{1}{y}\\right)\n\\]\nYou should be able to sketch this curve by recalling the properties of the \\(\\tanh\\) function:\n\\[\n\\tanh x \\approx x \\quad \\text{for small } x, \\quad \\tanh x \\approx 1 \\quad \\text{for } x \\gg 1\n\\]\n\n\n\n\n\n\nFigure 2: Mean energy \\(E/mH\\) versus scaled temperature for a single dipole\n\n\n\nThe curve is plotted in Figure 2.\n\n\n\n\n\n\nExpand to learn about hyperbolic functions and their identities\n\n\n\n\n\nHyperbolic Identities\nThe hyperbolic functions satisfy a number of identities. These allow expressions involving the hyperbolic functions to be written in different, yet equivalent forms. Several commonly used identities are given below.\n\nBasic Hyperbolic Functions\n\\[\n\\cosh x = \\frac{e^x + e^{-x}}{2}, \\quad \\sinh x = \\frac{e^x - e^{-x}}{2}\n\\]\n\\[\n\\tanh x = \\frac{\\sinh x}{\\cosh x} = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n\\]\n\\[\n\\operatorname{sech} x = \\frac{1}{\\cosh x} = \\frac{2}{e^x + e^{-x}}\n\\]\n\\[\n\\operatorname{cosech} x = \\frac{1}{\\sinh x} = \\frac{2}{e^x - e^{-x}}\n\\]\n\\[\n\\operatorname{coth} x = \\frac{\\cosh x}{\\sinh x} = \\frac{1}{\\tanh x} = \\frac{e^x + e^{-x}}{e^x - e^{-x}}\n\\]\n\n\nSome derivatives of hyperbolic functions\n\\[\n\\frac{d}{dx} (\\sinh x) = \\cosh x\n\\]\n\\[\n\\frac{d}{dx} (\\cosh x) = \\sinh x\n\\]\n\\[\n\\frac{d}{dx} (\\tanh x) = \\operatorname{sech}^2 x\n\\]\n\n\nFundamental Identities\n\\[\n\\cosh^2 x - \\sinh^2 x = 1\n\\]\n\\[\n1 - \\tanh^2 x = \\operatorname{sech}^2 x\n\\]\n\\[\n\\operatorname{coth}^2 x - 1 = \\operatorname{cosech}^2 x\n\\]",
    "crumbs": [
      "Chapters",
      "4. The Boltzmann distribution"
    ]
  },
  {
    "objectID": "chapter4.html#basic-hyperbolic-functions",
    "href": "chapter4.html#basic-hyperbolic-functions",
    "title": "4 The Boltzmann distribution",
    "section": "Basic Hyperbolic Functions",
    "text": "Basic Hyperbolic Functions\n\\[\n\\cosh x = \\frac{e^x + e^{-x}}{2}, \\quad \\sinh x = \\frac{e^x - e^{-x}}{2}\n\\]\n\\[\n\\tanh x = \\frac{\\sinh x}{\\cosh x} = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n\\]\n\\[\n\\operatorname{sech} x = \\frac{1}{\\cosh x} = \\frac{2}{e^x + e^{-x}}\n\\]\n\\[\n\\operatorname{cosech} x = \\frac{1}{\\sinh x} = \\frac{2}{e^x - e^{-x}}\n\\]\n\\[\n\\operatorname{coth} x = \\frac{\\cosh x}{\\sinh x} = \\frac{1}{\\tanh x} = \\frac{e^x + e^{-x}}{e^x - e^{-x}}\n\\]",
    "crumbs": [
      "Chapters",
      "4. The Boltzmann distribution"
    ]
  },
  {
    "objectID": "chapter4.html#some-derivatives-of-hyperbolic-functions",
    "href": "chapter4.html#some-derivatives-of-hyperbolic-functions",
    "title": "4 The Boltzmann distribution",
    "section": "Some derivatives of hyperbolic functions",
    "text": "Some derivatives of hyperbolic functions\n\\[\n\\frac{d}{dx} (\\sinh x) = \\cosh x\n\\]\n\\[\n\\frac{d}{dx} (\\cosh x) = \\sinh x\n\\]\n\\[\n\\frac{d}{dx} (\\tanh x) = \\operatorname{sech}^2 x\n\\]",
    "crumbs": [
      "Chapters",
      "4. The Boltzmann distribution"
    ]
  },
  {
    "objectID": "chapter4.html#fundamental-identities",
    "href": "chapter4.html#fundamental-identities",
    "title": "4 The Boltzmann distribution",
    "section": "Fundamental Identities",
    "text": "Fundamental Identities\n\\[\n\\cosh^2 x - \\sinh^2 x = 1\n\\]\n\\[\n1 - \\tanh^2 x = \\operatorname{sech}^2 x\n\\]\n\\[\n\\operatorname{coth}^2 x - 1 = \\operatorname{cosech}^2 x\n\\]",
    "crumbs": [
      "Chapters",
      "4. The Boltzmann distribution"
    ]
  },
  {
    "objectID": "chapter4.html#single-constituent-in-an-isolated-system-for-large-n",
    "href": "chapter4.html#single-constituent-in-an-isolated-system-for-large-n",
    "title": "4 The Boltzmann distribution",
    "section": "4.3. Single constituent in an isolated system for large \\(N\\)",
    "text": "4.3. Single constituent in an isolated system for large \\(N\\)\nIn order to gain more insight into the generality of the Boltzmann distribution, we return briefly to the scenario of chapters 3 and 4 where we considered an isolated system with a large number \\(N\\) of constituents. We saw we could define an entropy \\(S(E)\\) through the Planck relation and a temperature through key point 9.\nIn particular, for the model magnet, we used the Planck relation to find the entropy:\n\\[\nS(E) = -Nk \\left[ x \\ln x + (1 - x) \\ln(1 - x) \\right], \\quad x = \\frac{n}{N} = \\frac{1}{2}\\left(1 + \\frac{E}{NmH}\\right)\n\\]\nwhere \\(n\\) is the number of excited dipoles and \\(N\\) is the total number of dipoles.\nUsing key point 9, we find:\n\\[\n\\frac{1}{T} = \\frac{\\partial S}{\\partial E} = \\frac{dx}{dE} \\frac{\\partial S}{\\partial x} = -\\frac{Nk}{2NmH} \\left[ \\ln x + 1 - \\ln(1 - x) - 1 \\right] = -\\frac{k}{2mH} \\ln\\left(\\frac{x}{1 - x}\\right)\n\\]\nInverting this, we find:\n\\[\n\\exp\\left(-\\frac{2mH}{kT}\\right) = \\frac{x}{1 - x}\n\\]\nso that:\n\\[\nx = \\frac{n}{N} = \\frac{\\exp\\left(-\\frac{2mH}{kT}\\right)}{1 + \\exp\\left(-\\frac{2mH}{kT}\\right)} = \\frac{\\exp(-\\beta mH)}{\\exp(\\beta mH) + \\exp(-\\beta mH)}\n\\]\nThe frequency definition of probability implies that \\(\\frac{n}{N}\\) is the probability that a single dipole is in the excited state. We see we recover the Boltzmann distribution of the previous subsection for a single dipole!\nAt first, this seems very confusing: for an isolated system, we have all microstates equally likely—so where did the Boltzmann distribution suddenly appear from?\nThe answer is to realize that a single dipole exchanges energy with the \\(N - 1\\) other dipoles. Therefore, for large \\(N\\), the other dipoles act as a heat bath for the single dipole, which is why we recover the Boltzmann distribution for a single dipole.\nAnother way of thinking about this is to remember that although all microstates of the isolated system are equally likely, we have the constraint of total fixed energy. Therefore, for a single dipole, its two states are not equally likely since the state it is in dictates how much energy is left to share among the other dipoles. Therefore, the global constraint of fixed energy induces a Boltzmann distribution for a single dipole.",
    "crumbs": [
      "Chapters",
      "4. The Boltzmann distribution"
    ]
  },
  {
    "objectID": "Problem_set_4.html",
    "href": "Problem_set_4.html",
    "title": "Einstein Model and low density gases- Problem set 4",
    "section": "",
    "text": "Review the argument leading to the result (for the mean energy of a 1d harmonic oscillator of frequency \\(\\omega\\))\n\\[\n\\bar{\\varepsilon} = \\hbar \\omega (\\bar{n} + 1/2) \\quad \\text{where} \\quad \\bar{n} = \\frac{1}{e^{\\hbar \\omega / kT} - 1}\n\\]\nEstablish the behaviour of \\(\\bar{\\varepsilon}\\) at high \\(T\\) and think about what it does not depend on.\n\n\n\nShow that the vibrational entropy of a 3d solid, described by the Einstein model, is\n\\[\nS = 3Nk \\left[ \\frac{x}{e^x - 1} - \\ln \\left( 1 - e^{-x} \\right) \\right]\n\\]\nwhere\n\\[\nx \\equiv \\frac{\\hbar \\omega}{kT}\n\\]\nEstablish the low- and high-\\(T\\) limiting behaviour. Contrast the high-\\(T\\) behaviour with that of the magnet.\n\n\n\nCalculate the molar entropy of argon gas at its normal boiling point of \\(T_b = 87.29 \\, \\text{K}\\), assuming the appropriateness of the ideal (low density) gas theory. Compare the result with the experimental value derived from calorimetric studies (how?) between low temperatures and the boiling point, \\(S = 129.6 \\pm 0.5 \\, \\text{J mol}^{-1} \\, \\text{K}^{-1}\\).\n\n\n\nShow that the density of states function \\(g(\\epsilon)\\) for a gas of particles of mass \\(M\\) confined to a 2d surface of area \\(A\\) is of the form\n\\[\ng(\\epsilon) = \\frac{AM}{2\\pi \\hbar^2}\n\\]\nindependent of \\(\\epsilon\\).\n[Help: This is a challenging question. If you don’t make progress, make sure you understand the solution. You need to modify the argument used to calculate the density of states in 3d by recognizing that the “spheres” in \\(k\\)-space subtending the relevant allowed states are now “circles”.]\n\n\n\nWrite down the Fermi-Dirac (FD) distribution (making sure you can distinguish it from the BE distribution!), and explain what it means. Show that the FD distribution can be viewed as giving the probability that a state (of the prescribed energy) is occupied. Sketch the form of the distribution for (i) \\(T=0\\); (ii) a low but finite \\(T\\).\n\n\n\nThe mean energy of a gas of fermions can be written as\n\\[\n\\bar{E} = \\int_0^{\\infty} d\\epsilon \\, \\epsilon \\, g(\\epsilon) f_+(\\epsilon)\n\\]\nwhere \\(g(\\epsilon)\\) is the density of states function. Show that, at \\(T = 0\\), this equation implies\n\\[\n\\bar{E} = \\frac{3 N \\epsilon_f}{5}\n\\]\n\n\n\nReview the argument that the Fermi energy\n\\[\n\\varepsilon_f = \\lim_{T \\to 0} \\mu(T)\n\\]\nand for the ideal Fermi gas\n\\[\n\\varepsilon_f = \\left( \\frac{3}{8 \\pi} \\frac{N}{V} \\right)^{2/3} \\frac{h^2}{2M}\n\\]\nThe molar volume of metallic Na is \\(23.7 \\, \\text{cc}\\). Each atom contributes its single 3s electron to the conduction electron gas. Determine (a) the number of such electrons per unit volume, \\(N/V\\), and (b) the associated Fermi energy \\(\\varepsilon_f\\)."
  },
  {
    "objectID": "Problem_set_4.html#statistical-mechanics-of-the-1d-harmonic-oscillator",
    "href": "Problem_set_4.html#statistical-mechanics-of-the-1d-harmonic-oscillator",
    "title": "Einstein Model and low density gases- Problem set 4",
    "section": "",
    "text": "Review the argument leading to the result (for the mean energy of a 1d harmonic oscillator of frequency \\(\\omega\\))\n\\[\n\\bar{\\varepsilon} = \\hbar \\omega (\\bar{n} + 1/2) \\quad \\text{where} \\quad \\bar{n} = \\frac{1}{e^{\\hbar \\omega / kT} - 1}\n\\]\nEstablish the behaviour of \\(\\bar{\\varepsilon}\\) at high \\(T\\) and think about what it does not depend on."
  },
  {
    "objectID": "Problem_set_4.html#entropy-of-the-vibrating-solid",
    "href": "Problem_set_4.html#entropy-of-the-vibrating-solid",
    "title": "Einstein Model and low density gases- Problem set 4",
    "section": "",
    "text": "Show that the vibrational entropy of a 3d solid, described by the Einstein model, is\n\\[\nS = 3Nk \\left[ \\frac{x}{e^x - 1} - \\ln \\left( 1 - e^{-x} \\right) \\right]\n\\]\nwhere\n\\[\nx \\equiv \\frac{\\hbar \\omega}{kT}\n\\]\nEstablish the low- and high-\\(T\\) limiting behaviour. Contrast the high-\\(T\\) behaviour with that of the magnet."
  },
  {
    "objectID": "Problem_set_4.html#a-check-on-the-entropy-formula",
    "href": "Problem_set_4.html#a-check-on-the-entropy-formula",
    "title": "Einstein Model and low density gases- Problem set 4",
    "section": "",
    "text": "Calculate the molar entropy of argon gas at its normal boiling point of \\(T_b = 87.29 \\, \\text{K}\\), assuming the appropriateness of the ideal (low density) gas theory. Compare the result with the experimental value derived from calorimetric studies (how?) between low temperatures and the boiling point, \\(S = 129.6 \\pm 0.5 \\, \\text{J mol}^{-1} \\, \\text{K}^{-1}\\)."
  },
  {
    "objectID": "Problem_set_4.html#density-of-states-for-a-2d-gas",
    "href": "Problem_set_4.html#density-of-states-for-a-2d-gas",
    "title": "Einstein Model and low density gases- Problem set 4",
    "section": "",
    "text": "Show that the density of states function \\(g(\\epsilon)\\) for a gas of particles of mass \\(M\\) confined to a 2d surface of area \\(A\\) is of the form\n\\[\ng(\\epsilon) = \\frac{AM}{2\\pi \\hbar^2}\n\\]\nindependent of \\(\\epsilon\\).\n[Help: This is a challenging question. If you don’t make progress, make sure you understand the solution. You need to modify the argument used to calculate the density of states in 3d by recognizing that the “spheres” in \\(k\\)-space subtending the relevant allowed states are now “circles”.]"
  },
  {
    "objectID": "Problem_set_4.html#the-fermi-dirac-distribution",
    "href": "Problem_set_4.html#the-fermi-dirac-distribution",
    "title": "Einstein Model and low density gases- Problem set 4",
    "section": "",
    "text": "Write down the Fermi-Dirac (FD) distribution (making sure you can distinguish it from the BE distribution!), and explain what it means. Show that the FD distribution can be viewed as giving the probability that a state (of the prescribed energy) is occupied. Sketch the form of the distribution for (i) \\(T=0\\); (ii) a low but finite \\(T\\)."
  },
  {
    "objectID": "Problem_set_4.html#energy-of-the-electron-gas",
    "href": "Problem_set_4.html#energy-of-the-electron-gas",
    "title": "Einstein Model and low density gases- Problem set 4",
    "section": "",
    "text": "The mean energy of a gas of fermions can be written as\n\\[\n\\bar{E} = \\int_0^{\\infty} d\\epsilon \\, \\epsilon \\, g(\\epsilon) f_+(\\epsilon)\n\\]\nwhere \\(g(\\epsilon)\\) is the density of states function. Show that, at \\(T = 0\\), this equation implies\n\\[\n\\bar{E} = \\frac{3 N \\epsilon_f}{5}\n\\]"
  },
  {
    "objectID": "Problem_set_4.html#ideal-fermi-gas-model-of-conduction-electrons-in-na",
    "href": "Problem_set_4.html#ideal-fermi-gas-model-of-conduction-electrons-in-na",
    "title": "Einstein Model and low density gases- Problem set 4",
    "section": "",
    "text": "Review the argument that the Fermi energy\n\\[\n\\varepsilon_f = \\lim_{T \\to 0} \\mu(T)\n\\]\nand for the ideal Fermi gas\n\\[\n\\varepsilon_f = \\left( \\frac{3}{8 \\pi} \\frac{N}{V} \\right)^{2/3} \\frac{h^2}{2M}\n\\]\nThe molar volume of metallic Na is \\(23.7 \\, \\text{cc}\\). Each atom contributes its single 3s electron to the conduction electron gas. Determine (a) the number of such electrons per unit volume, \\(N/V\\), and (b) the associated Fermi energy \\(\\varepsilon_f\\)."
  },
  {
    "objectID": "chapter7.html",
    "href": "chapter7.html",
    "title": "7: More on magnetism; review",
    "section": "",
    "text": "In this chapter we tie up some loose ends and take stock.\n\n\nSee Mandl 3.1\nSo far we have considered the ‘model magnet’ without really explaining why such a simple model is a good caricature of the real physics. This needs a knowledge of quantum mechanics that you have acquired in the quantum mechanics course.\nThe magnetic behaviour of solids is due to the behaviour of the electrons and/or nucleus. The simplest case is where the magnetic behaviour is due to a single ion of the solid molecule that has a single electron which acts as a ‘spin 1/2’. Quantum mechanics tells us that when we apply a magnetic field (which implies a ‘quantisation axis’) then the dipole moment is quantised parallel to the field as \\(\\pm m\\). This is the basis of the ‘model magnet’ which is a model of paramagnetism, i.e., magnetic ordering in response to an external field.\nIn the weakly interacting system, we assumed that energy was only stored in interaction between dipoles and the external field. In reality, the dipoles interact amongst themselves. The ‘model magnet’ is a good model when the molecules of the solid are large so that the single ions carrying the dipole moment are well separated (See Mandl 3.1).\nInteractions between dipoles have two origins. Firstly, we have the classical electromagnetic interaction where each dipole produces a field that its neighbors sit in. This tends to align dipoles. Secondly, we have a quantum mechanical interaction which occurs when the wavefunctions of neighboring atoms overlap. This produces what is known as an ‘exchange interaction’, which generally tends to align dipoles and is stronger than the classical interaction. If the interaction between dipoles is strong enough, it can lead to the phenomenon of ferromagnetism, where the dipoles tend to align without the aid of an external field.\n\n\n\nAs a very basic model of ferromagnetic interactions, let us restrict ourselves to a one-dimensional array of dipoles.\nHere there is no external field, and the only energy comes from the ferromagnetic interactions between particles: if two neighboring dipoles are in the same direction, this is favorable, but if two neighboring dipoles are in opposite directions, this costs energy.\nThus, each ‘domain wall’ where neighboring dipoles are of opposite directions costs energy \\(J\\), say. Here ia an example of two domain walls (three domains) in a one-dimensional array of \\(N\\) dipoles.\n\\[\n\\uparrow\\uparrow \\cdots \\uparrow\\uparrow \\downarrow\\downarrow \\cdots \\downarrow\\downarrow \\uparrow\\uparrow \\cdots \\uparrow\\uparrow\n\\]\nTo estimate the dependence of the number of such domain walls on temperature, we can use a minimization of free energy argument.\nLet us consider \\(n\\) domain walls. The energy cost is \\(nJ\\).\nThe number of ways of arranging the \\(n\\) domain walls on the lattice is the number of ways of choosing \\(n\\) from the \\(N - 1\\) possible places the domain walls can be. (Do you see why it is \\(N - 1\\)? but actually \\(N - 1 \\approx N\\) for large \\(N\\).) Thus:\n\\[\nS(n) = k \\ln \\binom{N - 1}{n} \\approx -kN [x \\ln x + (1 - x) \\ln(1 - x)]\n\\] where \\(x = \\frac{n}{N}\\).\nThus, the free energy as a function of \\(x = \\frac{n}{N}\\) is: \\[\nF(x) = E(x) - TS(x) = N \\left\\{ Jx + kT [x \\ln x + (1 - x) \\ln(1 - x)] \\right\\}\n\\] Minimising with respect to \\(x\\) yields: \\[\nJ + kT [\\ln x - \\ln(1 - x)] = 0\n\\] \\[\n\\Rightarrow \\frac{x}{1 - x} = \\exp \\left( \\frac{-J}{kT} \\right)\n\\] \\[\n\\Rightarrow \\overline{n} = N \\frac{\\exp \\left( \\frac{-J}{kT} \\right)}{1 + \\exp \\left( \\frac{-J}{kT} \\right)}\n\\] The average number of domains \\(n + 1\\) tends to \\(N/2\\) as \\(T \\to \\infty\\) and to 1 as \\(T \\to 0\\) (in this limit \\(n\\) is not large so strictly the analysis isn’t valid, but the results give some insight).\nAs usual, there is a competition between energy, here favoring ferromagnetic order, and entropy: at very low \\(T\\) (\\(kT \\ll J\\)), energy wins, and we have very large ferromagnetically ordered domains; at high \\(T\\) (\\(kT \\gg J\\)), entropy dominates and the domains are very small.\n\n\n\nWe have now covered all the fundamental concepts of statistical mechanics. Before proceeding it is worthwhile to try and recap on the basic ideas. You should go through the key points (13 of them) of chapters 3–7. Here we try and pull some ideas together.\n\nFirst we met microstates and macrostates. One thing to remember is that for a system of \\(N\\) ‘particles’ the number of microstates is typically exponentially large in \\(N\\) e.g. \\(2^N\\) for the model magnet. Also to each macrostate there correpsonds there typically corresponds a large number of microstates, exponentially large in \\(N\\).\nFor an isolated system all microstates are equally likely and the system evolves to the macrostate with the largest weight. This corresponds to the second law i.e. maximising the entropy.\nFor a system in equilibrium with a heat bath the entropy of the composite (system + heat bath) is maximised and this gives the Boltzmann distribution for the system. From the point of view of the system, its free energy is minimised.\nThe concept of equilibrium when energy exchange is present gives a statistical mechanics definition of temperature.\nAlways, contact is made with thermodynamics through the fact that macroscopic variables are sharply defined for large \\(N\\) and correspond to thermodynamic functions of state.\nCalculations are most easily done for weakly interacting systems using the partition function technique. This is the usual approach. It implies each particle (of the system of weakly interacting particles) has a single particle Boltzmann distribution.\nMinimisation of the free energy is an alternative way of obtaining macroscopic variables that gives good intuition about energy–entropy competition and can be used for interacting systems (see e.g. chapter 7.2).",
    "crumbs": [
      "Chapters",
      "7. More on magnetism; review"
    ]
  },
  {
    "objectID": "chapter7.html#paramagnetism-and-ferromagnetism",
    "href": "chapter7.html#paramagnetism-and-ferromagnetism",
    "title": "7: More on magnetism; review",
    "section": "",
    "text": "See Mandl 3.1\nSo far we have considered the ‘model magnet’ without really explaining why such a simple model is a good caricature of the real physics. This needs a knowledge of quantum mechanics that you have acquired in the quantum mechanics course.\nThe magnetic behaviour of solids is due to the behaviour of the electrons and/or nucleus. The simplest case is where the magnetic behaviour is due to a single ion of the solid molecule that has a single electron which acts as a ‘spin 1/2’. Quantum mechanics tells us that when we apply a magnetic field (which implies a ‘quantisation axis’) then the dipole moment is quantised parallel to the field as \\(\\pm m\\). This is the basis of the ‘model magnet’ which is a model of paramagnetism, i.e., magnetic ordering in response to an external field.\nIn the weakly interacting system, we assumed that energy was only stored in interaction between dipoles and the external field. In reality, the dipoles interact amongst themselves. The ‘model magnet’ is a good model when the molecules of the solid are large so that the single ions carrying the dipole moment are well separated (See Mandl 3.1).\nInteractions between dipoles have two origins. Firstly, we have the classical electromagnetic interaction where each dipole produces a field that its neighbors sit in. This tends to align dipoles. Secondly, we have a quantum mechanical interaction which occurs when the wavefunctions of neighboring atoms overlap. This produces what is known as an ‘exchange interaction’, which generally tends to align dipoles and is stronger than the classical interaction. If the interaction between dipoles is strong enough, it can lead to the phenomenon of ferromagnetism, where the dipoles tend to align without the aid of an external field.",
    "crumbs": [
      "Chapters",
      "7. More on magnetism; review"
    ]
  },
  {
    "objectID": "chapter7.html#simple-model-of-ferromagnetism",
    "href": "chapter7.html#simple-model-of-ferromagnetism",
    "title": "7: More on magnetism; review",
    "section": "",
    "text": "As a very basic model of ferromagnetic interactions, let us restrict ourselves to a one-dimensional array of dipoles.\nHere there is no external field, and the only energy comes from the ferromagnetic interactions between particles: if two neighboring dipoles are in the same direction, this is favorable, but if two neighboring dipoles are in opposite directions, this costs energy.\nThus, each ‘domain wall’ where neighboring dipoles are of opposite directions costs energy \\(J\\), say. Here ia an example of two domain walls (three domains) in a one-dimensional array of \\(N\\) dipoles.\n\\[\n\\uparrow\\uparrow \\cdots \\uparrow\\uparrow \\downarrow\\downarrow \\cdots \\downarrow\\downarrow \\uparrow\\uparrow \\cdots \\uparrow\\uparrow\n\\]\nTo estimate the dependence of the number of such domain walls on temperature, we can use a minimization of free energy argument.\nLet us consider \\(n\\) domain walls. The energy cost is \\(nJ\\).\nThe number of ways of arranging the \\(n\\) domain walls on the lattice is the number of ways of choosing \\(n\\) from the \\(N - 1\\) possible places the domain walls can be. (Do you see why it is \\(N - 1\\)? but actually \\(N - 1 \\approx N\\) for large \\(N\\).) Thus:\n\\[\nS(n) = k \\ln \\binom{N - 1}{n} \\approx -kN [x \\ln x + (1 - x) \\ln(1 - x)]\n\\] where \\(x = \\frac{n}{N}\\).\nThus, the free energy as a function of \\(x = \\frac{n}{N}\\) is: \\[\nF(x) = E(x) - TS(x) = N \\left\\{ Jx + kT [x \\ln x + (1 - x) \\ln(1 - x)] \\right\\}\n\\] Minimising with respect to \\(x\\) yields: \\[\nJ + kT [\\ln x - \\ln(1 - x)] = 0\n\\] \\[\n\\Rightarrow \\frac{x}{1 - x} = \\exp \\left( \\frac{-J}{kT} \\right)\n\\] \\[\n\\Rightarrow \\overline{n} = N \\frac{\\exp \\left( \\frac{-J}{kT} \\right)}{1 + \\exp \\left( \\frac{-J}{kT} \\right)}\n\\] The average number of domains \\(n + 1\\) tends to \\(N/2\\) as \\(T \\to \\infty\\) and to 1 as \\(T \\to 0\\) (in this limit \\(n\\) is not large so strictly the analysis isn’t valid, but the results give some insight).\nAs usual, there is a competition between energy, here favoring ferromagnetic order, and entropy: at very low \\(T\\) (\\(kT \\ll J\\)), energy wins, and we have very large ferromagnetically ordered domains; at high \\(T\\) (\\(kT \\gg J\\)), entropy dominates and the domains are very small.",
    "crumbs": [
      "Chapters",
      "7. More on magnetism; review"
    ]
  },
  {
    "objectID": "chapter7.html#pause-for-breath",
    "href": "chapter7.html#pause-for-breath",
    "title": "7: More on magnetism; review",
    "section": "",
    "text": "We have now covered all the fundamental concepts of statistical mechanics. Before proceeding it is worthwhile to try and recap on the basic ideas. You should go through the key points (13 of them) of chapters 3–7. Here we try and pull some ideas together.\n\nFirst we met microstates and macrostates. One thing to remember is that for a system of \\(N\\) ‘particles’ the number of microstates is typically exponentially large in \\(N\\) e.g. \\(2^N\\) for the model magnet. Also to each macrostate there correpsonds there typically corresponds a large number of microstates, exponentially large in \\(N\\).\nFor an isolated system all microstates are equally likely and the system evolves to the macrostate with the largest weight. This corresponds to the second law i.e. maximising the entropy.\nFor a system in equilibrium with a heat bath the entropy of the composite (system + heat bath) is maximised and this gives the Boltzmann distribution for the system. From the point of view of the system, its free energy is minimised.\nThe concept of equilibrium when energy exchange is present gives a statistical mechanics definition of temperature.\nAlways, contact is made with thermodynamics through the fact that macroscopic variables are sharply defined for large \\(N\\) and correspond to thermodynamic functions of state.\nCalculations are most easily done for weakly interacting systems using the partition function technique. This is the usual approach. It implies each particle (of the system of weakly interacting particles) has a single particle Boltzmann distribution.\nMinimisation of the free energy is an alternative way of obtaining macroscopic variables that gives good intuition about energy–entropy competition and can be used for interacting systems (see e.g. chapter 7.2).",
    "crumbs": [
      "Chapters",
      "7. More on magnetism; review"
    ]
  },
  {
    "objectID": "Coursework/Ising with response functions.html",
    "href": "Coursework/Ising with response functions.html",
    "title": "Statistical Physics",
    "section": "",
    "text": "## We import standard libraries that provide required functionality such as random numbers, averaging and plotting\n\nimport numpy as np\nfrom numpy.random import rand\nimport matplotlib.pyplot as plt\nfrom scipy.sparse import spdiags,linalg,eye\n\n\n##----------------------------------------------------------------------\n##  Here we define some functions that will be used in the main code\n##----------------------------------------------------------------------\n\ndef initialstate(L):\n    '''\n    Generates a random spin configuration for initial condition\n    '''\n    state = 2*np.random.randint(2, size=(L,L))-1\n    return state\n\n\ndef mcmove(config, beta):\n    '''\n    Here we perform successive Monte Carlo updates of the microstates. \n    We choose a spin at random and propose to flip it to the 'other' state. \n    The proposal is accepted or rejected depending on the energy change associated with the flip.\n    If the energy change of a flip is negative (ie the energy decreases) we always accept the proposed flip. \n    If the energy change is positive we accept the proposal with a probability dependent on the Boltzmann factor for the energy change.\n    '''\n\n    for i in range(L):\n        for j in range(L):\n                a = np.random.randint(0, L)\n                b = np.random.randint(0, L)\n                s =  config[a, b]\n                nb = config[(a+1)%L,b] + config[a,(b+1)%L] + config[(a-1)%L,b] + config[a,(b-1)%L]\n                cost = 2*s*nb\n\n                if cost &lt; 0:\n                    s *= -1\n                elif rand() &lt; np.exp(-cost*beta):\n                    s *= -1\n                config[a, b] = s\n    return config\n\n\ndef calcEnergy(config):\n    '''\n    Calculates the total energy of a given configuration, ie it calculates the energy macrostate associated with a given microstate.\n    '''\n    energy = 0\n\n    for i in range(len(config)):\n        for j in range(len(config)):\n            S = config[i,j]\n            nb = config[(i+1)%L, j] + config[i,(j+1)%L] + config[(i-1)%L, j] + config[i,(j-1)%L]\n            energy += -nb*S\n    return energy/2.  # to compensate for over-counting\n\n\ndef calcMag(config):\n    '''\n    Calculates the total magnetization of a given configuration, ie it calculates the magnetisation macrostate associated with a given microstate.\n    '''\n    mag = np.sum(config)\n    return mag\n\n\ndef microstatePlot(config, i, L, T):\n    ''' This modules plts the microstate once passed to it along with time etc '''\n    X, Y = np.meshgrid(range(L), range(L))\n    plt.pcolormesh(X, Y, config, cmap=plt.cm.viridis);\n    plt.title('Temperature={:.2f}, Time={}'.format(T,i)); \n    plt.axis('tight')\n    plt.show()\n\n\n#----------------------------------------------------------------------\n## Simulation parameters: Here we set (and can change) various parameters\n#----------------------------------------------------------------------\n\nnt      = 20          #  number of temperature points\nL       = 10          #  size of the lattice, L x L\neqSteps = 2**9        #  number of MC sweeps for equilibration\nmcSteps = 2**13       #  number of MC sweeps for calculation\n\nT       = np.linspace(1.5, 3.5, nt);  #Temperature range to be investigated in nt steps\nE,M,C_s,X_s = np.zeros(nt), np.zeros(nt), np.zeros(nt), np.zeros(nt) #Here we define an array for holding accumulated quantities at each T\nn1, n2  = 1.0/(mcSteps*L*L), 1.0/(mcSteps*mcSteps*L*L) # factors for dividing by number of samples, and by system size to get intensive values\n\n\n#----------------------------------------------------------------------\n#  Main Loop\n#----------------------------------------------------------------------\n\n\nfor tt in range(nt):\n    config = initialstate(L)         # initialise\n\n    E1 = M1 = E2 = M2 = 0\n    iT=1.0/T[tt]; iT2=iT*iT;         # Note that for convenience we set Boltzmann's constant k_B=1\n\n    for i in range(eqSteps):         # equilibrate\n        mcmove(config, iT)           # Monte Carlo moves\n    microstatePlot(config, i, L, T[tt]);\n    \n    for i in range(mcSteps):\n        mcmove(config, iT)\n        Ene = calcEnergy(config)     # calculate the energy\n        Mag = calcMag(config)        # calculate the magnetisation\n\n        E1 = E1 + Ene\n        M1 = M1 + Mag\n        M2 = M2 + Mag*Mag\n        E2 = E2 + Ene*Ene\n    \n\n    # Calculate intensive per-spin average quantities \n    E[tt] = n1*E1\n    M[tt] = n1*M1\n\n    # Add code here to calculate the heat capacity and magnetic susceptibility. You may wish to use iT,IT2, n1, n2 defined above\n    C_s[tt] = (n1*E2 - n2*E1*E1)*iT2\n    X_s[tt] = (n1*M2 - n2*M1*M1)*iT\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#----------------------------------------------------------------------\n#  plot the calculated values\n#----------------------------------------------------------------------\n\nf = plt.figure(figsize=(18, 18)); #\n\n\nsp =  f.add_subplot(2, 2, 1 );\nplt.scatter(T, E, s=50, marker='o', color='IndianRed')\nplt.xlabel(\"Temperature (T)\", fontsize=20);\nplt.ylabel(\"Average energy per spin\", fontsize=20);         plt.axis('tight');\n\n\nsp =  f.add_subplot(2, 2, 2 );\nplt.scatter(T, abs(M), s=50, marker='o', color='RoyalBlue')\nplt.xlabel(\"Temperature (T)\", fontsize=20);\nplt.ylabel(\"Average magnetization per spin\", fontsize=20);   plt.axis('tight');\n\n\nsp =  f.add_subplot(2, 2, 3 );\nplt.scatter(T, C_s, s=50, marker='o', color='IndianRed')\nplt.xlabel(\"Temperature (T)\", fontsize=20);\nplt.ylabel(\"Specific heat capacity\", fontsize=20);   plt.axis('tight');\n\n\nsp =  f.add_subplot(2, 2, 4 );\nplt.scatter(T, X_s, s=50, marker='o', color='RoyalBlue')\nplt.xlabel(\"Temperature (T)\", fontsize=20);\nplt.ylabel(\"Magnetic susceptibility per spin\", fontsize=20);   plt.axis('tight');"
  },
  {
    "objectID": "Coursework/assignment_prep.html",
    "href": "Coursework/assignment_prep.html",
    "title": "Preparatory reading and setup for coursework assignment: Investigating ferro- and para-magnetism in a two dimensional model magnet.",
    "section": "",
    "text": "In this coursework assignment, you will explore the two-dimensional (2d) square Ising model. This model is used to represent the interactions between spins (either pointing up or down) arranged on a two-dimensional lattice, which is relevant for understanding ferromagnetic materials. At lower temperatures, the system becomes ordered, meaning most of the spins align in the same direction. However, as the temperature rises above a certain critical point \\(T_c\\), the order is disrupted due to temperature fluctuations, leading to a random arrangement of spins (see Figure 1).\nThe objective of this project is to analyze this phase transition through numerical simulations using Monte Carlo simulation techniques. While some of the concepts we will use are theoretically advanced, we will focus on understanding them at a level that allows us to implement them in code. Further reading is encouraged for those interested in the more detailed theoretical background.\n\n\n\n\n\n\n\nFigure 1: Representation of spin alignment in the 2D Ising model.The arrows indicate spin-up or spin-down states. - (a) System below \\(T_c\\), where spins are ordered. - (b) System above \\(T_c\\), where spins are disordered.",
    "crumbs": [
      "Coursework preparation",
      "Coursework assignment preparation"
    ]
  },
  {
    "objectID": "Coursework/assignment_prep.html#the-2d-ising-model",
    "href": "Coursework/assignment_prep.html#the-2d-ising-model",
    "title": "Preparatory reading and setup for coursework assignment: Investigating ferro- and para-magnetism in a two dimensional model magnet.",
    "section": "2.1 The 2D Ising Model",
    "text": "2.1 The 2D Ising Model\nWe consider a square lattice with \\(L\\times L\\) sites, where the spins at each site \\((i,j)\\) are denoted as \\(s_{i,j}\\), with possible values of \\(s_{i,j} = \\pm 1\\). The system’s energy, is given by:\n\\[\nE(S) = -\\frac{J}{2} \\sum_{i,j} s_{i,j} \\left( s_{i+1,j} + s_{i-1,j} + s_{i,j+1} + s_{i,j-1} \\right),\n\\tag{1}\\] where \\(J&gt;0\\) is the interaction strength between neighboring spins, and the factor \\(\\frac{1}{2}\\) avoids counting the same interactions twice. Here, \\(S = \\{s_{i,j}\\}\\) represents a particular microstate of spins, and we assume periodic boundary conditions, such that \\(s_{i+N_x,j} = s_{i,j}\\) and \\(s_{i,j+N_y} = s_{i,j}\\).\nThe partition function \\(Z(\\beta)\\), which sums over all possible microstates and describes the system’s statistical properties, is defined as:\n\\[\nZ(\\beta) = \\sum_{S} \\exp\\left( -\\beta E(S) \\right),\n\\tag{2}\\]\nwhere \\(\\beta = \\frac{1}{k T}\\), with \\(T\\) being the temperature and \\(k\\) the Boltzmann constant. The sum runs over all possible spin microstates \\(S\\).\nThe average (also called the `expectation’) value of an observable macrovariable \\(O\\) is given by:\n\\[\n\\overline O  = \\frac{1}{Z(\\beta)} \\sum_{S} O(S) \\exp\\left( -\\beta E(S) \\right).\n\\tag{3}\\]\nRelevant macrovariables for the Ising magnet are the total energy \\(E\\) defined above and the total magnetisation \\(M=\\sum_i s_i\\).\nThe total number of possible states \\(S\\) for a system of size \\(L \\times L\\) is \\(2^{L^2}\\). This makes direct computation of sums like Equation 3 impractical for even relatively small volumes. Therefore, we will introduce a statistical method to approximate these sums.",
    "crumbs": [
      "Coursework preparation",
      "Coursework assignment preparation"
    ]
  },
  {
    "objectID": "Coursework/assignment_prep.html#monte-carlo-simulation",
    "href": "Coursework/assignment_prep.html#monte-carlo-simulation",
    "title": "Preparatory reading and setup for coursework assignment: Investigating ferro- and para-magnetism in a two dimensional model magnet.",
    "section": "2.2 Monte Carlo simulation",
    "text": "2.2 Monte Carlo simulation\nMonte Carlo methods are a class of algorithms that use repeated random sampling to obtain numerical solutions. For instance, integrals can be approximated using Monte Carlo techniques by generating \\({\\cal N}\\) random samples from the domain of integration \\(A\\) and evaluating the function at these points. For a 1D integral, the result is approximated as:\n\\[\nI = \\int_A dx \\, f(x) \\approx \\frac{1}{{\\cal N}} \\sum_{i=1}^{{\\cal N}} f(x_i),\n\\]\nwhere \\(x_i\\) are random points uniformly distributed in \\(A\\). In practice, while we cannot generate an infinite number of samples, using a large but finite \\({\\cal N}\\) still provides a good approximation.\nThis idea can also be applied to sums, like the one in Eq. (3). However, for the Ising model, where the sum runs over all possible spin microstates, generating uniformly distributed random samples is inefficient, as most microstates contribute negligibly to the partition function. A more efficient approach focuses on generating only those microstates that have a significant contribution. This technique is known as importance sampling.\nTo generate these important microstates, we use the Metropolis algorithm. Starting from an initial microstate, we generate the next microstate as follows:\n\nSelect a random spin \\(s_{i,j}\\) and flip it: \\(s_{i,j} \\to -s_{i,j}\\).\nThis changes the microstate from \\(S = \\{s_{1,1}, s_{1,2}, \\dots, s_{i,j}, \\dots, s_{L,L}\\}\\) to \\(S' = \\{s_{1,1}, s_{1,2}, \\dots, -s_{i,j}, \\dots, s_{L,L}\\}\\).\nCompute the energy change \\(\\Delta E = E(S') - E(S)\\).\nAccept the new microstate \\(S'\\) with the following probability:\n\n\\[\nW(s_{i,j} \\to -s_{i,j}) =\n\\begin{cases}\n1 & \\text{if } \\Delta E &lt; 0, \\\\\ne^{-\\beta \\Delta E} & \\text{otherwise}.\n\\end{cases}\n\\]\nIf the new microstate is rejected, the system remains in the current microstate \\(S\\). Note that in performing these updates it is convenient to define a reduced temperature \\(T=kT/J\\). Another way of thinking about this is that we are working with a system for which \\(J=k=1\\).\nA single update involves repeating these steps for all \\(L^2\\) spins in the lattice.\nThe expectation value of a macrovariable \\(O\\) can then be approximated as:\n\\[\n\\overline O \\approx \\frac{1}{{\\cal N}} \\sum_{n=0}^{{\\cal N}} O_n,\n\\]\nwhere \\(O_n\\) is the value of the macrovariable measured for the \\(n\\)-th microstate \\(S_n\\), and \\({\\cal N}\\) is the total number of microstates generated.",
    "crumbs": [
      "Coursework preparation",
      "Coursework assignment preparation"
    ]
  },
  {
    "objectID": "Coursework/assignment_prep.html#setup",
    "href": "Coursework/assignment_prep.html#setup",
    "title": "Preparatory reading and setup for coursework assignment: Investigating ferro- and para-magnetism in a two dimensional model magnet.",
    "section": "3.1 Setup",
    "text": "3.1 Setup\nThis should be completed in the week prior to the release of the assignment to make sure that any technical problems are resolved.\n\nOn the PH20040 Blackboard page open the Resources and Tools tab\nScroll down to Notable and open it (if off campus make sure you have the UoB VPN enabled)\nSelect the Jupyter Notebook (Legacy) notebook server option\nWhen the notebook has opened click the +Gitrepo button\nUnder enter Git Repository insert: https://github.com/nbwilding/Ising-coursework\nPress the “clone” button. This will download a notebook called Ising.ipynb into Jupyter\nCheck that the program runs\nFamiliarise yourself with the main features of the program. Pay attention to how to change the temperature range and number of spins \\(N=L^2\\), and how the energy and magnetisation and their averages are calculated. Be aware that the program can take several minutes to run depending on the system size.\n\nThe assignment itself will be released on Friday 14th March on Blackbiard via the Assessment, Submission and Feedback tab. The submission deadline is Friday 21st March at 12:30.\nPlease contact me (nigel.wilding@bristol.ac.uk) if you have trouble with the setup described above, detailing the problem you encountered.",
    "crumbs": [
      "Coursework preparation",
      "Coursework assignment preparation"
    ]
  },
  {
    "objectID": "NotesFigs/notebooks/magwhtfn.html",
    "href": "NotesFigs/notebooks/magwhtfn.html",
    "title": "Statistical Physics",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the function s(x)\ndef s(x):\n    return - (1 - x) * np.log(1 - x) - x * np.log(x)\n\n# Generate x values in the range from 0 to 1\nx_values = np.linspace(0.001, 0.999, 100)  # Avoiding x = 0 and x = 1 for logarithmic function\n\n# Compute corresponding y values\ny_values = s(x_values)\n\n# Plot the function\nplt.plot(x_values, y_values)\nplt.xlabel('$n/N$')\nplt.ylabel('$\\ln\\Omega(N,E)/N$')\nplt.legend()\nplt.grid(True)\npath=\"/Users/phxnw/Dropbox/Statphys/Course_material/NotesFigs/logmagweightfn.png\"\nplt.savefig(path,format=\"png\",dpi=1200)\nplt.show()\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument."
  },
  {
    "objectID": "NotesFigs/notebooks/coinflip.html",
    "href": "NotesFigs/notebooks/coinflip.html",
    "title": "Statistical Physics",
    "section": "",
    "text": "import random\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef flip_coin(n):\n    \"\"\"Simulate flipping a fair coin n times.\"\"\"\n    outcomes = [random.choice(['H', 'T']) for _ in range(n)]\n    heads_count = outcomes.count('H')\n    return heads_count / n\n\ndef simulate_coin_flips(N, M):\n    \"\"\"Simulate flipping a fair coin N times and repeat M times.\"\"\"\n    fractions = [flip_coin(N) for _ in range(M)]\n    return fractions\n\ndef plot_probability_histogram(fractions):\n    \"\"\"Plot a histogram of the probabilities of the fractions of heads.\"\"\"\n    weights = np.ones_like(fractions) / len(fractions)\n    plt.hist(fractions, bins=np.linspace(0, 1.0, 100), weights=weights, edgecolor='black')\n    plt.xlabel('Fraction of heads')\n    plt.ylabel('Probability')\n    plt.xticks(np.arange(0, 1, 0.1))\n    plt.xlim(0.1,0.9)\n    plt.title('Probability Histogram of Fraction of Heads in N={} Coin Flips'.format(N))\n    plt.grid(True)\n    path=\"/Users/phxnw/Desktop/toss_hist_Nlarge.png\"\n    plt.savefig(path,format=\"png\")\n    plt.show()\n\nif __name__ == \"__main__\":\n    N = int(input(\"Enter the number of times to flip the coin each time: \"))\n    M = int(input(\"Enter the number of times to repeat the simulation: \"))\n    \n    fractions = simulate_coin_flips(N, M)\n    plot_probability_histogram(fractions)"
  },
  {
    "objectID": "NotesFigs/notebooks/magwhts.html",
    "href": "NotesFigs/notebooks/magwhts.html",
    "title": "Statistical Physics",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\nx_values = [0.499,0.5,0.501]\ny_values = [0,1.0,0]\n\n# Compute corresponding y values\n\nplt.tick_params(left = False, right = False , labelleft = False ,   labelbottom = True, bottom = True)\n\n  \n# Plot the function\n  \nplt.plot(x_values, y_values,'-')\nplt.xlabel('$n/N$')\nplt.xlim(0,1)\nplt.ylim(0,1.1)\nplt.ylabel('Weight function $\\Omega(N,E)$')\n    \npath=\"/Users/phxnw/Dropbox/Statphys/Course_material/NotesFigs/NAmagweightfn.png\"\nplt.savefig(path,format=\"png\",dpi=1200)\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to PHYS20040: Statistical Mechanics!",
    "section": "",
    "text": "Lecturer: Prof. Nigel Wilding (nigel.wilding@bristol.ac.uk)\n\n\n\n\n\n\nThis course in statistical mechanics introduces you to the powerful methods used to describe the behavior of systems with a large number of particles, linking the microscopic properties of individual atoms and molecules with macroscopic observables such as temperature, pressure, and entropy. Statistical mechanics provides a framework to understand thermodynamics from a probabilistic standpoint, using statistical tools to connect the microstates of a system to its overall behavior. In this course, you’ll learn fundamental concepts like free energies, partition functions, and the Boltzmann distribution, which allow us to describe systems in thermal equilibrium. Applications will include understanding phase transitions, properties of gases and magnets, and fluctuations, with examples from both classical and quantum systems. By the end of the course, you’ll have a toolkit for analyzing complex physical systems, making statistical mechanics an essential foundation for advanced studies in physics, chemistry, and materials science.\n\nThe navigation bar on the left will allow you to access the lecture notes and problem sets.\n\nDelivery and format\n\nDetailed e-notes (see Blackboard) can be viewed on a variety of devices. Pdf is also available.\nI will give ‘traditional’ lectures (Tues, Wed, Fri) in which I use slides to summarise and explain the lecture content. Questions welcome (within reason…)\nTry to read ahead in the notes, then come to lectures, listen to my explanations and then reread the notes.\nRewriting the notes or slides to express your own thoughts and understanding, or annotating a pdf copy can help wire the material into your own way of thinking.\nThere are group problem classes (Tues, Thurs) where you can try problem sheets and seek help. I will go over some problems with the class.\n\n\n\nQuestions and comments\nIf you have any questions about the course, please don’t hesitate to contact me, either by email (see above) or in a problems class.\nFinally, this is a new course for 2025, with the lecture notes written from scratch. If you find any errors or mistakes or something which isn’t clear, please let me know, or fill in this anonymous form:\n\n\n\n\n\n\nSubmit an error/mistake/query",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "chapter9.html",
    "href": "chapter9.html",
    "title": "9. Ideal gas and indistinguishability",
    "section": "",
    "text": "Blundell and Blundell chapter 21, Baierlein 5.6\nSo far we have applied the Boltzmann theory for weakly interacting systems to solids and their magnetic and vibrational properties. We now turn to the other example of a weakly interacting system mentioned earlier- the ideal gas. In fact we have to modify the theory a little to cope with this system due to the property of indistinguishability.\n\n\nSince atoms of some element or molecules of the same type all have the same chemical make-up they are truly identical.\nIn solids we can still tell which atom is which because they are localised at the lattice sites of the crystal structure. So we can label the atom nearest to site \\(1\\), as atom number \\(1\\) and know that basically it will always be the same atom.\nHowever in gases, or liquids, the identity of the particles is problematic. The problem stems from quantum mechanics. In quantum mechanics to correctly describe a many particle system one has to consider a many-particle wavefunction. We will not delve into the issues here, but will just argue that we cannot treat particles individually because we cannot keep track of their identities.\n\n\n\n\n\n\nKey point 14\n\n\n\nIdentical non-localised particles must be treated as indistinguishable.\n\n\n\n\n\nWe seek to modify in a simple way the Boltzmann theory to take into account indistinguishability.\nRoughly speaking, and provided the density of particles is low (we’ll elaborate on this later) the number of microstates of a system of indistinguishable particles is less by a factor of \\(1/N!\\) compared to the case where the particles are treated distinguishably. Thus to correct for the overcounting due to indistinguishability, we divide the partition function by \\(N!\\):\n\\[\nZ_{\\text{indist.}} = \\frac{1}{N!} \\sum_{i_1 = i_2 \\cdots = i_N} \\exp(-\\beta [\\epsilon_{i_1} + \\epsilon_{i_2} + \\cdots + \\epsilon_{i_N}]) = \\frac{1}{N!} [Z(1)]^N\n\\] where \\(i_n\\) is the state of particle \\(n\\).\nThe approximation that allows us to correct for indistinguishability via the (classical) factor \\(1/N!\\) is valid when the particle density is low. It is known as the semi-classical approximation. As we shall see, at high density one has to take quantum indistinguishability into account.\n\n\n\nWe now have to evaluate \\(Z(1)\\) the single particle partition function. Here the single particle is simply a free particle in a box (container for the ideal gas) which we take to be a cube of side \\(L\\).\nThe time independent Schrödinger’s equation for the free particle (\\(V = 0\\)) reduces to the equation for standing waves:\n\\[\n\\left(- \\frac{\\hbar^2}{2M} \\nabla^2 - \\epsilon \\right) \\psi = 0\n\\]\nConsider first the one-dimensional case which becomes:\n\\[\n\\psi'' = -k^2\\psi \\quad \\text{where} \\quad k^2 = \\frac{2M\\epsilon}{\\hbar^2} = 0\n\\]\nWe have to fit the boundary conditions that \\(\\psi\\) vanishes at the boundaries \\(x = 0, L\\). Thus:\n\\[\n\\psi = A \\sin kx \\quad \\text{with} \\quad k = \\frac{n\\pi}{L} \\quad \\text{and} \\quad n = 1, 2, 3, \\ldots\n\\]\nThe generalisation to three dimensions with the boundary conditions that \\(\\psi\\) vanishes at \\(x, y, z = 0, L\\) is straightforward:\n\\[\n\\psi = A \\sin k_xx \\sin k_yy \\sin k_zz \\quad \\text{with} \\quad k_x = \\frac{n_x\\pi}{L}, \\; k_y = \\frac{n_y\\pi}{L}, \\; k_z = \\frac{n_z\\pi}{L}\n\\]\n\\[\n\\epsilon = \\frac{\\hbar^2}{2M} (k_x^2 + k_y^2 + k_z^2) = \\frac{\\hbar^2\\pi^2}{2ML^2} (n_x^2 + n_y^2 + n_z^2)\n\\]\nThe task now is to sum over all allowed values of \\(n_x, n_y, n_z\\):\n\\[\nZ(1) = \\sum_{n_x, n_y, n_z} \\exp(-\\beta \\epsilon(n_x, n_y, n_z)) = \\exp(-\\beta \\epsilon(1, 1, 1)) + \\exp(-\\beta \\epsilon(2, 1, 1)) + \\ldots\n\\]\nIn principle this looks quite difficult but the way to simplify matters is to convert the sum into an integral.\n\n\n\n\n\n\nFigure 1: A 2-d representation of ‘n-space’. The density of points is one. An integration shell of radius \\(n\\) and thickness \\(dn\\) is indicated.\n\n\n\nReferring to Figure 2 we see that \\(n\\)-space contains points, which represent the allowed quantum states of the particle, distributed with unit density. Since the energy depends only on the magnitude \\(n\\) of the position vector of a point in n-space:\n\\[\nn = (n_x^2 + n_y^2 + n_z^2)^{1/2}\n\\]\nwe convert to an integral with respect to \\(n\\). We think of adding up shells of constant \\(n\\) whose volume are \\(\\frac{1}{8} 4\\pi n^2 dn\\) and which, since the density of points is one, contain on average \\(\\frac{1}{8} 4\\pi n^2 dn\\) points. The volume is of a spherical shell of radius \\(n\\) and thickness \\(dn\\); the division by 8 is because we are restricted to the octant where all components of \\(n\\) are positive.\n\\[\n\\sum_{n_x, n_y, n_z} [\\cdots] \\rightarrow \\frac{1}{8} \\int [\\cdots] 4\\pi n^2 dn\n\\]\nIn particular:\n\\[\nZ(1) \\rightarrow \\frac{\\pi}{2} \\int_0^\\infty \\exp\\left(-\\beta \\frac{\\hbar^2\\pi^2n^2}{2ML^2}\\right) n^2 dn\n\\]\nThis is of course an approximation. It is a good approximation when:\n\\[\n\\frac{\\hbar^2\\pi^2}{2ML^2} \\ll kT\n\\]"
  },
  {
    "objectID": "chapter9.html#indistinguishability",
    "href": "chapter9.html#indistinguishability",
    "title": "9. Ideal gas and indistinguishability",
    "section": "",
    "text": "Since atoms of some element or molecules of the same type all have the same chemical make-up they are truly identical.\nIn solids we can still tell which atom is which because they are localised at the lattice sites of the crystal structure. So we can label the atom nearest to site \\(1\\), as atom number \\(1\\) and know that basically it will always be the same atom.\nHowever in gases, or liquids, the identity of the particles is problematic. The problem stems from quantum mechanics. In quantum mechanics to correctly describe a many particle system one has to consider a many-particle wavefunction. We will not delve into the issues here, but will just argue that we cannot treat particles individually because we cannot keep track of their identities.\n\n\n\n\n\n\nKey point 14\n\n\n\nIdentical non-localised particles must be treated as indistinguishable."
  },
  {
    "objectID": "chapter9.html#accounting-for-indistinguishability",
    "href": "chapter9.html#accounting-for-indistinguishability",
    "title": "9. Ideal gas and indistinguishability",
    "section": "",
    "text": "We seek to modify in a simple way the Boltzmann theory to take into account indistinguishability.\nRoughly speaking, and provided the density of particles is low (we’ll elaborate on this later) the number of microstates of a system of indistinguishable particles is less by a factor of \\(1/N!\\) compared to the case where the particles are treated distinguishably. Thus to correct for the overcounting due to indistinguishability, we divide the partition function by \\(N!\\):\n\\[\nZ_{\\text{indist.}} = \\frac{1}{N!} \\sum_{i_1 = i_2 \\cdots = i_N} \\exp(-\\beta [\\epsilon_{i_1} + \\epsilon_{i_2} + \\cdots + \\epsilon_{i_N}]) = \\frac{1}{N!} [Z(1)]^N\n\\] where \\(i_n\\) is the state of particle \\(n\\).\nThe approximation that allows us to correct for indistinguishability via the (classical) factor \\(1/N!\\) is valid when the particle density is low. It is known as the semi-classical approximation. As we shall see, at high density one has to take quantum indistinguishability into account."
  },
  {
    "objectID": "chapter9.html#calculation-of-z1",
    "href": "chapter9.html#calculation-of-z1",
    "title": "9. Ideal gas and indistinguishability",
    "section": "",
    "text": "We now have to evaluate \\(Z(1)\\) the single particle partition function. Here the single particle is simply a free particle in a box (container for the ideal gas) which we take to be a cube of side \\(L\\).\nThe time independent Schrödinger’s equation for the free particle (\\(V = 0\\)) reduces to the equation for standing waves:\n\\[\n\\left(- \\frac{\\hbar^2}{2M} \\nabla^2 - \\epsilon \\right) \\psi = 0\n\\]\nConsider first the one-dimensional case which becomes:\n\\[\n\\psi'' = -k^2\\psi \\quad \\text{where} \\quad k^2 = \\frac{2M\\epsilon}{\\hbar^2} = 0\n\\]\nWe have to fit the boundary conditions that \\(\\psi\\) vanishes at the boundaries \\(x = 0, L\\). Thus:\n\\[\n\\psi = A \\sin kx \\quad \\text{with} \\quad k = \\frac{n\\pi}{L} \\quad \\text{and} \\quad n = 1, 2, 3, \\ldots\n\\]\nThe generalisation to three dimensions with the boundary conditions that \\(\\psi\\) vanishes at \\(x, y, z = 0, L\\) is straightforward:\n\\[\n\\psi = A \\sin k_xx \\sin k_yy \\sin k_zz \\quad \\text{with} \\quad k_x = \\frac{n_x\\pi}{L}, \\; k_y = \\frac{n_y\\pi}{L}, \\; k_z = \\frac{n_z\\pi}{L}\n\\]\n\\[\n\\epsilon = \\frac{\\hbar^2}{2M} (k_x^2 + k_y^2 + k_z^2) = \\frac{\\hbar^2\\pi^2}{2ML^2} (n_x^2 + n_y^2 + n_z^2)\n\\]\nThe task now is to sum over all allowed values of \\(n_x, n_y, n_z\\):\n\\[\nZ(1) = \\sum_{n_x, n_y, n_z} \\exp(-\\beta \\epsilon(n_x, n_y, n_z)) = \\exp(-\\beta \\epsilon(1, 1, 1)) + \\exp(-\\beta \\epsilon(2, 1, 1)) + \\ldots\n\\]\nIn principle this looks quite difficult but the way to simplify matters is to convert the sum into an integral.\n\n\n\n\n\n\nFigure 1: A 2-d representation of ‘n-space’. The density of points is one. An integration shell of radius \\(n\\) and thickness \\(dn\\) is indicated.\n\n\n\nReferring to Figure 2 we see that \\(n\\)-space contains points, which represent the allowed quantum states of the particle, distributed with unit density. Since the energy depends only on the magnitude \\(n\\) of the position vector of a point in n-space:\n\\[\nn = (n_x^2 + n_y^2 + n_z^2)^{1/2}\n\\]\nwe convert to an integral with respect to \\(n\\). We think of adding up shells of constant \\(n\\) whose volume are \\(\\frac{1}{8} 4\\pi n^2 dn\\) and which, since the density of points is one, contain on average \\(\\frac{1}{8} 4\\pi n^2 dn\\) points. The volume is of a spherical shell of radius \\(n\\) and thickness \\(dn\\); the division by 8 is because we are restricted to the octant where all components of \\(n\\) are positive.\n\\[\n\\sum_{n_x, n_y, n_z} [\\cdots] \\rightarrow \\frac{1}{8} \\int [\\cdots] 4\\pi n^2 dn\n\\]\nIn particular:\n\\[\nZ(1) \\rightarrow \\frac{\\pi}{2} \\int_0^\\infty \\exp\\left(-\\beta \\frac{\\hbar^2\\pi^2n^2}{2ML^2}\\right) n^2 dn\n\\]\nThis is of course an approximation. It is a good approximation when:\n\\[\n\\frac{\\hbar^2\\pi^2}{2ML^2} \\ll kT\n\\]"
  },
  {
    "objectID": "chapter12.html",
    "href": "chapter12.html",
    "title": "12. Quantum gases",
    "section": "",
    "text": "12. Quantum gases\nIn the previous chapter we derived the grand canonical distribution. It applies to a system in equilibrium with a reservoir of energy and particles. This kind of system is referred to as an open system. The grand canonical distribution is given by:\n\\[\nP_r = \\frac{1}{Z} \\exp(-\\beta E_r + \\beta \\mu N_r)\n\\]\nwhere\n\\[\nZ = \\sum_r \\exp(-\\beta E_r + \\beta \\mu N_r)\n\\]\nHere, we use \\(r\\) to label the microstates to avoid a later clash of notation. Microstate \\(r\\) contains \\(N_r\\) particles and has energy \\(E_r\\).\n\n12.1. \\(N\\) as a function of \\(\\mu\\)\nIn the grand canonical distribution, microstates of the system with all numbers of particles are possible. However, we expect that for large \\(N\\) , the distribution of the particle number will become sharp.\nThe mean number of particles is calculated as follows:\n\\[\n\\overline{N} = \\sum_r N_r P_r = \\frac{1}{Z} \\sum_r N_r \\exp\\left(\\beta \\left[N_r \\mu - E_r \\right]\\right)\n\\]\n\\[\n\\overline{N} = \\frac{1}{\\beta} \\frac{\\partial \\ln Z}{\\partial \\mu}\n\\]\nwhere a similar idea to chapter 6 — where the mean energy was written as a derivative with respect to \\(\\beta\\) — has been used.\nBy a similar argument for the energy fluctuations in chapter 5, we find\n\\[\n\\frac{\\left( \\Delta \\overline{N^2} \\right)^{1/2}}{\\overline {N}} \\sim \\frac{1}{N^{1/2}}\n\\]\nTherefore, we see that \\(N\\) is sharp about \\(\\overline{N}\\) (i.e., fluctuations are small on the scale of the mean).\nThus, we\n\n\n\n\n\n\nKey Point 16:\n\n\n\nChoose \\(\\mu\\) to fix \\(\\overline{N} = N(\\mu)\\)\n\n\nin the same way that in the canonical distribution for a large system \\(T\\) fixes \\(\\overline{E} = E(T)\\) .\nRemember also that \\(\\mu\\) depends on temperature.\n\n\n12.2. Indistinguishable particles\nQuantum particles have access to a set of states (we will call them ‘single-particle- states’) defined by the solutions to the Schrödinger equation for one particle in a box. We will use \\(i=1,2,\\ldots \\infty\\) as a label of these states, and will denote the associated energies by \\(\\epsilon_1,\\epsilon_2,\\ldots\\epsilon_\\infty\\). Note: In the following, the term ‘quantum state’ will be used interchangeably with ‘single-particle state.’\nRecall the definition of a microstate (chapter 2):\n\nDistinguishable particles: A microstate is specified by \\(i_1, i_2, \\ldots, i_N\\) , i.e., the state of each particle.\nIndistinguishable particles: A microstate is specified by \\(n_1, n_2, \\ldots\\) , where \\(n_i\\) is the occupation number, i.e., the number of particles in single-particle state \\(i\\).\n\nThus, for indistinguishable particles in microstate \\(r\\) specified by the set \\(\\{n_i\\}\\) :\n\\[\nN_r = \\sum_i n_i \\quad \\text{and} \\quad E_r = \\sum_i n_i \\epsilon_i\n\\]\nwhere \\(\\epsilon_i\\) is the energy of single-particle state i.\nWe can write:\n\\[\n\\beta (N_r \\mu - E_r) = \\beta \\sum_i n_i (\\mu - \\epsilon_i)\n\\]\nand the sum over all possible microstates \\(r\\) becomes:\n\\[\n\\sum_r \\to \\sum_{n_1} \\sum_{n_2} \\cdots\n\\]\ni.e., a sum over all possible occupation numbers \\(n_i\\) of all quantum states.\nIt should be noted that in the canonical distribution, where \\(N\\) is fixed, a sum over all \\(n_i\\) is non-trivial because, for example, the value of occupation number \\(n_1\\) affects what values the other occupation numbers are allowed to take. However, in the grand canonical distribution, the sums ‘decouple,’ and the problem of the constraint on \\(N\\) is replaced by the problem of choosing \\(\\mu\\) to give the desired \\(N(\\mu)\\).\nNow consider the grand canonical partition function:\n\\[\n{\\cal Z} = \\sum_r \\exp\\left(\\beta \\left[N_r \\mu - E_r\\right]\\right)\n\\]\n\\[\n= \\left[\\sum_{n_1} \\sum_{n_2} \\cdots \\right] \\exp\\left(\\beta \\sum_i n_i (\\mu - \\epsilon_i)\\right)\n\\]\n\\[\n= \\left[\\sum_{n_1} \\exp\\left(\\beta n_1 (\\mu - \\epsilon_1)\\right)\\right] \\times \\left[\\sum_{n_2} \\exp\\left(\\beta n_2 (\\mu - \\epsilon_2)\\right)\\right] \\times \\cdots\n\\]\n\\[\n= {\\cal Z}_1 \\times {\\cal Z}_2 \\times \\cdots = \\prod_i {\\cal Z}_i\n\\]\nwhere \\({\\cal Z}_i\\) is the partition function for quantum state \\(i\\).\nThus, a factorization into single-state partition functions occurs. This should be contrasted with the factorization into single-particle partition functions that occurred in the canonical (Boltzmann) distribution.\nUsing the Factorization of \\({\\cal Z}\\) consider the probability of a microstate \\(r = \\{n_1, n_2, n_3, \\ldots\\}\\):\n\\[\nP_r = P(n_1, n_2, \\ldots) = \\frac{\\exp\\left(\\beta n_1 (\\mu - \\epsilon_1)\\right)}{{\\cal Z}_1} \\times \\frac{\\exp\\left(\\beta n_2 (\\mu - \\epsilon_2)\\right)}{{\\cal Z}_2} \\times \\cdots\n\\]\n\\[\n= P(n_1) P(n_2) \\cdots\n\\]\nwhere the single state distribution is:\n\\[\nP(n_i) = \\frac{\\exp\\left(\\beta n_i (\\mu - \\epsilon_i)\\right)}{Z_i}\n\\]\nTo understand this result, one can think of the quantum state \\(i\\) as being free to exchange particles with the rest of the quantum states, which therefore act as a reservoir of particles and energy. Thus, the quantum state \\(i\\) is itself an open system.\n\n\n12.3. Fermions and Bosons\nIn quantum mechanics, you have met the concepts of:\n\nSpin\nFermions and bosons\n\nFermions have spin equal to a half-integral multiple of \\(\\hbar\\), e.g., the magnetic dipoles we considered in the model magnet have spin \\(s = 1/2\\), and therefore, there are \\(2s + 1 = 2\\) spin states (up or down for the dipole).\nExamples of fermions are electrons, neutrons, protons, and composite particles made of an odd number of fermions, e.g., \\(^3\\text{He}\\), whose nucleus contains two protons and a neutron and is therefore a fermion; also, the whole atom is a fermion.\nBosons have spin equal to an integral multiple of \\(\\hbar\\) (note that they can have spin zero). Examples are photons and composite particles made up of an even number of fermions, e.g., \\(^4\\text{He}\\).\nThe most important thing for our purposes is the Pauli exclusion principle:\n\n\n\n\n\n\nThere can be at most one fermion in any quantum state.\n\n\n\nIn quantum mechanics, you will see how this comes from the antisymmetry of the many-particle wavefunction for fermions, but the boxed fact is all you need to know here.\nNow, consider the single state partition function. Due to the exclusion principle for fermions, an occupation number \\(n_i\\) can only take the values 0 or 1. Therefore, for fermions:\n\\[\n{\\cal Z}_i = \\sum_{n_i = 0, 1} \\exp\\left(\\beta n_i (\\mu - \\epsilon_i)\\right) = 1 + \\exp\\left(\\beta (\\mu - \\epsilon_i)\\right)\n\\]\nOn the other hand, there is no such restriction for bosons, for which \\(n_i\\) can take all values from 0 to \\(\\infty\\).\nFor bosons:\n\\[\n{\\cal Z}_i = \\sum_{n_i = 0}^{\\infty} \\exp\\left(\\beta n_i (\\mu - \\epsilon_i)\\right) = \\frac{1}{1 - \\exp\\left(\\beta (\\mu - \\epsilon_i)\\right)}\n\\]\nfor \\(\\exp\\left(\\beta (\\mu - \\epsilon_i)\\right) &lt; 1\\).\nTo understand this result for bosons, recall that:\n\\[\n\\sum_{n = 0}^{\\infty} x^n = \\frac{1}{1 - x} \\quad \\text{for } |x| &lt; 1\n\\]\nWe can now calculate \\(\\overline{n}_i\\), the average number of particles in quantum state \\(i\\). To do the calculation at the same time for fermions and bosons, write:\n\\[\n{\\cal Z}_i = \\left[1 \\pm \\exp\\left(\\beta (\\mu - \\epsilon_i)\\right)\\right]^{\\pm 1}\n\\]\nwhere \\(+\\) refers to bosons, and \\(−\\) refers to fermions.\nNow:\n\\[\n\\overline{n}_i = \\sum_{n_i} n_i P(n_i) = \\frac{1}{\\beta} \\frac{\\partial \\ln Z_i}{\\partial \\mu}.\n\\]\nThus:\n\\[\n\\overline{n}_i = \\pm \\frac{1}{\\beta} \\frac{\\partial}{\\partial \\mu} \\ln \\left[1 \\pm \\exp\\left(\\beta (\\mu - \\epsilon_i)\\right)\\right]\n\\]\n\\[\n= \\pm \\frac{1}{\\beta} (\\pm \\beta) \\frac{\\exp\\left(\\beta (\\mu - \\epsilon_i)\\right)}{1 \\pm \\exp\\left(\\beta (\\mu - \\epsilon_i)\\right)}\n\\]\n\\[\n= \\frac{\\exp\\left(\\beta (\\mu - \\epsilon_i)\\right)}{1 \\pm \\exp\\left(\\beta (\\mu - \\epsilon_i)\\right)}\n\\]\n\\[\n= \\frac{1}{\\exp\\left(\\beta (\\epsilon_i - \\mu)\\right) \\pm 1}\n\\]\nThe final results for the mean number of particles in quantum state \\(i\\) (that has energy \\(\\epsilon_i\\)) are known as the Fermi-Dirac and Bose-Einstein distributions:\n\n\n\n\n\n\nKey Point 17:\n\n\n\n\\[\n\\overline{n}_i = f_{\\pm}(\\epsilon_i) = \\frac{1}{\\exp\\left(\\beta (\\epsilon_i - \\mu)\\right) \\pm 1}\n\\] where \\(+\\) refers to fermions, and \\(−\\) refers to bosons.\n\n\n\\(N(\\mu)\\) (or \\(\\mu(N)\\)) is determined by the equation:\n\\[\nN = \\sum_i \\overline{n}_i = \\sum_i f_{\\pm}(\\epsilon_i)\n\\]"
  },
  {
    "objectID": "chapter11.html",
    "href": "chapter11.html",
    "title": "11. Systems with varying particle number",
    "section": "",
    "text": "See Mandl 8.1\nSo far we have developed the Boltzmann distribution where the number of particles \\(N\\) is fixed and the energy is a free macroscopic variable. In this chapter, we consider systems where the particle number is also free to fluctuate.\n\n\nThere are two reasons for allowing the particle number to fluctuate:\n\nWe may wish to consider systems free to exchange particles. An important example is phase co-existence whereby, e.g., a liquid and its vapor are in equilibrium, and a molecule may either be part of the liquid phase or of the gas phase.\nThe second motivation is that since the particle number is a macroscopic variable, for a large system it is sharply defined at some mean value. Thus, a large system with varying particle number is expected to have the same behavior as a system of fixed particle number.\n\nWe then use the system with varying particle number as ‘a means to an end,’ the end being the study of quantum gases for which the calculations turn out to be easier when the particle number is not fixed.\n\n\n\nConsider first a system as in Figure 1, where two halves (at equilibrium at the same temperature) are free to exchange particles. Since the total number \\(N=N_1+N_2\\) of particles is conserved:\n\n\n\n\n\n\nFigure 1: System at equilibrium comprising two subsystems free to exchange particles\n\n\n\n\\[\ndN_1 = -dN_2\n\\]\nAs the free energy is extensive, it can be written as a sum of contributions from each half of the system:\n\\[\nF = F_1(N_1) + F_2(N_2)\n\\]\n\\[\ndF = \\frac{\\partial F_1(N_1)}{\\partial N_1} dN_1 + \\frac{\\partial F_2(N_2)}{\\partial N_2} dN_2 = \\left[\\frac{\\partial F_1(N_1)}{\\partial N_1} - \\frac{\\partial F_2(N_2)}{\\partial N_2} \\right] dN_1\n\\]\nAt equilibrium, the free energy should be minimized, and \\(dF = 0\\), therefore we must have:\n\\[\n\\frac{\\partial F_1(N_1)}{\\partial N_1} = \\frac{\\partial F_2(N_2)}{\\partial N_2}\n\\]\n\n\n\n\n\n\nDefinition\n\n\n\n\\[\\mu \\equiv \\left(\\frac{\\partial F}{\\partial N} \\right)_{T,V}\\]\n\n\nis the chemical potential which is a quantity which is common to two systems that can exchange particles.\nA statement that is often made is: “the chemical potential is like a temperature for particle number”. If a system is not in equilibrium so that there is a chemical potential gradient, then particles will diffuse down then gradient. This is similar to heat diffusing down a temperature gradient (Figure 2).\n\n\n\n\n\n\nFigure 2: How gradients of temperature and chemical potential effect heat and mass flow respectively\n\n\n\n\n\n\nA specific example may be helpful. Consider an ideal gas confined to two connected equal volumes \\(V\\) (see Figure 3) separated by height \\(H\\) in a gravitational field \\(g\\). In the semi-classical treatment, the partition function factorizes into the contribution from the upper volume \\(Z_u\\) and from the lower volume \\(Z_l\\) :\n\n\n\n\n\n\nFigure 3: Simple example of two systems free to exchange particles (Baierlein Figure 7.1)\n\n\n\n\\[\nZ(N_l, N_u) = Z_l(N_l) \\times Z_u(N_u) = \\left(\\frac{V}{\\lambda^3}\\right)^{N_l} \\frac{1}{N_l!} \\times \\left(\\frac{V}{\\lambda^3}\\right)^{N_u} e^{-\\beta mgHN_u} \\frac{1}{N_u!}\n\\] where \\(\\lambda = \\frac{h}{\\sqrt{2\\pi MkT}}\\) . Using semi-classical results from 10.3:\n\\[\nF_l = -kT \\ln Z_l = N_l kT \\left[\\ln\\left(\\frac{N_l \\lambda^3}{V}\\right) - 1 \\right]\n\\]\n\\[\nF_u = N_u kT \\left[\\ln\\left(\\frac{N_u \\lambda^3}{V}\\right) - 1 \\right] + N_u MgH\n\\]\n\\[\n\\mu_l = \\frac{\\partial F_l}{\\partial N_l} = kT \\ln \\left(\\frac{N_l}{V \\lambda^3}\\right)\n\\]\n\\[\n\\mu_u = \\frac{\\partial F_u}{\\partial N_u} = kT \\ln \\left(\\frac{N_u}{V \\lambda^3}\\right) + MgH\n\\]\nEquating the chemical potentials \\(\\mu_l = \\mu_u\\) yields:\n\\[\nkT \\ln \\left(\\frac{N_l}{N_u}\\right) = MgH \\Rightarrow N_u = N_l e^{-MgH/kT}\n\\]\nThis gives the density dependence on height in the isothermal atmosphere. Note that \\(\\mu\\) increases with density, and \\(\\mu_u\\) increases with \\(H\\) , which supports the claim that the chemical potential measures the tendency for particles to diffuse.\n\n\n\n\n\n\nExpand to read about an equivalent definition of \\(\\mu\\)\n\n\n\n\n\nConsider the scenario of Figure 1, where a system isolated from the rest of the universe comprises two subsystems free to exchange energy and particles. Since the total system is isolated, we must maximize the entropy (see chapter 4, Tutorial 3.1):\n\\[\nS = S_1(E_1, N_1) + S_2(E_2, N_2)\n\\]\n\\[\ndS = \\left[\\left(\\frac{\\partial S_1}{\\partial E_1}\\right) - \\left(\\frac{\\partial S_2}{\\partial E_2}\\right)\\right] dE_1 + \\left[\\left(\\frac{\\partial S_1}{\\partial N_1}\\right) - \\left(\\frac{\\partial S_2}{\\partial N_2}\\right)\\right] dN_1\n\\]\nSince \\(dE_1\\) and \\(dN_1\\) are independent, to have \\(dS = 0\\) , we must have common values of:\n\\[\n\\left(\\frac{\\partial S}{\\partial E}\\right)_{N,V} = \\frac{1}{T}, \\quad \\left(\\frac{\\partial S}{\\partial N}\\right)_{E,V} = -\\frac{\\mu}{T}\n\\] where we have used the fact that for an ideal gas \\(E=0\\) so from \\(F=E-TS\\), \\(S=-F/T\\).\nThus, we identify:\n\\[\n\\mu = -T \\left(\\frac{\\partial S}{\\partial N}\\right)_{E,V}\n\\]\nThis definition is equivalent to the previous boxed definition of \\(\\mu\\).\n\n\n\n\n\n\nConsider the scenario of Figure 4, where, in analogy with chapter 4, the ‘composite’ comprises the system and the bath, which this time serves as a reservoir of energy and particles at constant \\(T\\) and \\(\\mu\\) .\n\n\n\n\n\n\nFigure 4: Composite of system + heat and particle reservoir\n\n\n\nA microstate \\(r\\) of the system has energy \\(E\\) and \\(N\\) particles. In analogy with chapter 4, using the principle of equal a priori probabilities, the probability of microstate \\(r\\) obeys:\n\\[\nP_r \\propto \\Omega_b(E_{tot} - E, N_{tot} - N) = \\exp\\left(\\frac{S_b(E_{tot} - E, N_{tot} - N)}{k}\\right)\n\\] where the Planck relation has been used. We proceed as in chapter 4 and Taylor expand the entropy (since \\(N, E \\ll N_{TOT}, E_{TOT}\\) ), keeping only the first few terms:\n\\[\nS_b(E_{TOT} - E, N_{TOT} - N) = S_b(E_{TOT}, N_{TOT}) - E \\frac{\\partial S_b(E_{TOT}, N_{TOT})}{\\partial E} - N \\frac{\\partial S_b(E_{TOT}, N_{TOT})}{\\partial N} + \\dots\n\\]\n\\[\n= \\text{const} - \\frac{E}{T} + \\frac{N \\mu}{T}\n\\]\nand\n\\[\nP_r \\propto \\exp \\left(\\frac{1}{kT} (N \\mu - E) \\right)\n\\]\nWe have now derived the ‘grand canonical distribution’ or Gibbs-Boltzmann distribution (as opposed to the canonical distribution, which is the Boltzmann distribution). The probability that a system in equilibrium with a reservoir of energy and particles at temperature \\(T\\) and chemical potential \\(\\mu\\) is in microstate \\(r\\) , which has energy \\(E_r\\) and particle number \\(N_r\\) , is:\n\n\n\n\n\n\nKey Point 15:\n\n\n\n\\[\nP_r = \\frac{1}{Z} \\exp(-\\beta E_r + \\beta \\mu N_r), \\quad Z = \\sum_j \\exp(-\\beta E_j + \\beta \\mu N_j), \\quad \\beta = \\frac{1}{kT}\n\\]"
  },
  {
    "objectID": "chapter11.html#motivations",
    "href": "chapter11.html#motivations",
    "title": "11. Systems with varying particle number",
    "section": "",
    "text": "There are two reasons for allowing the particle number to fluctuate:\n\nWe may wish to consider systems free to exchange particles. An important example is phase co-existence whereby, e.g., a liquid and its vapor are in equilibrium, and a molecule may either be part of the liquid phase or of the gas phase.\nThe second motivation is that since the particle number is a macroscopic variable, for a large system it is sharply defined at some mean value. Thus, a large system with varying particle number is expected to have the same behavior as a system of fixed particle number.\n\nWe then use the system with varying particle number as ‘a means to an end,’ the end being the study of quantum gases for which the calculations turn out to be easier when the particle number is not fixed."
  },
  {
    "objectID": "chapter11.html#the-chemical-potential",
    "href": "chapter11.html#the-chemical-potential",
    "title": "11. Systems with varying particle number",
    "section": "",
    "text": "Consider first a system as in Figure 1, where two halves (at equilibrium at the same temperature) are free to exchange particles. Since the total number \\(N=N_1+N_2\\) of particles is conserved:\n\n\n\n\n\n\nFigure 1: System at equilibrium comprising two subsystems free to exchange particles\n\n\n\n\\[\ndN_1 = -dN_2\n\\]\nAs the free energy is extensive, it can be written as a sum of contributions from each half of the system:\n\\[\nF = F_1(N_1) + F_2(N_2)\n\\]\n\\[\ndF = \\frac{\\partial F_1(N_1)}{\\partial N_1} dN_1 + \\frac{\\partial F_2(N_2)}{\\partial N_2} dN_2 = \\left[\\frac{\\partial F_1(N_1)}{\\partial N_1} - \\frac{\\partial F_2(N_2)}{\\partial N_2} \\right] dN_1\n\\]\nAt equilibrium, the free energy should be minimized, and \\(dF = 0\\), therefore we must have:\n\\[\n\\frac{\\partial F_1(N_1)}{\\partial N_1} = \\frac{\\partial F_2(N_2)}{\\partial N_2}\n\\]\n\n\n\n\n\n\nDefinition\n\n\n\n\\[\\mu \\equiv \\left(\\frac{\\partial F}{\\partial N} \\right)_{T,V}\\]\n\n\nis the chemical potential which is a quantity which is common to two systems that can exchange particles.\nA statement that is often made is: “the chemical potential is like a temperature for particle number”. If a system is not in equilibrium so that there is a chemical potential gradient, then particles will diffuse down then gradient. This is similar to heat diffusing down a temperature gradient (Figure 2).\n\n\n\n\n\n\nFigure 2: How gradients of temperature and chemical potential effect heat and mass flow respectively"
  },
  {
    "objectID": "chapter11.html#example",
    "href": "chapter11.html#example",
    "title": "11. Systems with varying particle number",
    "section": "",
    "text": "A specific example may be helpful. Consider an ideal gas confined to two connected equal volumes \\(V\\) (see Figure 3) separated by height \\(H\\) in a gravitational field \\(g\\). In the semi-classical treatment, the partition function factorizes into the contribution from the upper volume \\(Z_u\\) and from the lower volume \\(Z_l\\) :\n\n\n\n\n\n\nFigure 3: Simple example of two systems free to exchange particles (Baierlein Figure 7.1)\n\n\n\n\\[\nZ(N_l, N_u) = Z_l(N_l) \\times Z_u(N_u) = \\left(\\frac{V}{\\lambda^3}\\right)^{N_l} \\frac{1}{N_l!} \\times \\left(\\frac{V}{\\lambda^3}\\right)^{N_u} e^{-\\beta mgHN_u} \\frac{1}{N_u!}\n\\] where \\(\\lambda = \\frac{h}{\\sqrt{2\\pi MkT}}\\) . Using semi-classical results from 10.3:\n\\[\nF_l = -kT \\ln Z_l = N_l kT \\left[\\ln\\left(\\frac{N_l \\lambda^3}{V}\\right) - 1 \\right]\n\\]\n\\[\nF_u = N_u kT \\left[\\ln\\left(\\frac{N_u \\lambda^3}{V}\\right) - 1 \\right] + N_u MgH\n\\]\n\\[\n\\mu_l = \\frac{\\partial F_l}{\\partial N_l} = kT \\ln \\left(\\frac{N_l}{V \\lambda^3}\\right)\n\\]\n\\[\n\\mu_u = \\frac{\\partial F_u}{\\partial N_u} = kT \\ln \\left(\\frac{N_u}{V \\lambda^3}\\right) + MgH\n\\]\nEquating the chemical potentials \\(\\mu_l = \\mu_u\\) yields:\n\\[\nkT \\ln \\left(\\frac{N_l}{N_u}\\right) = MgH \\Rightarrow N_u = N_l e^{-MgH/kT}\n\\]\nThis gives the density dependence on height in the isothermal atmosphere. Note that \\(\\mu\\) increases with density, and \\(\\mu_u\\) increases with \\(H\\) , which supports the claim that the chemical potential measures the tendency for particles to diffuse.\n\n\n\n\n\n\nExpand to read about an equivalent definition of \\(\\mu\\)\n\n\n\n\n\nConsider the scenario of Figure 1, where a system isolated from the rest of the universe comprises two subsystems free to exchange energy and particles. Since the total system is isolated, we must maximize the entropy (see chapter 4, Tutorial 3.1):\n\\[\nS = S_1(E_1, N_1) + S_2(E_2, N_2)\n\\]\n\\[\ndS = \\left[\\left(\\frac{\\partial S_1}{\\partial E_1}\\right) - \\left(\\frac{\\partial S_2}{\\partial E_2}\\right)\\right] dE_1 + \\left[\\left(\\frac{\\partial S_1}{\\partial N_1}\\right) - \\left(\\frac{\\partial S_2}{\\partial N_2}\\right)\\right] dN_1\n\\]\nSince \\(dE_1\\) and \\(dN_1\\) are independent, to have \\(dS = 0\\) , we must have common values of:\n\\[\n\\left(\\frac{\\partial S}{\\partial E}\\right)_{N,V} = \\frac{1}{T}, \\quad \\left(\\frac{\\partial S}{\\partial N}\\right)_{E,V} = -\\frac{\\mu}{T}\n\\] where we have used the fact that for an ideal gas \\(E=0\\) so from \\(F=E-TS\\), \\(S=-F/T\\).\nThus, we identify:\n\\[\n\\mu = -T \\left(\\frac{\\partial S}{\\partial N}\\right)_{E,V}\n\\]\nThis definition is equivalent to the previous boxed definition of \\(\\mu\\)."
  },
  {
    "objectID": "chapter11.html#grand-canonical-distribution",
    "href": "chapter11.html#grand-canonical-distribution",
    "title": "11. Systems with varying particle number",
    "section": "",
    "text": "Consider the scenario of Figure 4, where, in analogy with chapter 4, the ‘composite’ comprises the system and the bath, which this time serves as a reservoir of energy and particles at constant \\(T\\) and \\(\\mu\\) .\n\n\n\n\n\n\nFigure 4: Composite of system + heat and particle reservoir\n\n\n\nA microstate \\(r\\) of the system has energy \\(E\\) and \\(N\\) particles. In analogy with chapter 4, using the principle of equal a priori probabilities, the probability of microstate \\(r\\) obeys:\n\\[\nP_r \\propto \\Omega_b(E_{tot} - E, N_{tot} - N) = \\exp\\left(\\frac{S_b(E_{tot} - E, N_{tot} - N)}{k}\\right)\n\\] where the Planck relation has been used. We proceed as in chapter 4 and Taylor expand the entropy (since \\(N, E \\ll N_{TOT}, E_{TOT}\\) ), keeping only the first few terms:\n\\[\nS_b(E_{TOT} - E, N_{TOT} - N) = S_b(E_{TOT}, N_{TOT}) - E \\frac{\\partial S_b(E_{TOT}, N_{TOT})}{\\partial E} - N \\frac{\\partial S_b(E_{TOT}, N_{TOT})}{\\partial N} + \\dots\n\\]\n\\[\n= \\text{const} - \\frac{E}{T} + \\frac{N \\mu}{T}\n\\]\nand\n\\[\nP_r \\propto \\exp \\left(\\frac{1}{kT} (N \\mu - E) \\right)\n\\]\nWe have now derived the ‘grand canonical distribution’ or Gibbs-Boltzmann distribution (as opposed to the canonical distribution, which is the Boltzmann distribution). The probability that a system in equilibrium with a reservoir of energy and particles at temperature \\(T\\) and chemical potential \\(\\mu\\) is in microstate \\(r\\) , which has energy \\(E_r\\) and particle number \\(N_r\\) , is:\n\n\n\n\n\n\nKey Point 15:\n\n\n\n\\[\nP_r = \\frac{1}{Z} \\exp(-\\beta E_r + \\beta \\mu N_r), \\quad Z = \\sum_j \\exp(-\\beta E_j + \\beta \\mu N_j), \\quad \\beta = \\frac{1}{kT}\n\\]"
  }
]